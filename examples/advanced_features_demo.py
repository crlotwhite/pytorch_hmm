"""
PyTorch HMM Library Feature Demonstration

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ìƒˆë¡œ êµ¬í˜„ëœ ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì˜ ì‚¬ìš©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤:
1. Dynamic Time Warping (DTW) ì •ë ¬
2. Connectionist Temporal Classification (CTC) ì •ë ¬  
3. Neural HMM with contextual modeling
4. Hidden Semi-Markov Model (HSMM) with duration modeling
5. Speech quality evaluation metrics
6. í†µí•© ì›Œí¬í”Œë¡œìš° ì˜ˆì œ
"""

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple
import time

# PyTorch HMM import
try:
    from pytorch_hmm import (
        # Core components
        HMMPyTorch, HMMLayer, 
        # Advanced models
        NeuralHMM, ContextualNeuralHMM, SemiMarkovHMM, DurationModel,
        # Alignment algorithms  
        DTWAligner, CTCAligner,
        # Evaluation metrics
        mel_cepstral_distortion, f0_root_mean_square_error, alignment_accuracy,
        comprehensive_speech_evaluation, print_evaluation_summary,
        # Utilities
        create_left_to_right_matrix, compute_state_durations
    )
    print("âœ“ Successfully imported PyTorch HMM library")
except ImportError as e:
    print(f"âŒ Import error: {e}")
    print("Please install the library first: pip install -e .")
    exit(1)


def demo_dtw_alignment():
    """DTW ì •ë ¬ ë°ëª¨"""
    print("\n" + "="*60)
    print("ğŸ¯ DEMO 1: Dynamic Time Warping (DTW) Alignment")
    print("="*60)
    
    # ìŒì†Œ íŠ¹ì§• ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” MFCC ë“±)
    print("Creating simulated phoneme and audio features...")
    
    # 5ê°œ ìŒì†Œì˜ íŠ¹ì§• ë²¡í„°
    phoneme_features = torch.randn(5, 12)  # 5 phonemes, 12-dim features
    print(f"Phoneme features shape: {phoneme_features.shape}")
    
    # 100 í”„ë ˆì„ì˜ ìŒì„± íŠ¹ì§• (ì‹¤ì œë¡œëŠ” ë” ê¸´ ì‹œí€€ìŠ¤)
    audio_features = torch.randn(100, 12)  # 100 frames, 12-dim features  
    print(f"Audio features shape: {audio_features.shape}")
    
    # DTW ì •ë ¬ ìˆ˜í–‰
    print("\nPerforming DTW alignment...")
    aligner = DTWAligner(distance_fn='cosine', step_pattern='symmetric')
    
    start_time = time.time()
    path_i, path_j, total_cost = aligner(phoneme_features, audio_features)
    dtw_time = time.time() - start_time
    
    print(f"DTW alignment completed in {dtw_time:.4f}s")
    print(f"Alignment path length: {len(path_i)}")
    print(f"Total DTW cost: {total_cost:.4f}")
    
    # ìŒì†Œ ê²½ê³„ ì¶”ì¶œ
    phoneme_boundaries = []
    current_phoneme = path_i[0].item()
    current_start = 0
    
    for i, phoneme_idx in enumerate(path_i):
        if phoneme_idx != current_phoneme:
            phoneme_boundaries.append((current_phoneme, current_start, path_j[i-1].item()))
            current_phoneme = phoneme_idx.item()
            current_start = path_j[i].item()
    
    # ë§ˆì§€ë§‰ ìŒì†Œ
    phoneme_boundaries.append((current_phoneme, current_start, path_j[-1].item()))
    
    print("\nExtracted phoneme boundaries:")
    for phoneme_id, start_frame, end_frame in phoneme_boundaries:
        duration_ms = (end_frame - start_frame) * 10  # 10ms per frame
        print(f"  Phoneme {phoneme_id}: frames {start_frame:3d}-{end_frame:3d} ({duration_ms:3d}ms)")
    
    return phoneme_boundaries


def demo_ctc_alignment():
    """CTC ì •ë ¬ ë°ëª¨"""
    print("\n" + "="*60)
    print("ğŸ¯ DEMO 2: CTC Alignment for Speech Recognition")
    print("="*60)
    
    # ìŒì„± ì¸ì‹ ì‹œë®¬ë ˆì´ì…˜
    vocab_size = 28  # 26 letters + blank + space
    sequence_length = 80
    batch_size = 2
    
    print(f"Simulating ASR with vocab_size={vocab_size}, seq_length={sequence_length}")
    
    # ìŒí–¥ ëª¨ë¸ì˜ ì¶œë ¥ (ë¡œê·¸ í™•ë¥ )
    log_probs = torch.log_softmax(torch.randn(sequence_length, batch_size, vocab_size), dim=-1)
    
    # íƒ€ê²Ÿ í…ìŠ¤íŠ¸ ("HELLO" ì™€ "WORLD")
    targets = torch.tensor([[8, 5, 12, 12, 15],   # HELLO (H=8, E=5, L=12, L=12, O=15)
                           [23, 15, 18, 12, 4]])  # WORLD (W=23, O=15, R=18, L=12, D=4)
    
    input_lengths = torch.full((batch_size,), sequence_length)
    target_lengths = torch.tensor([5, 5])
    
    print(f"Target texts: ['HELLO', 'WORLD']")
    print(f"Input lengths: {input_lengths.tolist()}")
    print(f"Target lengths: {target_lengths.tolist()}")
    
    # CTC ì •ë ¬ ë° ë””ì½”ë”©
    print("\nPerforming CTC alignment...")
    ctc_aligner = CTCAligner(num_classes=vocab_size, blank_id=0)
    
    # Loss ê³„ì‚°
    ctc_loss = ctc_aligner(log_probs, targets, input_lengths, target_lengths)
    print(f"CTC Loss: {ctc_loss.item():.4f}")
    
    # Greedy ë””ì½”ë”©
    decoded_sequences = ctc_aligner.decode(log_probs, input_lengths)
    
    print("\nDecoded sequences:")
    for i, decoded in enumerate(decoded_sequences):
        decoded_chars = [chr(ord('A') + idx - 1) if idx > 0 else '_' for idx in decoded.tolist()]
        print(f"  Sequence {i}: {decoded.tolist()} -> {''.join(decoded_chars)}")
    
    # ê°•ì œ ì •ë ¬
    alignments = ctc_aligner.align(log_probs, targets, input_lengths, target_lengths)
    
    print("\nForced alignment paths:")
    for i, alignment in enumerate(alignments):
        print(f"  Sequence {i}: {len(alignment)} frames")
        # ì²˜ìŒ 20 í”„ë ˆì„ì˜ ì •ë ¬ í‘œì‹œ
        first_20 = alignment[:20].tolist()
        alignment_chars = [chr(ord('A') + idx - 1) if idx > 0 else '_' for idx in first_20]
        print(f"    First 20 frames: {''.join(alignment_chars)}")


def demo_neural_hmm():
    """Neural HMM ë°ëª¨"""
    print("\n" + "="*60)
    print("ğŸ¯ DEMO 3: Neural HMM with Contextual Modeling")
    print("="*60)
    
    # ìŒì„± í•©ì„± ì‹œë‚˜ë¦¬ì˜¤: ìŒì†Œ ì‹œí€€ìŠ¤ -> ìŒí–¥ íŠ¹ì§•
    num_phonemes = 10
    num_states = 3  # ê° ìŒì†Œë‹¹ 3ê°œ ìƒíƒœ (ì‹œì‘-ì¤‘ê°„-ë)
    total_states = num_phonemes * num_states
    
    observation_dim = 80  # ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì°¨ì›
    context_dim = 64
    
    print(f"Setting up Neural HMM: {num_phonemes} phonemes, {num_states} states each")
    print(f"Observation dim: {observation_dim}, Context dim: {context_dim}")
    
    # Neural HMM ìƒì„±
    neural_hmm = NeuralHMM(
        num_states=total_states,
        observation_dim=observation_dim, 
        context_dim=context_dim,
        hidden_dim=128,
        transition_type='rnn',  # RNN ê¸°ë°˜ ì „ì´ ëª¨ë¸
        observation_type='mixture'  # Mixture Gaussian ê´€ì¸¡ ëª¨ë¸
    )
    
    # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ (ì–¸ì–´ì  íŠ¹ì§•, ìš´ìœ¨ ë“±)
    batch_size, seq_length = 2, 200
    context = torch.randn(batch_size, seq_length, context_dim)
    observations = torch.randn(batch_size, seq_length, observation_dim)
    
    print(f"\nInput shapes:")
    print(f"  Context: {context.shape}")  
    print(f"  Observations: {observations.shape}")
    
    # ì¶”ë¡  ìˆ˜í–‰
    print("\nPerforming Neural HMM inference...")
    start_time = time.time()
    
    with torch.no_grad():  # ë¹ ë¥¸ ì¶”ë¡ ì„ ìœ„í•´
        # Forward-backward
        posteriors, forward, backward = neural_hmm(observations, context)
        
        # Viterbi ë””ì½”ë”©
        optimal_states, state_scores = neural_hmm.viterbi_decode(observations, context)
    
    inference_time = time.time() - start_time
    
    print(f"Inference completed in {inference_time:.4f}s")
    print(f"Posterior shape: {posteriors.shape}")
    print(f"Optimal states shape: {optimal_states.shape}")
    
    # ìƒíƒœ ì ìœ  ë¶„ì„
    print("\nState occupancy analysis:")
    for b in range(batch_size):
        state_counts = torch.bincount(optimal_states[b], minlength=total_states)
        occupied_states = (state_counts > 0).sum().item()
        print(f"  Sequence {b}: {occupied_states}/{total_states} states used")
        
        # ì§€ì†ì‹œê°„ ë¶„ì„
        durations = compute_state_durations(optimal_states[b])
        avg_duration = durations.float().mean().item()
        print(f"    Average state duration: {avg_duration:.2f} frames")


def demo_contextual_neural_hmm():
    """Contextual Neural HMM ë°ëª¨"""
    print("\n" + "="*60)
    print("ğŸ¯ DEMO 4: Contextual Neural HMM for TTS")
    print("="*60)
    
    # TTS ì‹œë‚˜ë¦¬ì˜¤: ìŒì†Œ + ì»¨í…ìŠ¤íŠ¸ -> ìŒí–¥ ì •ë ¬
    phoneme_vocab_size = 50  # í•œêµ­ì–´ ìŒì†Œ ê°œìˆ˜
    num_states = 5
    observation_dim = 80
    
    print(f"TTS setup: {phoneme_vocab_size} phonemes, {num_states} HMM states")
    
    # Contextual Neural HMM ìƒì„±
    contextual_hmm = ContextualNeuralHMM(
        num_states=num_states,
        observation_dim=observation_dim,
        phoneme_vocab_size=phoneme_vocab_size,
        linguistic_context_dim=32,
        prosody_dim=8
    )
    
    # ì…ë ¥ ë°ì´í„°
    batch_size, seq_length = 1, 150
    
    # ìŒì†Œ ì‹œí€€ìŠ¤ (ì˜ˆ: "ì•ˆë…•í•˜ì„¸ìš”")
    phoneme_sequence = torch.tensor([[
        10, 11, 25, 26, 15, 16, 30, 31, 40, 41  # 5ê°œ ìŒì†Œ, ê°ê° 2í”„ë ˆì„
    ] * 15]).long()  # 150 í”„ë ˆì„ìœ¼ë¡œ í™•ì¥
    
    # ìš´ìœ¨ íŠ¹ì§• (F0, ì—ë„ˆì§€ ë“±)
    prosody_features = torch.randn(batch_size, seq_length, 8)
    
    # ìŒí–¥ íŠ¹ì§• (ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨)
    acoustic_features = torch.randn(batch_size, seq_length, observation_dim)
    
    print(f"Input shapes:")
    print(f"  Phoneme sequence: {phoneme_sequence.shape}")
    print(f"  Prosody features: {prosody_features.shape}")
    print(f"  Acoustic features: {acoustic_features.shape}")
    
    # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ë¡ 
    print("\nPerforming contextual inference...")
    start_time = time.time()
    
    with torch.no_grad():
        posteriors, forward, backward = contextual_hmm.forward_with_context(
            acoustic_features, phoneme_sequence, prosody_features)
    
    inference_time = time.time() - start_time
    
    print(f"Contextual inference completed in {inference_time:.4f}s")
    print(f"Posterior probabilities shape: {posteriors.shape}")
    
    # ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ìƒíƒœ ê²½ë¡œ
    most_likely_states = torch.argmax(posteriors, dim=-1)
    
    print("\nState transition analysis:")
    transitions = []
    for t in range(1, seq_length):
        if most_likely_states[0, t] != most_likely_states[0, t-1]:
            transitions.append((t, most_likely_states[0, t-1].item(), most_likely_states[0, t].item()))
    
    print(f"Total state transitions: {len(transitions)}")
    for i, (frame, from_state, to_state) in enumerate(transitions[:10]):  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
        print(f"  Frame {frame}: State {from_state} -> {to_state}")


def demo_semi_markov_hmm():
    """Semi-Markov HMM ë°ëª¨"""
    print("\n" + "="*60)
    print("ğŸ¯ DEMO 5: Hidden Semi-Markov Model (HSMM)")
    print("="*60)
    
    # ìŒì†Œ ì§€ì†ì‹œê°„ ëª¨ë¸ë§
    num_phonemes = 8
    observation_dim = 13  # MFCC ì°¨ì›
    max_duration = 20
    
    print(f"HSMM setup: {num_phonemes} phonemes, max duration {max_duration} frames")
    
    # HSMM ìƒì„±
    hsmm = SemiMarkovHMM(
        num_states=num_phonemes,
        observation_dim=observation_dim,
        max_duration=max_duration,
        duration_distribution='gamma',  # ê°ë§ˆ ë¶„í¬ë¡œ ì§€ì†ì‹œê°„ ëª¨ë¸ë§
        observation_model='gaussian'
    )
    
    print(f"Duration distribution: Gamma")
    print(f"Observation model: Gaussian")
    
    # ì§€ì†ì‹œê°„ ëª¨ë¸ ë¶„ì„
    print("\nDuration model analysis:")
    test_phonemes = torch.arange(num_phonemes)
    
    # ê° ìŒì†Œì˜ ì§€ì†ì‹œê°„ ë¶„í¬ í™•ì¸
    duration_distributions = hsmm.duration_model(test_phonemes)
    
    for phoneme_id in range(min(5, num_phonemes)):  # ì²˜ìŒ 5ê°œ ìŒì†Œë§Œ í‘œì‹œ
        probs = torch.exp(duration_distributions[phoneme_id])  # log -> prob
        most_likely_duration = torch.argmax(probs).item() + 1
        print(f"  Phoneme {phoneme_id}: Most likely duration = {most_likely_duration} frames")
    
    # ì‹œí€€ìŠ¤ ìƒ˜í”Œë§
    print("\nSampling sequence from HSMM...")
    sampled_states, sampled_durations, sampled_observations = hsmm.sample(
        num_states=6, max_length=100)
    
    print(f"Sampled sequence:")
    print(f"  States: {sampled_states.tolist()}")
    print(f"  Durations: {sampled_durations.tolist()}")
    print(f"  Total length: {sampled_durations.sum().item()} frames")
    print(f"  Observations shape: {sampled_observations.shape}")
    
    # ì§€ì†ì‹œê°„ í†µê³„
    avg_duration = sampled_durations.float().mean().item()
    std_duration = sampled_durations.float().std().item()
    print(f"  Average duration: {avg_duration:.2f} Â± {std_duration:.2f} frames")


def demo_evaluation_metrics():
    """ìŒì„± í’ˆì§ˆ í‰ê°€ ë©”íŠ¸ë¦­ ë°ëª¨"""
    print("\n" + "="*60)
    print("ğŸ¯ DEMO 6: Speech Quality Evaluation Metrics")
    print("="*60)
    
    # ì‹œë®¬ë ˆì´ì…˜ëœ TTS ì‹œìŠ¤í…œ í‰ê°€
    seq_length = 200
    mfcc_dim = 13
    
    print(f"Evaluating TTS system: {seq_length} frames, {mfcc_dim}-dim MFCC")
    
    # Ground truth íŠ¹ì§•ë“¤
    gt_mfcc = torch.randn(seq_length, mfcc_dim)
    gt_f0 = torch.abs(torch.randn(seq_length)) * 100 + 120  # 120-220 Hz
    gt_alignment = torch.randint(0, 10, (seq_length,))
    
    # ì˜ˆì¸¡ëœ íŠ¹ì§•ë“¤ (ì•½ê°„ì˜ ì˜¤ì°¨ í¬í•¨)
    noise_level = 0.1
    pred_mfcc = gt_mfcc + noise_level * torch.randn(seq_length, mfcc_dim)
    pred_f0 = gt_f0 + 5 * torch.randn(seq_length)  # 5Hz ì˜¤ì°¨
    pred_alignment = gt_alignment.clone()
    pred_alignment[::20] = (pred_alignment[::20] + 1) % 10  # 5% ì˜¤ë¥˜
    
    # ê°œë³„ ë©”íŠ¸ë¦­ ê³„ì‚°
    print("\nComputing individual metrics...")
    
    # MCD (Mel-Cepstral Distortion)
    mcd = mel_cepstral_distortion(gt_mfcc, pred_mfcc, exclude_c0=True)
    print(f"MCD: {mcd.item():.4f} dB")
    
    # F0 RMSE
    voiced_mask = gt_f0 > 0  # ìœ ì„±ìŒ êµ¬ê°„
    f0_rmse = f0_root_mean_square_error(gt_f0, pred_f0, voiced_mask)
    print(f"F0 RMSE: {f0_rmse.item():.4f} Hz")
    
    # ì •ë ¬ ì •í™•ë„
    align_acc = alignment_accuracy(pred_alignment, gt_alignment, tolerance=0)
    print(f"Alignment Accuracy: {align_acc.item():.4f} ({align_acc.item()*100:.1f}%)")
    
    # ì¢…í•© í‰ê°€
    print("\nComprehensive evaluation...")
    predicted_features = {
        'mfcc': pred_mfcc.unsqueeze(0),  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        'f0': pred_f0.unsqueeze(0),
        'alignment': pred_alignment
    }
    
    ground_truth_features = {
        'mfcc': gt_mfcc.unsqueeze(0),
        'f0': gt_f0.unsqueeze(0),
        'alignment': gt_alignment
    }
    
    comprehensive_metrics = comprehensive_speech_evaluation(
        predicted_features, ground_truth_features)
    
    print_evaluation_summary(comprehensive_metrics)
    
    # í’ˆì§ˆ í‰ê°€
    print("\nQuality Assessment:")
    if mcd < 5.0:
        print("  âœ“ Excellent MCD (< 5.0 dB)")
    elif mcd < 8.0:
        print("  âœ“ Good MCD (< 8.0 dB)")
    else:
        print("  âš  Poor MCD (> 8.0 dB)")
    
    if f0_rmse < 10.0:
        print("  âœ“ Excellent F0 accuracy (< 10 Hz)")
    elif f0_rmse < 20.0:
        print("  âœ“ Good F0 accuracy (< 20 Hz)")
    else:
        print("  âš  Poor F0 accuracy (> 20 Hz)")
    
    if align_acc > 0.9:
        print("  âœ“ Excellent alignment (> 90%)")
    elif align_acc > 0.8:
        print("  âœ“ Good alignment (> 80%)")
    else:
        print("  âš  Poor alignment (< 80%)")


def demo_integration_workflow():
    """í†µí•© ì›Œí¬í”Œë¡œìš° ë°ëª¨"""
    print("\n" + "="*60)
    print("ğŸ¯ DEMO 7: Complete TTS Pipeline Integration")
    print("="*60)
    
    print("Simulating complete Text-to-Speech pipeline...")
    
    # 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ì‹œë®¬ë ˆì´ì…˜)
    print("\n1. Text preprocessing:")
    text = "ì•ˆë…•í•˜ì„¸ìš”"  # "Hello" in Korean
    phonemes = [10, 15, 20, 25, 30]  # ìŒì†Œ IDë“¤
    print(f"   Text: '{text}' -> Phonemes: {phonemes}")
    
    # 2. ì–¸ì–´ì  íŠ¹ì§• ì¶”ì¶œ
    print("\n2. Linguistic feature extraction:")
    phoneme_sequence = torch.tensor([phonemes * 20]).long()  # 100 í”„ë ˆì„ìœ¼ë¡œ í™•ì¥
    prosody_features = torch.randn(1, 100, 8)  # ìš´ìœ¨ íŠ¹ì§•
    
    print(f"   Phoneme sequence: {phoneme_sequence.shape}")
    print(f"   Prosody features: {prosody_features.shape}")
    
    # 3. ì§€ì†ì‹œê°„ ì˜ˆì¸¡ (HSMM ì‚¬ìš©)
    print("\n3. Duration prediction with HSMM:")
    duration_model = DurationModel(
        num_states=len(set(phonemes)),
        max_duration=30,
        distribution_type='neural'
    )
    
    predicted_durations = duration_model.sample(torch.tensor(phonemes))
    print(f"   Predicted durations: {predicted_durations.tolist()}")
    total_frames = predicted_durations.sum().item()
    print(f"   Total frames: {total_frames}")
    
    # 4. ìŒí–¥ íŠ¹ì§• ìƒì„± (Neural HMM ì‚¬ìš©)
    print("\n4. Acoustic feature generation with Neural HMM:")
    acoustic_model = NeuralHMM(
        num_states=5,
        observation_dim=80,
        context_dim=40,  # ìŒì†Œ + ìš´ìœ¨ íŠ¹ì§•
        hidden_dim=128
    )
    
    # ì»¨í…ìŠ¤íŠ¸ ê²°í•©
    phoneme_emb = torch.randn(1, 100, 32)  # ìŒì†Œ ì„ë² ë”©
    context = torch.cat([phoneme_emb, prosody_features], dim=-1)
    
    # ìŒí–¥ íŠ¹ì§• ìƒì„±
    dummy_acoustic = torch.randn(1, 100, 80)
    posteriors, _, _ = acoustic_model(dummy_acoustic, context)
    
    print(f"   Generated acoustic features: {posteriors.shape}")
    
    # 5. ì •ë ¬ ë° í›„ì²˜ë¦¬ (DTW ì‚¬ìš©)
    print("\n5. Alignment refinement with DTW:")
    target_length = int(total_frames * 1.1)  # 10% ê¸¸ì´ ì¡°ì •
    
    dtw_aligner = DTWAligner(step_pattern='asymmetric')
    source_features = posteriors[0]  # (100, 80)
    target_features = torch.randn(target_length, 80)
    
    path_i, path_j, _ = dtw_aligner(source_features, target_features)
    print(f"   DTW alignment: {len(path_i)} -> {len(path_j)} frames")
    
    # 6. í’ˆì§ˆ í‰ê°€
    print("\n6. Quality evaluation:")
    
    # ì‹œë®¬ë ˆì´ì…˜ëœ ground truth
    gt_acoustic = torch.randn(1, 100, 80)
    gt_alignment = torch.randint(0, 5, (100,))
    
    pred_alignment = torch.argmax(posteriors, dim=-1)[0]
    align_acc = alignment_accuracy(pred_alignment, gt_alignment, tolerance=1)
    
    print(f"   Alignment accuracy: {align_acc.item():.4f}")
    
    # 7. ìµœì¢… ê²°ê³¼
    print("\n7. Pipeline summary:")
    total_time = 100 * 0.01  # 100 frames Ã— 10ms
    print(f"   Generated speech duration: {total_time:.2f} seconds")
    print(f"   Average phoneme duration: {predicted_durations.float().mean():.1f} frames")
    print(f"   Model complexity: Neural transitions + Gaussian observations")
    print(f"   Alignment method: DTW with cosine distance")
    
    print("\nâœ“ Complete TTS pipeline demonstration finished!")


def main():
    """ë©”ì¸ ë°ëª¨ ì‹¤í–‰"""
    print("ğŸš€ PyTorch HMM Advanced Features Demonstration")
    print("=" * 70)
    
    # ì‹œë“œ ì„¤ì •ìœ¼ë¡œ ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼
    torch.manual_seed(42)
    np.random.seed(42)
    
    # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ”§ Using device: {device.upper()}")
    
    if torch.cuda.is_available():
        print(f"   GPU: {torch.cuda.get_device_name()}")
        print(f"   Memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3} GB")
    
    # ëª¨ë“  ë°ëª¨ ì‹¤í–‰
    demos = [
        demo_dtw_alignment,
        demo_ctc_alignment, 
        demo_neural_hmm,
        demo_contextual_neural_hmm,
        demo_semi_markov_hmm,
        demo_evaluation_metrics,
        demo_integration_workflow
    ]
    
    for i, demo_func in enumerate(demos, 1):
        try:
            demo_func()
            print(f"\nâœ… Demo {i} completed successfully!")
        except Exception as e:
            print(f"\nâŒ Demo {i} failed: {e}")
            import traceback
            traceback.print_exc()
    
    # ìµœì¢… ìš”ì•½
    print("\n" + "="*70)
    print("ğŸ‰ All demonstrations completed!")
    print("="*70)
    
    print("\nKey takeaways:")
    print("â€¢ DTW provides flexible sequence alignment for variable-length data")
    print("â€¢ CTC enables end-to-end learning without explicit alignment")
    print("â€¢ Neural HMMs incorporate contextual information for better modeling")
    print("â€¢ HSMMs explicitly model state duration distributions")
    print("â€¢ Comprehensive metrics enable thorough quality evaluation")
    print("â€¢ Integration workflows demonstrate real-world applicability")
    
    print(f"\nğŸ“š For more information, check the documentation and examples!")
    print("   GitHub: https://github.com/crlotwhite/pytorch_hmm")


if __name__ == "__main__":
    main()
