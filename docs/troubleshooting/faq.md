# â“ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ)

PyTorch HMM ì‚¬ìš© ì‹œ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ë“¤ê³¼ í•´ë‹µì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.

## ğŸ“¦ ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

### Q1. PyTorch HMM ì„¤ì¹˜ ì‹œ ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤.

**A:** ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìˆœì„œëŒ€ë¡œ ì‹œë„í•´ë³´ì„¸ìš”:

```bash
# 1. Python ë²„ì „ í™•ì¸ (3.8+ í•„ìš”)
python --version

# 2. PyTorch ë²„ì „ í™•ì¸ (1.9+ í•„ìš”)
python -c "import torch; print(torch.__version__)"

# 3. ìºì‹œ ì •ë¦¬ í›„ ì¬ì„¤ì¹˜
pip cache purge
pip uninstall pytorch-hmm
pip install -e . --force-reinstall

# 4. uv ì‚¬ìš© (ê¶Œì¥)
pip install uv
uv sync
```

### Q2. CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤ê³  ë‚˜ì˜µë‹ˆë‹¤.

**A:** CUDA ì„¤ì •ì„ í™•ì¸í•´ë³´ì„¸ìš”:

```python
import torch

print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
print(f"CUDA ë²„ì „: {torch.version.cuda}")
print(f"GPU ê°œìˆ˜: {torch.cuda.device_count()}")

if torch.cuda.is_available():
    print(f"í˜„ì¬ GPU: {torch.cuda.get_device_name()}")
else:
    print("CUDA ë“œë¼ì´ë²„ë‚˜ PyTorch CUDA ë²„ì „ì„ í™•ì¸í•˜ì„¸ìš”")
```

**í•´ê²° ë°©ë²•:**
- NVIDIA ë“œë¼ì´ë²„ ìµœì‹  ë²„ì „ ì„¤ì¹˜
- PyTorch CUDA ë²„ì „ê³¼ ì‹œìŠ¤í…œ CUDA ë²„ì „ ì¼ì¹˜ í™•ì¸
- `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118`

### Q3. ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤.

**A:** ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì—¬ë³´ì„¸ìš”:

```python
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
batch_size = 8  # 32 ëŒ€ì‹ 

# ê·¸ë˜ë””ì–¸íŠ¸ ë¹„í™œì„±í™”
with torch.no_grad():
    result = hmm.forward(observations)

# GPU ë©”ëª¨ë¦¬ ì •ë¦¬
torch.cuda.empty_cache()

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
if torch.cuda.is_available():
    print(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f}GB")
```

## ğŸ”§ ëª¨ë¸ ì‚¬ìš©

### Q4. HMM ëª¨ë¸ì˜ ìƒíƒœ ìˆ˜ë¥¼ ì–´ë–»ê²Œ ê²°ì •í•˜ë‚˜ìš”?

**A:** ìƒíƒœ ìˆ˜ëŠ” ë°ì´í„°ì˜ íŠ¹ì„±ì— ë”°ë¼ ê²°ì •ë©ë‹ˆë‹¤:

- **ìŒì„± í•©ì„±**: ìŒì†Œ ìˆ˜ (ë³´í†µ 40-60ê°œ)
- **ìŒì„± ì¸ì‹**: ë‹¨ì–´ë‚˜ ìŒì†Œ ë‹¨ìœ„ (ë°ì´í„°ì— ë”°ë¼)
- **ì¼ë°˜ ì‹œê³„ì—´**: ë°ì´í„°ì˜ íŒ¨í„´ ë³µì¡ë„ì— ë”°ë¼

```python
# ìŒì„± í•©ì„± ì˜ˆì œ
num_phonemes = 50  # ì˜ì–´ ìŒì†Œ ìˆ˜
hmm = HMMPyTorch(num_states=num_phonemes)

# êµì°¨ ê²€ì¦ìœ¼ë¡œ ìµœì  ìƒíƒœ ìˆ˜ ì°¾ê¸°
def find_optimal_states(data, state_range=(5, 20)):
    best_score = float('-inf')
    best_states = 5
    
    for num_states in range(*state_range):
        hmm = HMMPyTorch(num_states=num_states)
        score = hmm.score(data)  # ë¡œê·¸ ìš°ë„
        
        if score > best_score:
            best_score = score
            best_states = num_states
    
    return best_states
```

### Q5. Forward-backwardì™€ Viterbiì˜ ì°¨ì´ì ì€?

**A:** ë‘ ì•Œê³ ë¦¬ì¦˜ì˜ ëª©ì ì´ ë‹¤ë¦…ë‹ˆë‹¤:

| ì•Œê³ ë¦¬ì¦˜ | ëª©ì  | ì¶œë ¥ | ì‚¬ìš© ì‹œê¸° |
|----------|------|------|-----------|
| **Forward-backward** | ê° ì‹œì ì˜ ìƒíƒœ í™•ë¥  ê³„ì‚° | ì‚¬í›„ í™•ë¥  ë¶„í¬ | í•™ìŠµ, í™•ë¥ ì  ì¶”ë¡  |
| **Viterbi** | ìµœì  ìƒíƒœ ì‹œí€€ìŠ¤ ì°¾ê¸° | ë‹¨ì¼ ìƒíƒœ ì‹œí€€ìŠ¤ | ë””ì½”ë”©, ì •ë ¬ |

```python
# Forward-backward: í™•ë¥  ë¶„í¬
posteriors, log_likelihood = hmm.forward_backward(observations)
print(f"ê° ì‹œì ì˜ ìƒíƒœ í™•ë¥ : {posteriors.shape}")  # [batch, time, states]

# Viterbi: ìµœì  ê²½ë¡œ
best_path, score = hmm.viterbi_decode(observations)
print(f"ìµœì  ìƒíƒœ ì‹œí€€ìŠ¤: {best_path.shape}")  # [batch, time]
```

### Q6. MixtureGaussianHMMì˜ mixture ìˆ˜ëŠ” ì–´ë–»ê²Œ ì •í•˜ë‚˜ìš”?

**A:** Mixture ìˆ˜ëŠ” ë°ì´í„° ë³µì¡ë„ì— ë”°ë¼ ê²°ì •ë©ë‹ˆë‹¤:

```python
# ì¼ë°˜ì ì¸ ê°€ì´ë“œë¼ì¸
data_complexity_guide = {
    "ê°„ë‹¨í•œ ë°ì´í„°": 1,      # ë‹¨ì¼ ê°€ìš°ì‹œì•ˆ
    "ì¤‘ê°„ ë³µì¡ë„": 2-4,      # ì¼ë°˜ì ì¸ ìŒì„± ë°ì´í„°
    "ë³µì¡í•œ ë°ì´í„°": 5-8,    # ë…¸ì´ì¦ˆê°€ ë§ì€ í™˜ê²½
    "ë§¤ìš° ë³µì¡í•œ ë°ì´í„°": 8+  # ë‹¤ì¤‘ í™”ì, ë‹¤ì–‘í•œ í™˜ê²½
}

# BIC(Bayesian Information Criterion)ë¡œ ìµœì ê°’ ì°¾ê¸°
def find_optimal_mixtures(data, max_mixtures=8):
    bic_scores = []
    
    for n_mix in range(1, max_mixtures + 1):
        hmm = MixtureGaussianHMM(
            num_states=5, 
            obs_dim=data.size(-1), 
            num_mixtures=n_mix
        )
        
        # ê°„ë‹¨í•œ í•™ìŠµ
        for _ in range(10):
            hmm.forward(data)
        
        # BIC ê³„ì‚° (ê°„ì†Œí™”ëœ ë²„ì „)
        log_likelihood = hmm.forward(data).sum()
        n_params = n_mix * data.size(-1) * 2  # í‰ê· , ë¶„ì‚°
        bic = -2 * log_likelihood + n_params * torch.log(torch.tensor(data.numel()))
        bic_scores.append(bic.item())
    
    optimal_mixtures = bic_scores.index(min(bic_scores)) + 1
    return optimal_mixtures
```

## ğŸš€ ì„±ëŠ¥ ìµœì í™”

### Q7. ì‹¤ì‹œê°„ ì²˜ë¦¬ê°€ ë„ˆë¬´ ëŠë¦½ë‹ˆë‹¤.

**A:** ë‹¤ìŒ ìµœì í™” ë°©ë²•ë“¤ì„ ì‹œë„í•´ë³´ì„¸ìš”:

```python
# 1. ë°°ì¹˜ í¬ê¸° ìµœì í™”
def find_optimal_batch_size():
    for batch_size in [1, 2, 4, 8, 16, 32]:
        try:
            data = torch.randn(batch_size, 1000, 80, device='cuda')
            start_time = time.time()
            _ = hmm.forward(data)
            end_time = time.time()
            
            latency = (end_time - start_time) / batch_size * 1000
            print(f"ë°°ì¹˜ í¬ê¸° {batch_size}: {latency:.2f}ms/sample")
            
            if latency < 10:  # 10ms ì´í•˜ë©´ ì‹¤ì‹œê°„ ê°€ëŠ¥
                return batch_size
        except RuntimeError:
            continue
    return 1

# 2. Mixed precision ì‚¬ìš©
with torch.cuda.amp.autocast():
    result = hmm.forward(observations)

# 3. ì²­í¬ í¬ê¸° ì¡°ì •
streaming_processor = StreamingHMMProcessor(
    hmm_model=hmm,
    chunk_size=80,    # ë” ì‘ì€ ì²­í¬ (5ms)
    overlap=40        # ì˜¤ë²„ë© ì¤„ì´ê¸°
)
```

### Q8. GPU ë©”ëª¨ë¦¬ë¥¼ ë” íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ë ¤ë©´?

**A:** ë©”ëª¨ë¦¬ ìµœì í™” ê¸°ë²•ë“¤ì„ ì ìš©í•˜ì„¸ìš”:

```python
# 1. ì¸í”Œë ˆì´ìŠ¤ ì—°ì‚° ì‚¬ìš©
def memory_efficient_forward(hmm, observations):
    # ë©”ëª¨ë¦¬ ë¯¸ë¦¬ í• ë‹¹
    batch_size, seq_len, obs_dim = observations.shape
    result = torch.empty(batch_size, seq_len, hmm.num_states, 
                        device=observations.device)
    
    # ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬
    chunk_size = 100
    for i in range(0, seq_len, chunk_size):
        end_idx = min(i + chunk_size, seq_len)
        chunk = observations[:, i:end_idx]
        
        with torch.no_grad():
            result[:, i:end_idx] = hmm.forward(chunk)
    
    return result

# 2. ê·¸ë˜ë””ì–¸íŠ¸ ì²´í¬í¬ì¸íŒ…
import torch.utils.checkpoint as checkpoint

def checkpointed_forward(hmm, observations):
    return checkpoint.checkpoint(hmm.forward, observations)

# 3. ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
def monitor_memory():
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {allocated:.2f}GB / {reserved:.2f}GB")
```

## ğŸµ ìŒì„± ì²˜ë¦¬

### Q9. ìŒì„± ë°ì´í„° ì „ì²˜ë¦¬ëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?

**A:** ìŒì„± ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸:

```python
import torchaudio
import torchaudio.transforms as transforms

class AudioPreprocessor:
    def __init__(self, sample_rate=16000, n_mels=80):
        self.sample_rate = sample_rate
        self.mel_transform = transforms.MelSpectrogram(
            sample_rate=sample_rate,
            n_mels=n_mels,
            n_fft=1024,
            hop_length=256,
            win_length=1024
        )
        self.log_transform = transforms.AmplitudeToDB()
    
    def preprocess(self, audio_path):
        # ì˜¤ë””ì˜¤ ë¡œë“œ
        waveform, sr = torchaudio.load(audio_path)
        
        # ë¦¬ìƒ˜í”Œë§
        if sr != self.sample_rate:
            resampler = transforms.Resample(sr, self.sample_rate)
            waveform = resampler(waveform)
        
        # ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ë³€í™˜
        mel_spec = self.mel_transform(waveform)
        log_mel = self.log_transform(mel_spec)
        
        # ì •ê·œí™”
        log_mel = (log_mel - log_mel.mean()) / log_mel.std()
        
        return log_mel.transpose(-1, -2)  # [time, mel_dim]

# ì‚¬ìš© ì˜ˆì œ
preprocessor = AudioPreprocessor()
features = preprocessor.preprocess("audio.wav")
```

### Q10. TTS ì‹œìŠ¤í…œì—ì„œ ì •ë ¬ í’ˆì§ˆì„ ë†’ì´ë ¤ë©´?

**A:** ì •ë ¬ í’ˆì§ˆ í–¥ìƒ ë°©ë²•ë“¤:

```python
# 1. ì „ì´ í–‰ë ¬ ì œì•½ ì¶”ê°€
def create_constrained_transitions(num_states, self_loop_prob=0.8):
    """Left-to-right ì œì•½ì´ ìˆëŠ” ì „ì´ í–‰ë ¬"""
    P = torch.zeros(num_states, num_states)
    
    for i in range(num_states):
        if i == num_states - 1:  # ë§ˆì§€ë§‰ ìƒíƒœ
            P[i, i] = 1.0
        else:
            P[i, i] = self_loop_prob        # ìê¸° ìì‹ 
            P[i, i + 1] = 1 - self_loop_prob  # ë‹¤ìŒ ìƒíƒœ
    
    return P

# 2. ì§€ì†ì‹œê°„ ëª¨ë¸ë§ ì¶”ê°€
from pytorch_hmm import SemiMarkovHMM

duration_hmm = SemiMarkovHMM(
    num_states=num_phonemes,
    obs_dim=80,
    max_duration=10  # ìµœëŒ€ ì§€ì†ì‹œê°„
)

# 3. ì •ë ¬ ì •í™•ë„ í‰ê°€
def evaluate_alignment(predicted_alignment, ground_truth):
    """ì •ë ¬ ì •í™•ë„ ê³„ì‚°"""
    correct = (predicted_alignment == ground_truth).float()
    accuracy = correct.mean()
    
    # ê²½ê³„ ì •í™•ë„ (ìŒì†Œ ê²½ê³„ì—ì„œì˜ ì •í™•ë„)
    boundaries = (ground_truth[1:] != ground_truth[:-1]).nonzero()
    boundary_accuracy = correct[boundaries].mean() if len(boundaries) > 0 else 0
    
    return {
        'frame_accuracy': accuracy.item(),
        'boundary_accuracy': boundary_accuracy.item() if isinstance(boundary_accuracy, torch.Tensor) else boundary_accuracy
    }
```

## ğŸ”¬ ê³ ê¸‰ ì‚¬ìš©ë²•

### Q11. ì»¤ìŠ¤í…€ ê´€ì¸¡ ëª¨ë¸ì„ ë§Œë“¤ë ¤ë©´?

**A:** ì‚¬ìš©ì ì •ì˜ ê´€ì¸¡ ëª¨ë¸ êµ¬í˜„:

```python
import torch.nn as nn

class CustomEmissionModel(nn.Module):
    """ì»¤ìŠ¤í…€ ê´€ì¸¡ ëª¨ë¸ ì˜ˆì œ"""
    
    def __init__(self, num_states, obs_dim, hidden_dim=256):
        super().__init__()
        self.num_states = num_states
        self.obs_dim = obs_dim
        
        # ê° ìƒíƒœë³„ ì‹ ê²½ë§
        self.state_networks = nn.ModuleList([
            nn.Sequential(
                nn.Linear(obs_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, 1)
            ) for _ in range(num_states)
        ])
    
    def forward(self, observations):
        """ê´€ì¸¡ í™•ë¥  ê³„ì‚°"""
        batch_size, seq_len, _ = observations.shape
        log_probs = torch.zeros(batch_size, seq_len, self.num_states)
        
        for state in range(self.num_states):
            state_log_probs = self.state_networks[state](observations)
            log_probs[:, :, state] = state_log_probs.squeeze(-1)
        
        return log_probs

# HMMê³¼ í†µí•©
class CustomHMM(nn.Module):
    def __init__(self, num_states, obs_dim):
        super().__init__()
        self.emission_model = CustomEmissionModel(num_states, obs_dim)
        self.transition_matrix = nn.Parameter(
            torch.randn(num_states, num_states)
        )
    
    def forward(self, observations):
        emission_probs = self.emission_model(observations)
        # Forward-backward ì•Œê³ ë¦¬ì¦˜ ì ìš©
        # ... (êµ¬í˜„ ìƒëµ)
        return emission_probs
```

### Q12. ë‹¤ì¤‘ GPUì—ì„œ í•™ìŠµí•˜ë ¤ë©´?

**A:** ë¶„ì‚° í•™ìŠµ ì„¤ì •:

```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# 1. ë¶„ì‚° í™˜ê²½ ì´ˆê¸°í™”
def setup_distributed():
    dist.init_process_group(backend='nccl')
    torch.cuda.set_device(dist.get_rank())

# 2. ëª¨ë¸ì„ DDPë¡œ ë˜í•‘
def create_distributed_model(hmm):
    device = torch.device(f'cuda:{dist.get_rank()}')
    hmm = hmm.to(device)
    ddp_hmm = DDP(hmm, device_ids=[dist.get_rank()])
    return ddp_hmm

# 3. ë¶„ì‚° ë°ì´í„° ë¡œë”
from torch.utils.data.distributed import DistributedSampler

def create_distributed_dataloader(dataset, batch_size):
    sampler = DistributedSampler(dataset)
    dataloader = DataLoader(
        dataset, 
        batch_size=batch_size, 
        sampler=sampler,
        num_workers=4
    )
    return dataloader

# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    setup_distributed()
    
    hmm = MixtureGaussianHMM(num_states=50, obs_dim=80)
    ddp_hmm = create_distributed_model(hmm)
    
    # í•™ìŠµ ë£¨í”„
    for batch in distributed_dataloader:
        loss = ddp_hmm(batch)
        loss.backward()
        optimizer.step()
```

## ğŸš¨ ë¬¸ì œ í•´ê²°

### Q13. "RuntimeError: CUDA out of memory" ì˜¤ë¥˜ í•´ê²°ë²•?

**A:** ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜ í•´ê²° ë‹¨ê³„:

```python
# 1. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
def check_memory_usage():
    if torch.cuda.is_available():
        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        
        print(f"ì´ GPU ë©”ëª¨ë¦¬: {total_memory:.2f}GB")
        print(f"í• ë‹¹ëœ ë©”ëª¨ë¦¬: {allocated:.2f}GB")
        print(f"ì˜ˆì•½ëœ ë©”ëª¨ë¦¬: {reserved:.2f}GB")
        print(f"ì‚¬ìš© ê°€ëŠ¥: {total_memory - reserved:.2f}GB")

# 2. ë©”ëª¨ë¦¬ ìµœì í™” í•¨ìˆ˜
def optimize_memory_usage():
    # ìºì‹œ ì •ë¦¬
    torch.cuda.empty_cache()
    
    # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
    import gc
    gc.collect()
    
    # ë©”ëª¨ë¦¬ ë‹¨í¸í™” ìµœì†Œí™”
    torch.cuda.memory._record_memory_history(enabled=None)

# 3. ë°°ì¹˜ í¬ê¸° ìë™ ì¡°ì •
def auto_adjust_batch_size(model, data_shape, max_batch_size=64):
    for batch_size in [max_batch_size // (2**i) for i in range(6)]:
        try:
            dummy_data = torch.randn(batch_size, *data_shape[1:], device='cuda')
            with torch.no_grad():
                _ = model(dummy_data)
            return batch_size
        except RuntimeError as e:
            if "out of memory" in str(e):
                torch.cuda.empty_cache()
                continue
            else:
                raise e
    return 1
```

### Q14. ëª¨ë¸ ì €ì¥/ë¡œë“œ ì‹œ ì£¼ì˜ì‚¬í•­ì€?

**A:** ëª¨ë¸ ì €ì¥/ë¡œë“œ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤:

```python
# 1. ì•ˆì „í•œ ëª¨ë¸ ì €ì¥
def save_model_safely(model, path, metadata=None):
    checkpoint = {
        'model_state_dict': model.state_dict(),
        'model_config': {
            'num_states': model.num_states,
            'obs_dim': getattr(model, 'obs_dim', None),
            'num_mixtures': getattr(model, 'num_mixtures', None)
        },
        'pytorch_version': torch.__version__,
        'metadata': metadata or {}
    }
    
    # ì›ìì  ì €ì¥ (ì„ì‹œ íŒŒì¼ ì‚¬ìš©)
    temp_path = path + '.tmp'
    torch.save(checkpoint, temp_path)
    
    # ì €ì¥ ì„±ê³µ ì‹œ ì›ë³¸ íŒŒì¼ë¡œ ì´ë™
    import os
    os.rename(temp_path, path)
    print(f"ëª¨ë¸ì´ {path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# 2. ì•ˆì „í•œ ëª¨ë¸ ë¡œë“œ
def load_model_safely(model_class, path, device='cpu'):
    try:
        checkpoint = torch.load(path, map_location=device)
        
        # ì„¤ì • ì •ë³´ë¡œ ëª¨ë¸ ìƒì„±
        config = checkpoint['model_config']
        model = model_class(**config)
        
        # ìƒíƒœ ë”•ì…”ë„ˆë¦¬ ë¡œë“œ
        model.load_state_dict(checkpoint['model_state_dict'])
        model.to(device)
        
        print(f"ëª¨ë¸ì´ {path}ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"PyTorch ë²„ì „: {checkpoint.get('pytorch_version', 'Unknown')}")
        
        return model
    
    except Exception as e:
        print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

# 3. ë²„ì „ í˜¸í™˜ì„± í™•ì¸
def check_compatibility(checkpoint_path):
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    saved_version = checkpoint.get('pytorch_version', 'Unknown')
    current_version = torch.__version__
    
    if saved_version != current_version:
        print(f"âš ï¸ PyTorch ë²„ì „ ë¶ˆì¼ì¹˜: ì €ì¥ë¨({saved_version}) vs í˜„ì¬({current_version})")
        print("ëª¨ë¸ ë™ì‘ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        print("âœ… PyTorch ë²„ì „ í˜¸í™˜")
```

## ğŸ“ ì¶”ê°€ ë„ì›€ë§

### ë” ë§ì€ ì •ë³´ê°€ í•„ìš”í•˜ë‹¤ë©´:

1. **[GitHub Issues](https://github.com/crlotwhite/pytorch_hmm/issues)** - ë²„ê·¸ ì‹ ê³  ë° ê¸°ëŠ¥ ìš”ì²­
2. **[GitHub Discussions](https://github.com/crlotwhite/pytorch_hmm/discussions)** - ì‚¬ìš©ë²• ì§ˆë¬¸
3. **[ë¬¸ì„œ](../README.md)** - ìƒì„¸ ë¬¸ì„œ ë° íŠœí† ë¦¬ì–¼
4. **[ì˜ˆì œ ì½”ë“œ](../examples/)** - ì‹¤ì œ ì‚¬ìš© ì˜ˆì œë“¤

### ì§ˆë¬¸í•˜ê¸° ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸:

- [ ] ìµœì‹  ë²„ì „ì„ ì‚¬ìš©í•˜ê³  ìˆë‚˜ìš”?
- [ ] ì—ëŸ¬ ë©”ì‹œì§€ ì „ë¬¸ì„ í™•ì¸í–ˆë‚˜ìš”?
- [ ] ë¹„ìŠ·í•œ ì´ìŠˆê°€ ì´ë¯¸ ìˆëŠ”ì§€ ê²€ìƒ‰í–ˆë‚˜ìš”?
- [ ] ìµœì†Œí•œì˜ ì¬í˜„ ê°€ëŠ¥í•œ ì˜ˆì œë¥¼ ì¤€ë¹„í–ˆë‚˜ìš”?

---

**ì´ FAQì—ì„œ ë‹µì„ ì°¾ì§€ ëª»í–ˆë‹¤ë©´ ì–¸ì œë“  GitHub Issuesì— ì§ˆë¬¸í•´ì£¼ì„¸ìš”! ğŸ¤** 