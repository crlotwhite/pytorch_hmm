# ğŸš€ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

PyTorch HMMì„ 5ë¶„ ë§Œì— ì‹œì‘í•˜ëŠ” ê°€ì´ë“œì…ë‹ˆë‹¤.

## ğŸ“¦ ì„¤ì¹˜

### ìš”êµ¬ì‚¬í•­
- Python 3.8+
- PyTorch 1.9+
- CUDA (GPU ê°€ì†ìš©, ì„ íƒì‚¬í•­)

### ì„¤ì¹˜ ë°©ë²•

```bash
# 1. ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/crlotwhite/pytorch_hmm.git
cd pytorch_hmm

# 2. ì˜ì¡´ì„± ì„¤ì¹˜ (uv ê¶Œì¥)
pip install uv
uv sync

# ë˜ëŠ” pip ì‚¬ìš©
pip install -e .
```

## ğŸ¯ ì²« ë²ˆì§¸ HMM ëª¨ë¸

### 1. ê¸°ë³¸ HMM ì‚¬ìš©

```python
import torch
from pytorch_hmm import HMMPyTorch

# GPU ì‚¬ìš© ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")

# HMM ëª¨ë¸ ìƒì„± (5ê°œ ìƒíƒœ)
num_states = 5
hmm = HMMPyTorch(num_states=num_states).to(device)

# ê´€ì¸¡ ë°ì´í„° ìƒì„± (ë°°ì¹˜ í¬ê¸°: 2, ì‹œí€€ìŠ¤ ê¸¸ì´: 100)
batch_size, seq_len = 2, 100
observations = torch.randn(batch_size, seq_len, num_states, device=device)

# Forward-backward ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰
posteriors, log_likelihood = hmm.forward_backward(observations)

print(f"Log-likelihood: {log_likelihood}")
print(f"Posterior shape: {posteriors.shape}")  # [batch_size, seq_len, num_states]
```

### 2. Viterbi ë””ì½”ë”©

```python
# ìµœì  ìƒíƒœ ì‹œí€€ìŠ¤ ì°¾ê¸°
best_states, best_scores = hmm.viterbi_decode(observations)

print(f"ìµœì  ìƒíƒœ ì‹œí€€ìŠ¤ (ì²« 10ê°œ): {best_states[0, :10]}")
print(f"ìµœì  ì ìˆ˜: {best_scores[0]}")
```

## ğŸ”¥ ê³ ê¸‰ ëª¨ë¸ ì‚¬ìš©

### 1. MixtureGaussianHMM

```python
from pytorch_hmm import MixtureGaussianHMM

# ê°€ìš°ì‹œì•ˆ í˜¼í•© HMM ìƒì„±
mixture_hmm = MixtureGaussianHMM(
    num_states=5,
    obs_dim=80,        # ìŒì„± íŠ¹ì§• ì°¨ì› (ì˜ˆ: ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨)
    num_mixtures=3     # ê°€ìš°ì‹œì•ˆ í˜¼í•© ìˆ˜
).to(device)

# ìŒì„± íŠ¹ì§• ë°ì´í„° (ì˜ˆ: ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨)
mel_features = torch.randn(batch_size, seq_len, 80, device=device)

# Forward pass
log_probs = mixture_hmm.forward(mel_features)
print(f"Log probabilities shape: {log_probs.shape}")
```

### 2. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°

```python
from pytorch_hmm import StreamingHMMProcessor

# ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í”„ë¡œì„¸ì„œ ìƒì„±
streaming_processor = StreamingHMMProcessor(
    hmm_model=hmm,
    chunk_size=160,    # 10ms @ 16kHz
    overlap=80         # 50% ì˜¤ë²„ë©
)

# ì‹¤ì‹œê°„ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
def simulate_real_time_processing():
    """ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜"""
    total_frames = 1000
    chunk_size = 160
    
    for i in range(0, total_frames, chunk_size):
        # ì˜¤ë””ì˜¤ ì²­í¬ ì‹œë®¬ë ˆì´ì…˜
        audio_chunk = torch.randn(chunk_size, 80, device=device)
        
        # ì‹¤ì‹œê°„ ì²˜ë¦¬
        result = streaming_processor.process_chunk(audio_chunk)
        
        if result is not None:
            print(f"ì²­í¬ {i//chunk_size + 1} ì²˜ë¦¬ ì™„ë£Œ, ê²°ê³¼ shape: {result.shape}")

simulate_real_time_processing()
```

## ğŸµ ìŒì„± í•©ì„± ì˜ˆì œ

### TTS ì‹œìŠ¤í…œ í†µí•©

```python
import torch.nn as nn
from pytorch_hmm import HMMLayer

class SimpleTTSModel(nn.Module):
    """ê°„ë‹¨í•œ TTS ëª¨ë¸ ì˜ˆì œ"""
    
    def __init__(self, text_dim=256, num_phonemes=50, mel_dim=80):
        super().__init__()
        
        # í…ìŠ¤íŠ¸ ì¸ì½”ë”
        self.text_encoder = nn.Sequential(
            nn.Linear(text_dim, 512),
            nn.ReLU(),
            nn.Linear(512, 256)
        )
        
        # HMM ì •ë ¬ ë ˆì´ì–´
        self.hmm_layer = HMMLayer(
            num_states=num_phonemes,
            learnable_transitions=True
        )
        
        # ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ë””ì½”ë”
        self.mel_decoder = nn.Sequential(
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, mel_dim)
        )
    
    def forward(self, text_features):
        # í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        encoded = self.text_encoder(text_features)
        
        # HMM ì •ë ¬
        aligned_features, posteriors = self.hmm_layer(encoded)
        
        # ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ìƒì„±
        mel_output = self.mel_decoder(aligned_features)
        
        return mel_output, posteriors

# ëª¨ë¸ ìƒì„± ë° í…ŒìŠ¤íŠ¸
tts_model = SimpleTTSModel().to(device)

# í…ìŠ¤íŠ¸ íŠ¹ì§• (ì˜ˆ: ìŒì†Œ ì„ë² ë”©)
text_features = torch.randn(batch_size, seq_len, 256, device=device)

# TTS ì‹¤í–‰
mel_output, posteriors = tts_model(text_features)

print(f"ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ shape: {mel_output.shape}")
print(f"ì •ë ¬ í™•ë¥  shape: {posteriors.shape}")
```

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ë¹ ë¥¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

```python
import time

def quick_benchmark():
    """ë¹ ë¥¸ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    # í…ŒìŠ¤íŠ¸ ì„¤ì •
    batch_size, seq_len, obs_dim = 32, 1000, 80
    num_iterations = 100
    
    # ëª¨ë¸ê³¼ ë°ì´í„° ì¤€ë¹„
    hmm = MixtureGaussianHMM(num_states=10, obs_dim=obs_dim).to(device)
    observations = torch.randn(batch_size, seq_len, obs_dim, device=device)
    
    # ì›Œë°ì—…
    for _ in range(10):
        _ = hmm.forward(observations)
    
    # ì„±ëŠ¥ ì¸¡ì •
    torch.cuda.synchronize() if device.type == "cuda" else None
    start_time = time.time()
    
    for _ in range(num_iterations):
        log_probs = hmm.forward(observations)
    
    torch.cuda.synchronize() if device.type == "cuda" else None
    end_time = time.time()
    
    # ê²°ê³¼ ê³„ì‚°
    avg_time = (end_time - start_time) / num_iterations
    throughput = (batch_size * seq_len) / avg_time
    realtime_factor = throughput / 16000  # 16kHz ê¸°ì¤€
    
    print(f"ğŸ“Š ì„±ëŠ¥ ê²°ê³¼:")
    print(f"   â±ï¸  í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_time*1000:.2f}ms")
    print(f"   ğŸš€ ì²˜ë¦¬ëŸ‰: {throughput:.0f} frames/sec")
    print(f"   âš¡ ì‹¤ì‹œê°„ ë°°ìˆ˜: {realtime_factor:.1f}x")
    
    if realtime_factor > 100:
        print(f"   âœ… ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥! ({realtime_factor:.0f}x ì‹¤ì‹œê°„)")
    else:
        print(f"   âš ï¸ ì‹¤ì‹œê°„ ì²˜ë¦¬ ì–´ë ¤ì›€ ({realtime_factor:.1f}x)")

# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
quick_benchmark()
```

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### 1. GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
```python
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
batch_size = 8  # 32 ëŒ€ì‹  8 ì‚¬ìš©

# ë˜ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ì‚¬ìš©
with torch.no_grad():
    result = hmm.forward(observations)
```

#### 2. ëŠë¦° ì²˜ë¦¬ ì†ë„
```python
# Mixed precision ì‚¬ìš©
with torch.cuda.amp.autocast():
    result = hmm.forward(observations)

# JIT ì»´íŒŒì¼ (ê°€ëŠ¥í•œ ê²½ìš°)
try:
    jit_hmm = torch.jit.script(hmm)
    print("âœ… JIT ì»´íŒŒì¼ ì„±ê³µ")
except:
    print("âŒ JIT ì»´íŒŒì¼ ì‹¤íŒ¨, ì¼ë°˜ ëª¨ë¸ ì‚¬ìš©")
    jit_hmm = hmm
```

#### 3. ì„¤ì¹˜ ë¬¸ì œ
```bash
# CUDA ë²„ì „ í™•ì¸
python -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.version.cuda}')"

# ì˜ì¡´ì„± ì¬ì„¤ì¹˜
pip uninstall pytorch-hmm
pip install -e . --force-reinstall
```

## ğŸ“š ë‹¤ìŒ ë‹¨ê³„

### ë” ìì„¸í•œ í•™ìŠµ
1. **[ê¸°ë³¸ HMM ì‚¬ìš©ë²•](basic_hmm.md)** - HMM ì´ë¡ ê³¼ ìƒì„¸ ì‚¬ìš©ë²•
2. **[ê³ ê¸‰ ëª¨ë¸ í™œìš©](advanced_models.md)** - Neural HMM, HSMM ë“±
3. **[ì„±ëŠ¥ ìµœì í™”](optimization.md)** - GPU ê°€ì†, ë©”ëª¨ë¦¬ ìµœì í™”
4. **[ì‹¤ì œ ì‘ìš©](../examples/integration_examples.md)** - ì‹¤ì œ í”„ë¡œì íŠ¸ í†µí•©

### ì˜ˆì œ ì½”ë“œ
- **[ê¸°ë³¸ ì˜ˆì œ](../examples/basic_examples.md)** - ë‹¤ì–‘í•œ ê¸°ë³¸ ì‚¬ìš© ì˜ˆì œ
- **[ë²¤ì¹˜ë§ˆí¬ ì˜ˆì œ](../examples/benchmark_examples.md)** - ì„±ëŠ¥ ì¸¡ì • ì½”ë“œ

### ë„ì›€ë§
- **[FAQ](../troubleshooting/faq.md)** - ìì£¼ ë¬»ëŠ” ì§ˆë¬¸
- **[GitHub Issues](https://github.com/crlotwhite/pytorch_hmm/issues)** - ë²„ê·¸ ì‹ ê³  ë° ì§ˆë¬¸

---

**ì¶•í•˜í•©ë‹ˆë‹¤! ğŸ‰ PyTorch HMM ê¸°ë³¸ ì‚¬ìš©ë²•ì„ ìµí˜”ìŠµë‹ˆë‹¤. ì´ì œ ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì„ íƒí—˜í•´ë³´ì„¸ìš”!** 