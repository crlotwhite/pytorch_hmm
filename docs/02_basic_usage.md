# 2. ê¸°ë³¸ ì‚¬ìš©ë²•

ì´ ë¬¸ì„œì—ì„œëŠ” PyTorch HMM ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ê¸°ë³¸ ì‚¬ìš©ë²•ì„ ë‹¨ê³„ë³„ë¡œ í•™ìŠµí•©ë‹ˆë‹¤. ì„¤ì¹˜ë¶€í„° ì²« ë²ˆì§¸ ëª¨ë¸ ìƒì„±, ì¶”ë¡ , ì‹ ê²½ë§ í†µí•©ê¹Œì§€ ì‹¤ì œ ì½”ë“œì™€ í•¨ê»˜ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“š ëª©ì°¨

1. [ì„¤ì¹˜ ë° ì„¤ì •](#1-ì„¤ì¹˜-ë°-ì„¤ì •)
2. [ì²« ë²ˆì§¸ HMM ëª¨ë¸](#2-ì²«-ë²ˆì§¸-hmm-ëª¨ë¸)
3. [Forward-Backward vs Viterbi](#3-forward-backward-vs-viterbi)
4. [ì‹ ê²½ë§ê³¼ì˜ í†µí•©](#4-ì‹ ê²½ë§ê³¼ì˜-í†µí•©)
5. [ë°°ì¹˜ ì²˜ë¦¬](#5-ë°°ì¹˜-ì²˜ë¦¬)
6. [GPU ì‚¬ìš©ë²•](#6-gpu-ì‚¬ìš©ë²•)
7. [ì‹¤ì œ ìŒì„± ë°ì´í„° ì˜ˆì œ](#7-ì‹¤ì œ-ìŒì„±-ë°ì´í„°-ì˜ˆì œ)

## 1. ì„¤ì¹˜ ë° ì„¤ì •

### 1.1 ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

```bash
# Python ë²„ì „ í™•ì¸
python --version  # Python 3.8+ í•„ìš”

# PyTorch ì„¤ì¹˜ (CUDA ì§€ì›)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# ê¸°ë³¸ ì˜ì¡´ì„±
pip install numpy matplotlib scipy
```

### 1.2 PyTorch HMM ì„¤ì¹˜

```bash
# ê°œë°œ ë²„ì „ ì„¤ì¹˜ (ê¶Œì¥)
git clone https://github.com/your-repo/pytorch_hmm.git
cd pytorch_hmm
pip install -e .

# ë˜ëŠ” PyPIì—ì„œ ì„¤ì¹˜ (í–¥í›„)
# pip install pytorch-hmm
```

### 1.3 ì„¤ì¹˜ í™•ì¸

```python
import torch
from pytorch_hmm import HMMPyTorch, HMMLayer, create_left_to_right_matrix

print(f"PyTorch ë²„ì „: {torch.__version__}")
print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
print("PyTorch HMM ë¼ì´ë¸ŒëŸ¬ë¦¬ ì •ìƒ ì„¤ì¹˜ë¨!")
```

## 2. ì²« ë²ˆì§¸ HMM ëª¨ë¸

### 2.1 ê¸°ë³¸ HMM ìƒì„±

```python
import torch
from pytorch_hmm import HMMPyTorch, create_left_to_right_matrix

# 1. ì „ì´ í–‰ë ¬ ìƒì„±
num_states = 5
transition_matrix = create_left_to_right_matrix(
    num_states, 
    self_loop_prob=0.7  # ìê¸° ìì‹ ìœ¼ë¡œì˜ ì „ì´ í™•ë¥ 
)

print("ì „ì´ í–‰ë ¬:")
print(transition_matrix)
```

**ì¶œë ¥:**
```
ì „ì´ í–‰ë ¬:
tensor([[0.7000, 0.3000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.7000, 0.3000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.7000, 0.3000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.7000, 0.3000],
        [0.0000, 0.0000, 0.0000, 0.0000, 1.0000]])
```

### 2.2 HMM ëª¨ë¸ ì´ˆê¸°í™”

```python
# 2. HMM ëª¨ë¸ ìƒì„±
hmm = HMMPyTorch(transition_matrix)

print(f"ìƒíƒœ ìˆ˜: {hmm.K}")
print(f"ë””ë°”ì´ìŠ¤: {hmm.device}")
print(f"ì „ì´ í–‰ë ¬ í˜•íƒœ: {hmm.A.shape}")
```

### 2.3 ê´€ì¸¡ ë°ì´í„° ì¤€ë¹„

```python
# 3. ê´€ì¸¡ ë°ì´í„° ìƒì„± (ì˜ˆ: ìŒì„± íŠ¹ì§•)
batch_size = 2
seq_len = 100
feature_dim = num_states  # ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ìƒíƒœ ìˆ˜ì™€ ë™ì¼

# í™•ë¥ ì  ê´€ì¸¡ ë°ì´í„° (ê° ì‹œì ì—ì„œ ê° ìƒíƒœì— ëŒ€í•œ í™•ë¥ )
observations = torch.softmax(
    torch.randn(batch_size, seq_len, num_states), 
    dim=-1
)

print(f"ê´€ì¸¡ ë°ì´í„° í˜•íƒœ: {observations.shape}")
print(f"í™•ë¥  í•© í™•ì¸: {observations[0, 0].sum()}")  # 1.0ì´ì–´ì•¼ í•¨
```

### 2.4 ê¸°ë³¸ ì¶”ë¡ 

```python
# 4. Forward-backward ì•Œê³ ë¦¬ì¦˜
posteriors, forward, backward = hmm.forward_backward(observations)

print(f"í›„ë°©í™•ë¥  í˜•íƒœ: {posteriors.shape}")
print(f"ì²« ë²ˆì§¸ ì‹œì  í›„ë°©í™•ë¥  í•©: {posteriors[0, 0].sum()}")

# 5. Viterbi ë””ì½”ë”©
optimal_states, scores = hmm.viterbi_decode(observations)

print(f"ìµœì  ìƒíƒœ ì‹œí€€ìŠ¤ í˜•íƒœ: {optimal_states.shape}")
print(f"ì²« 10ê°œ ìƒíƒœ: {optimal_states[0, :10].tolist()}")
```

## 3. Forward-Backward vs Viterbi

ë‘ ì•Œê³ ë¦¬ì¦˜ì˜ ì°¨ì´ì ê³¼ ì‚¬ìš© ì‹œê¸°ë¥¼ ì‹¤ì œ ì˜ˆì œë¡œ ë¹„êµí•´ë³´ê² ìŠµë‹ˆë‹¤.

### 3.1 ë¹„êµ ì‹¤í—˜ ì„¤ì •

```python
import time
import matplotlib.pyplot as plt

def compare_algorithms():
    # ë” ê¸´ ì‹œí€€ìŠ¤ë¡œ í…ŒìŠ¤íŠ¸
    num_states = 6
    seq_len = 200
    
    P = create_left_to_right_matrix(num_states, self_loop_prob=0.8)
    hmm = HMMPyTorch(P)
    
    # ê´€ì¸¡ ë°ì´í„°
    observations = torch.softmax(
        torch.randn(1, seq_len, num_states), dim=-1
    )
    
    print(f"í…ŒìŠ¤íŠ¸ ì„¤ì •: {num_states}ê°œ ìƒíƒœ, {seq_len} ê¸¸ì´ ì‹œí€€ìŠ¤")
    return hmm, observations

hmm, observations = compare_algorithms()
```

### 3.2 Forward-Backward ë¶„ì„

```python
# Forward-Backward ì‹¤í–‰
print("\n=== Forward-Backward ì•Œê³ ë¦¬ì¦˜ ===")
start_time = time.time()
posteriors, forward, backward = hmm.forward_backward(observations)
fb_time = time.time() - start_time

# ì†Œí”„íŠ¸ ì •ë ¬ (ê°€ì¥ í™•ë¥  ë†’ì€ ìƒíƒœ)
soft_alignment = torch.argmax(posteriors, dim=-1)[0]

print(f"ì‹¤í–‰ ì‹œê°„: {fb_time:.4f}ì´ˆ")
print(f"ì¶œë ¥: í™•ë¥ ì  í›„ë°©í™•ë¥  (soft alignment)")
print(f"ì†Œí”„íŠ¸ ì •ë ¬ (ì²˜ìŒ 15ê°œ): {soft_alignment[:15].tolist()}")

# ë¶ˆí™•ì‹¤ì„± ë¶„ì„
uncertainty = -torch.sum(posteriors * torch.log(posteriors + 1e-8), dim=-1)
print(f"í‰ê·  ë¶ˆí™•ì‹¤ì„±: {uncertainty.mean():.3f}")
```

### 3.3 Viterbi ë¶„ì„

```python
# Viterbi ì‹¤í–‰
print("\n=== Viterbi ì•Œê³ ë¦¬ì¦˜ ===")
start_time = time.time()
hard_alignment, scores = hmm.viterbi_decode(observations)
viterbi_time = time.time() - start_time

print(f"ì‹¤í–‰ ì‹œê°„: {viterbi_time:.4f}ì´ˆ")
print(f"ì¶œë ¥: ê²°ì •ì  ì •ë ¬ (hard alignment)")
print(f"í•˜ë“œ ì •ë ¬ (ì²˜ìŒ 15ê°œ): {hard_alignment[0, :15].tolist()}")
print(f"ìµœì  ê²½ë¡œ ì ìˆ˜: {scores[0]:.3f}")
```

### 3.4 ê²°ê³¼ ë¹„êµ ë° ë¶„ì„

```python
# ì •ë ¬ ì¼ì¹˜ë„ ë¶„ì„
agreement = (soft_alignment == hard_alignment[0]).float().mean()
print(f"\n=== ë¹„êµ ê²°ê³¼ ===")
print(f"ì†ë„ ë¹„ìœ¨ (Viterbi/FB): {viterbi_time/fb_time:.2f}x")
print(f"ì •ë ¬ ì¼ì¹˜ë„: {agreement:.3f} ({agreement*100:.1f}%)")

# ìƒíƒœ ì§€ì†ì‹œê°„ ë¹„êµ
from pytorch_hmm.utils import compute_state_durations

soft_durations = compute_state_durations(soft_alignment)
hard_durations = compute_state_durations(hard_alignment[0])

print(f"í‰ê·  ì§€ì†ì‹œê°„ (soft): {soft_durations.float().mean():.2f}")
print(f"í‰ê·  ì§€ì†ì‹œê°„ (hard): {hard_durations.float().mean():.2f}")
```

### 3.5 ì‹œê°í™”

```python
def visualize_alignments(soft_align, hard_align, seq_len=50):
    """ì •ë ¬ ê²°ê³¼ ì‹œê°í™”"""
    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 8))
    
    # ì†Œí”„íŠ¸ ì •ë ¬
    ax1.plot(soft_align[:seq_len].numpy(), 'b-', linewidth=2, label='Soft Alignment')
    ax1.set_title('Forward-Backward (Soft Alignment)')
    ax1.set_ylabel('State')
    ax1.grid(True, alpha=0.3)
    
    # í•˜ë“œ ì •ë ¬
    ax2.plot(hard_align[:seq_len].numpy(), 'r-', linewidth=2, label='Hard Alignment')
    ax2.set_title('Viterbi (Hard Alignment)')
    ax2.set_ylabel('State')
    ax2.grid(True, alpha=0.3)
    
    # ì°¨ì´ì  í‘œì‹œ
    diff = (soft_align != hard_align).float()[:seq_len]
    ax3.fill_between(range(seq_len), diff.numpy(), alpha=0.5, color='orange')
    ax3.set_title('Alignment Differences')
    ax3.set_xlabel('Time Frame')
    ax3.set_ylabel('Different')
    ax3.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

# ì‹œê°í™” ì‹¤í–‰
visualize_alignments(soft_alignment, hard_alignment[0])
```

### 3.6 ì–¸ì œ ì–´ë–¤ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í• ê¹Œ?

```python
print("\n=== ì‚¬ìš© ê°€ì´ë“œ ===")
print("ğŸ“Š Forward-Backwardë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°:")
print("   â€¢ ëª¨ë¸ í•™ìŠµ (gradient ê³„ì‚° í•„ìš”)")
print("   â€¢ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”ê°€ ì¤‘ìš”í•œ ê²½ìš°")
print("   â€¢ ì—¬ëŸ¬ ê°€ëŠ¥ì„±ì„ ê³ ë ¤í•œ ì†Œí”„íŠ¸ ê²°ì •")
print("   â€¢ ì•™ìƒë¸”ì´ë‚˜ ìœµí•©ì—ì„œ í™•ë¥  ì •ë³´ í™œìš©")

print("\nğŸ¯ Viterbië¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°:")
print("   â€¢ ìµœì¢… ì¶”ë¡  (ëª…í™•í•œ ê²°ì • í•„ìš”)")
print("   â€¢ ì‹¤ì‹œê°„ ì²˜ë¦¬ (ì†ë„ ìš°ì„ )")
print("   â€¢ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì´ ì¤‘ìš”í•œ ê²½ìš°")
print("   â€¢ ëª…í™•í•œ ê²½ê³„ ê²€ì¶œì´ í•„ìš”í•œ ê²½ìš°")
```

## 4. ì‹ ê²½ë§ê³¼ì˜ í†µí•©

PyTorch HMMì˜ ê°€ì¥ ê°•ë ¥í•œ ê¸°ëŠ¥ ì¤‘ í•˜ë‚˜ëŠ” ì‹ ê²½ë§ê³¼ì˜ ìì—°ìŠ¤ëŸ¬ìš´ í†µí•©ì…ë‹ˆë‹¤.

### 4.1 HMMLayer ê¸°ë³¸ ì‚¬ìš©ë²•

```python
import torch.nn as nn
from pytorch_hmm import HMMLayer

class SimpleAlignmentModel(nn.Module):
    def __init__(self, input_dim, num_states, hidden_dim=128):
        super().__init__()
        
        # íŠ¹ì§• ì¸ì½”ë”
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # HMM ì •ë ¬ ë ˆì´ì–´
        self.hmm_layer = HMMLayer(
            num_states=num_states,
            learnable_transitions=True,  # ì „ì´ í™•ë¥  í•™ìŠµ ê°€ëŠ¥
            transition_type="left_to_right",
            viterbi_inference=False  # í•™ìŠµ ì‹œ soft alignment
        )
        
        # ì¶œë ¥ ë””ì½”ë”
        self.decoder = nn.Linear(hidden_dim, input_dim)
    
    def forward(self, x, return_alignment=False):
        # 1. íŠ¹ì§• ì¸ì½”ë”©
        encoded = self.encoder(x)
        
        # 2. HMM ì •ë ¬
        aligned_features, posteriors = self.hmm_layer(encoded)
        
        # 3. ë””ì½”ë”©
        output = self.decoder(aligned_features)
        
        if return_alignment:
            return output, posteriors
        return output

# ëª¨ë¸ ìƒì„± ë° í…ŒìŠ¤íŠ¸
input_dim = 80  # ì˜ˆ: ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì°¨ì›
num_states = 10  # ìŒì†Œë³„ ìƒíƒœ ìˆ˜
model = SimpleAlignmentModel(input_dim, num_states)

# í…ŒìŠ¤íŠ¸ ë°ì´í„°
batch_size, seq_len = 4, 150
test_input = torch.randn(batch_size, seq_len, input_dim)

# Forward pass
output, alignment = model(test_input, return_alignment=True)

print(f"ì…ë ¥ í˜•íƒœ: {test_input.shape}")
print(f"ì¶œë ¥ í˜•íƒœ: {output.shape}")
print(f"ì •ë ¬ í˜•íƒœ: {alignment.shape}")
```

### 4.2 ìŒì„± í•©ì„± ëª¨ë¸ ì˜ˆì œ

```python
class TTSAlignmentModel(nn.Module):
    """Text-to-Speech ì •ë ¬ ëª¨ë¸"""
    
    def __init__(self, text_vocab_size, num_phonemes, mel_dim=80):
        super().__init__()
        
        # í…ìŠ¤íŠ¸ ì„ë² ë”©
        self.text_embedding = nn.Embedding(text_vocab_size, 256)
        
        # í…ìŠ¤íŠ¸ ì¸ì½”ë”
        self.text_encoder = nn.LSTM(
            input_size=256,
            hidden_size=256,
            num_layers=2,
            batch_first=True,
            bidirectional=True
        )
        
        # HMM ì •ë ¬ (ìŒì†Œë³„)
        self.hmm_layers = nn.ModuleList([
            HMMLayer(
                num_states=3,  # ê° ìŒì†Œë‹¹ 3ê°œ ìƒíƒœ
                learnable_transitions=True,
                transition_type="left_to_right"
            ) for _ in range(num_phonemes)
        ])
        
        # ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ë””ì½”ë”
        self.mel_decoder = nn.Sequential(
            nn.Linear(512, 512),  # bidirectional LSTM output
            nn.ReLU(),
            nn.Linear(512, mel_dim)
        )
        
        self.num_phonemes = num_phonemes
    
    def forward(self, text_tokens, phoneme_ids, target_length=None):
        batch_size, text_len = text_tokens.shape
        
        # 1. í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        embedded = self.text_embedding(text_tokens)
        encoded, _ = self.text_encoder(embedded)
        
        # 2. ìŒì†Œë³„ HMM ì •ë ¬
        aligned_features = []
        all_alignments = []
        
        for i, phoneme_id in enumerate(phoneme_ids.unique()):
            # í•´ë‹¹ ìŒì†Œì˜ íŠ¹ì§• ì¶”ì¶œ
            phoneme_mask = (phoneme_ids == phoneme_id)
            if phoneme_mask.sum() == 0:
                continue
                
            phoneme_features = encoded[phoneme_mask]
            
            # HMM ì •ë ¬
            aligned, alignment = self.hmm_layers[phoneme_id](
                phoneme_features.unsqueeze(0)
            )
            
            aligned_features.append(aligned.squeeze(0))
            all_alignments.append(alignment.squeeze(0))
        
        # 3. íŠ¹ì§• ì—°ê²°
        if aligned_features:
            combined_features = torch.cat(aligned_features, dim=0)
        else:
            combined_features = encoded.mean(dim=1, keepdim=True)
        
        # 4. ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ìƒì„±
        mel_output = self.mel_decoder(combined_features)
        
        return mel_output, all_alignments

# ì‚¬ìš© ì˜ˆì œ
vocab_size = 1000
num_phonemes = 40
model = TTSAlignmentModel(vocab_size, num_phonemes)

# ìƒ˜í”Œ ë°ì´í„°
text_tokens = torch.randint(0, vocab_size, (2, 20))  # ë°°ì¹˜ í¬ê¸° 2, ê¸¸ì´ 20
phoneme_ids = torch.randint(0, num_phonemes, (20,))  # ìŒì†Œ ID

mel_output, alignments = model(text_tokens, phoneme_ids)
print(f"ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì¶œë ¥: {mel_output.shape}")
```

### 4.3 í•™ìŠµ ê°€ëŠ¥í•œ ì „ì´ í–‰ë ¬

```python
class LearnableTransitionHMM(nn.Module):
    def __init__(self, num_states, feature_dim):
        super().__init__()
        
        # í•™ìŠµ ê°€ëŠ¥í•œ ì „ì´ í–‰ë ¬ íŒŒë¼ë¯¸í„°
        self.transition_logits = nn.Parameter(
            torch.randn(num_states, num_states)
        )
        
        # ê´€ì¸¡ ëª¨ë¸
        self.observation_model = nn.Linear(feature_dim, num_states)
        
        self.num_states = num_states
    
    def get_transition_matrix(self):
        """ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ ì •ê·œí™”ëœ ì „ì´ í–‰ë ¬ ë°˜í™˜"""
        return torch.softmax(self.transition_logits, dim=-1)
    
    def forward(self, features):
        # 1. ê´€ì¸¡ í™•ë¥  ê³„ì‚°
        obs_logits = self.observation_model(features)
        obs_probs = torch.softmax(obs_logits, dim=-1)
        
        # 2. ì „ì´ í–‰ë ¬ ì–»ê¸°
        transition_matrix = self.get_transition_matrix()
        
        # 3. HMM ìƒì„± ë° ì¶”ë¡ 
        hmm = HMMPyTorch(transition_matrix)
        posteriors, _, _ = hmm.forward_backward(obs_probs)
        
        return posteriors, transition_matrix

# í•™ìŠµ ì˜ˆì œ
model = LearnableTransitionHMM(num_states=5, feature_dim=80)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# ìƒ˜í”Œ ë°ì´í„°
features = torch.randn(2, 100, 80)
target_alignment = torch.randint(0, 5, (2, 100))

# í•™ìŠµ ë£¨í”„
for epoch in range(10):
    optimizer.zero_grad()
    
    posteriors, trans_matrix = model(features)
    
    # ì†ì‹¤ ê³„ì‚° (ì˜ˆ: í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼)
    loss = nn.CrossEntropyLoss()(
        posteriors.reshape(-1, 5),
        target_alignment.reshape(-1)
    )
    
    loss.backward()
    optimizer.step()
    
    if epoch % 5 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

print("í•™ìŠµëœ ì „ì´ í–‰ë ¬:")
print(model.get_transition_matrix())
```

## 5. ë°°ì¹˜ ì²˜ë¦¬

íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬ëŠ” ì‹¤ì œ ì‘ìš©ì—ì„œ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.

### 5.1 ê¸°ë³¸ ë°°ì¹˜ ì²˜ë¦¬

```python
def batch_processing_example():
    # ë‹¤ì–‘í•œ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ë“¤
    sequences = [
        torch.randn(50, 5),   # 50 í”„ë ˆì„
        torch.randn(75, 5),   # 75 í”„ë ˆì„
        torch.randn(100, 5),  # 100 í”„ë ˆì„
        torch.randn(60, 5),   # 60 í”„ë ˆì„
    ]
    
    print("ì›ë³¸ ì‹œí€€ìŠ¤ ê¸¸ì´:", [len(seq) for seq in sequences])
    
    # íŒ¨ë”©ì„ í†µí•œ ë°°ì¹˜ ìƒì„±
    from torch.nn.utils.rnn import pad_sequence
    
    # íŒ¨ë”© (ë°°ì¹˜ ìš°ì„ )
    padded_batch = pad_sequence(sequences, batch_first=True, padding_value=0)
    
    # ê¸¸ì´ ì •ë³´ ì €ì¥
    lengths = torch.tensor([len(seq) for seq in sequences])
    
    print(f"íŒ¨ë”©ëœ ë°°ì¹˜ í˜•íƒœ: {padded_batch.shape}")
    print(f"ì‹¤ì œ ê¸¸ì´: {lengths.tolist()}")
    
    return padded_batch, lengths

padded_batch, lengths = batch_processing_example()
```

### 5.2 ë§ˆìŠ¤í‚¹ì„ í†µí•œ íš¨ìœ¨ì  ì²˜ë¦¬

```python
def masked_hmm_processing(observations, lengths):
    """ë§ˆìŠ¤í‚¹ì„ ì‚¬ìš©í•œ HMM ë°°ì¹˜ ì²˜ë¦¬"""
    
    batch_size, max_len, num_states = observations.shape
    
    # 1. ë§ˆìŠ¤í¬ ìƒì„±
    mask = torch.arange(max_len).unsqueeze(0) < lengths.unsqueeze(1)
    print(f"ë§ˆìŠ¤í¬ í˜•íƒœ: {mask.shape}")
    
    # 2. HMM ëª¨ë¸
    P = create_left_to_right_matrix(num_states, self_loop_prob=0.8)
    hmm = HMMPyTorch(P)
    
    # 3. ë§ˆìŠ¤í‚¹ëœ ê´€ì¸¡ í™•ë¥ 
    masked_obs = observations * mask.unsqueeze(-1).float()
    
    # 4. Forward-backward (ë§ˆìŠ¤í‚¹ ê³ ë ¤)
    posteriors, forward, backward = hmm.forward_backward(masked_obs)
    
    # 5. ë§ˆìŠ¤í‚¹ëœ ê²°ê³¼ë§Œ ì‚¬ìš©
    masked_posteriors = posteriors * mask.unsqueeze(-1).float()
    
    return masked_posteriors, mask

# ì‹¤í–‰
masked_posteriors, mask = masked_hmm_processing(
    torch.softmax(padded_batch, dim=-1), lengths
)

print(f"ë§ˆìŠ¤í‚¹ëœ í›„ë°©í™•ë¥  í˜•íƒœ: {masked_posteriors.shape}")

# ì‹¤ì œ ì‹œí€€ìŠ¤ë³„ ê²°ê³¼ ì¶”ì¶œ
for i, length in enumerate(lengths):
    seq_posteriors = masked_posteriors[i, :length]
    print(f"ì‹œí€€ìŠ¤ {i} í›„ë°©í™•ë¥  í˜•íƒœ: {seq_posteriors.shape}")
```

### 5.3 ë™ì  ë°°ì¹˜ í¬ê¸° ì²˜ë¦¬

```python
class DynamicBatchHMM(nn.Module):
    def __init__(self, num_states):
        super().__init__()
        self.num_states = num_states
        
        # í•™ìŠµ ê°€ëŠ¥í•œ HMM ë ˆì´ì–´
        self.hmm_layer = HMMLayer(
            num_states=num_states,
            learnable_transitions=True
        )
    
    def forward(self, observations, lengths=None):
        batch_size = observations.size(0)
        
        if lengths is None:
            # ëª¨ë“  ì‹œí€€ìŠ¤ê°€ ê°™ì€ ê¸¸ì´
            return self.hmm_layer(observations)
        
        # ê¸¸ì´ê°€ ë‹¤ë¥¸ ê²½ìš° ê°œë³„ ì²˜ë¦¬ í›„ ê²°í•©
        results = []
        alignments = []
        
        for i in range(batch_size):
            seq_len = lengths[i]
            seq_obs = observations[i, :seq_len].unsqueeze(0)
            
            aligned, alignment = self.hmm_layer(seq_obs)
            
            results.append(aligned.squeeze(0))
            alignments.append(alignment.squeeze(0))
        
        # ë‹¤ì‹œ íŒ¨ë”©í•˜ì—¬ ë°°ì¹˜ë¡œ ë§Œë“¤ê¸°
        from torch.nn.utils.rnn import pad_sequence
        
        padded_results = pad_sequence(results, batch_first=True)
        padded_alignments = pad_sequence(alignments, batch_first=True)
        
        return padded_results, padded_alignments

# ì‚¬ìš© ì˜ˆì œ
dynamic_hmm = DynamicBatchHMM(num_states=5)
aligned_features, alignments = dynamic_hmm(padded_batch, lengths)

print(f"ì •ë ¬ëœ íŠ¹ì§• í˜•íƒœ: {aligned_features.shape}")
print(f"ì •ë ¬ ì •ë³´ í˜•íƒœ: {alignments.shape}")
```

## 6. GPU ì‚¬ìš©ë²•

GPU ê°€ì†ì„ í†µí•´ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 6.1 ê¸°ë³¸ GPU ì„¤ì •

```python
def setup_gpu():
    """GPU ì„¤ì • ë° í™•ì¸"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    if torch.cuda.is_available():
        print(f"GPU ì´ë¦„: {torch.cuda.get_device_name()}")
        print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
        print(f"í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1e6:.1f} MB")
    
    return device

device = setup_gpu()
```

### 6.2 GPU HMM ì²˜ë¦¬

```python
def gpu_hmm_example():
    """GPUì—ì„œ HMM ì²˜ë¦¬ ì˜ˆì œ"""
    
    # í° ë°°ì¹˜ í¬ê¸°ì™€ ê¸´ ì‹œí€€ìŠ¤ë¡œ í…ŒìŠ¤íŠ¸
    batch_size = 32
    seq_len = 500
    num_states = 10
    
    print(f"í…ŒìŠ¤íŠ¸ ì„¤ì •: ë°°ì¹˜ í¬ê¸° {batch_size}, ì‹œí€€ìŠ¤ ê¸¸ì´ {seq_len}")
    
    # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™
    observations = torch.softmax(
        torch.randn(batch_size, seq_len, num_states), dim=-1
    ).to(device)
    
    # HMM ëª¨ë¸ë„ GPUë¡œ ì´ë™
    P = create_left_to_right_matrix(num_states, self_loop_prob=0.8)
    hmm = HMMPyTorch(P.to(device))
    
    print(f"ê´€ì¸¡ ë°ì´í„° ë””ë°”ì´ìŠ¤: {observations.device}")
    print(f"HMM ëª¨ë¸ ë””ë°”ì´ìŠ¤: {hmm.device}")
    
    # GPUì—ì„œ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
    if device.type == 'cuda':
        torch.cuda.synchronize()  # GPU ë™ê¸°í™”
    
    start_time = time.time()
    
    # Forward-backward
    posteriors, forward, backward = hmm.forward_backward(observations)
    
    if device.type == 'cuda':
        torch.cuda.synchronize()
    
    gpu_time = time.time() - start_time
    
    print(f"GPU ì²˜ë¦¬ ì‹œê°„: {gpu_time:.4f}ì´ˆ")
    print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1e6:.1f} MB")
    
    return posteriors

gpu_posteriors = gpu_hmm_example()
```

### 6.3 ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬

```python
def memory_efficient_processing():
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì  HMM ì²˜ë¦¬"""
    
    # ë§¤ìš° í° ë°ì´í„°ì…‹ ì‹œë®¬ë ˆì´ì…˜
    total_sequences = 1000
    chunk_size = 50  # í•œ ë²ˆì— ì²˜ë¦¬í•  ì‹œí€€ìŠ¤ ìˆ˜
    
    num_states = 8
    seq_len = 200
    
    P = create_left_to_right_matrix(num_states).to(device)
    hmm = HMMPyTorch(P)
    
    all_results = []
    
    print(f"ì´ {total_sequences}ê°œ ì‹œí€€ìŠ¤ë¥¼ {chunk_size}ê°œì”© ì²­í¬ë¡œ ì²˜ë¦¬")
    
    for chunk_start in range(0, total_sequences, chunk_size):
        chunk_end = min(chunk_start + chunk_size, total_sequences)
        current_chunk_size = chunk_end - chunk_start
        
        # ì²­í¬ ë°ì´í„° ìƒì„±
        chunk_obs = torch.softmax(
            torch.randn(current_chunk_size, seq_len, num_states), dim=-1
        ).to(device)
        
        # ì²˜ë¦¬
        with torch.no_grad():  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´
            posteriors, _, _ = hmm.forward_backward(chunk_obs)
            
            # CPUë¡œ ì´ë™í•˜ì—¬ ì €ì¥ (GPU ë©”ëª¨ë¦¬ ì ˆì•½)
            all_results.append(posteriors.cpu())
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if device.type == 'cuda':
            torch.cuda.empty_cache()
        
        if chunk_start % (chunk_size * 5) == 0:
            print(f"ì²˜ë¦¬ ì™„ë£Œ: {chunk_end}/{total_sequences}")
    
    print("ëª¨ë“  ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ")
    return all_results

# ì‹¤í–‰ (GPUê°€ ìˆëŠ” ê²½ìš°ë§Œ)
if device.type == 'cuda':
    results = memory_efficient_processing()
    print(f"ì´ ê²°ê³¼ ì²­í¬ ìˆ˜: {len(results)}")
```

## 7. ì‹¤ì œ ìŒì„± ë°ì´í„° ì˜ˆì œ

ì‹¤ì œ ìŒì„± ì²˜ë¦¬ ì‹œë‚˜ë¦¬ì˜¤ì— ê°€ê¹Œìš´ ì˜ˆì œë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

### 7.1 ìŒì†Œ ì •ë ¬ ì‹œë®¬ë ˆì´ì…˜

```python
def phoneme_alignment_example():
    """ìŒì†Œ-ìŒí–¥ ì •ë ¬ ì˜ˆì œ"""
    
    # ìŒì†Œ ì‹œí€€ìŠ¤ (ì˜ˆ: "HELLO")
    phonemes = ['H', 'E', 'L', 'L', 'O']
    phoneme_to_id = {p: i for i, p in enumerate(set(phonemes))}
    phoneme_ids = [phoneme_to_id[p] for p in phonemes]
    
    print(f"ìŒì†Œ ì‹œí€€ìŠ¤: {phonemes}")
    print(f"ìŒì†Œ ID: {phoneme_ids}")
    
    # ê° ìŒì†Œë‹¹ 3ê°œ ìƒíƒœ (ì‹œì‘-ì¤‘ê°„-ë)
    states_per_phoneme = 3
    total_states = len(set(phonemes)) * states_per_phoneme
    
    # ìŒì„± íŠ¹ì§• ì‹œë®¬ë ˆì´ì…˜ (ì˜ˆ: MFCC 13ì°¨ì›)
    audio_frames = 150  # 1.5ì´ˆ (10ms í”„ë ˆì„)
    feature_dim = 13
    
    # ì‹¤ì œë¡œëŠ” ìŒì„± íŒŒì¼ì—ì„œ ì¶”ì¶œ
    audio_features = torch.randn(1, audio_frames, feature_dim)
    
    print(f"ìŒì„± íŠ¹ì§• í˜•íƒœ: {audio_features.shape}")
    
    # íŠ¹ì§•ì„ ìƒíƒœ í™•ë¥ ë¡œ ë³€í™˜ (ì‹¤ì œë¡œëŠ” ê°€ìš°ì‹œì•ˆ ëª¨ë¸ ë“± ì‚¬ìš©)
    feature_to_state = nn.Linear(feature_dim, total_states)
    state_probs = torch.softmax(feature_to_state(audio_features), dim=-1)
    
    # ìŒì†Œë³„ HMM ì •ë ¬
    aligned_phonemes = []
    frame_idx = 0
    
    for phoneme, phoneme_id in zip(phonemes, phoneme_ids):
        # ê° ìŒì†Œì˜ ì˜ˆìƒ ì§€ì†ì‹œê°„ (ì‹¤ì œë¡œëŠ” ì–¸ì–´ ëª¨ë¸ì—ì„œ ì˜ˆì¸¡)
        expected_duration = audio_frames // len(phonemes)
        end_frame = min(frame_idx + expected_duration + 20, audio_frames)
        
        # í•´ë‹¹ êµ¬ê°„ì˜ ìƒíƒœ í™•ë¥ 
        phoneme_states = slice(phoneme_id * states_per_phoneme, 
                              (phoneme_id + 1) * states_per_phoneme)
        segment_probs = state_probs[:, frame_idx:end_frame, phoneme_states]
        
        # ìŒì†Œë³„ HMM
        P_phoneme = create_left_to_right_matrix(states_per_phoneme, 0.8)
        hmm_phoneme = HMMPyTorch(P_phoneme)
        
        # ì •ë ¬
        posteriors, _, _ = hmm_phoneme.forward_backward(segment_probs)
        optimal_states, _ = hmm_phoneme.viterbi_decode(segment_probs)
        
        # ì‹¤ì œ í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
        phoneme_alignment = optimal_states[0] + phoneme_id * states_per_phoneme
        
        aligned_phonemes.append({
            'phoneme': phoneme,
            'start_frame': frame_idx,
            'end_frame': end_frame,
            'alignment': phoneme_alignment,
            'duration': end_frame - frame_idx
        })
        
        frame_idx = end_frame
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n=== ìŒì†Œ ì •ë ¬ ê²°ê³¼ ===")
    for info in aligned_phonemes:
        duration_ms = info['duration'] * 10  # 10ms per frame
        print(f"ìŒì†Œ '{info['phoneme']}': "
              f"í”„ë ˆì„ {info['start_frame']:3d}-{info['end_frame']:3d} "
              f"({duration_ms:3d}ms)")
    
    return aligned_phonemes

# ì‹¤í–‰
alignment_result = phoneme_alignment_example()
```

### 7.2 ìŒì„± í’ˆì§ˆ í‰ê°€

```python
def speech_quality_evaluation():
    """ì •ë ¬ í’ˆì§ˆ í‰ê°€ ì˜ˆì œ"""
    
    # í•©ì„±ëœ ìŒì„±ê³¼ ì°¸ì¡° ìŒì„± ì‹œë®¬ë ˆì´ì…˜
    reference_alignment = torch.tensor([0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2])
    predicted_alignment = torch.tensor([0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2])
    
    print("ì°¸ì¡° ì •ë ¬:", reference_alignment.tolist())
    print("ì˜ˆì¸¡ ì •ë ¬:", predicted_alignment.tolist())
    
    # í”„ë ˆì„ ë‹¨ìœ„ ì •í™•ë„
    frame_accuracy = (reference_alignment == predicted_alignment).float().mean()
    print(f"í”„ë ˆì„ ì •í™•ë„: {frame_accuracy:.3f}")
    
    # ê²½ê³„ ê²€ì¶œ ì •í™•ë„
    def find_boundaries(alignment):
        boundaries = []
        for i in range(1, len(alignment)):
            if alignment[i] != alignment[i-1]:
                boundaries.append(i)
        return boundaries
    
    ref_boundaries = find_boundaries(reference_alignment)
    pred_boundaries = find_boundaries(predicted_alignment)
    
    print(f"ì°¸ì¡° ê²½ê³„: {ref_boundaries}")
    print(f"ì˜ˆì¸¡ ê²½ê³„: {pred_boundaries}")
    
    # ê²½ê³„ í—ˆìš© ì˜¤ì°¨ ë‚´ ì •í™•ë„ (Â±1 í”„ë ˆì„)
    tolerance = 1
    correct_boundaries = 0
    
    for ref_b in ref_boundaries:
        for pred_b in pred_boundaries:
            if abs(ref_b - pred_b) <= tolerance:
                correct_boundaries += 1
                break
    
    boundary_accuracy = correct_boundaries / len(ref_boundaries) if ref_boundaries else 0
    print(f"ê²½ê³„ ì •í™•ë„ (Â±{tolerance} í”„ë ˆì„): {boundary_accuracy:.3f}")
    
    return frame_accuracy, boundary_accuracy

# ì‹¤í–‰
frame_acc, boundary_acc = speech_quality_evaluation()
```

### 7.3 ì‹¤ì‹œê°„ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜

```python
def streaming_hmm_simulation():
    """ìŠ¤íŠ¸ë¦¬ë° HMM ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜"""
    
    num_states = 5
    chunk_size = 20  # 20 í”„ë ˆì„ì”© ì²˜ë¦¬
    total_frames = 200
    
    # HMM ëª¨ë¸
    P = create_left_to_right_matrix(num_states, 0.8)
    hmm = HMMPyTorch(P)
    
    # ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ ìœ ì§€
    streaming_state = {
        'previous_forward': None,
        'accumulated_posteriors': [],
        'current_frame': 0
    }
    
    print(f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬: {chunk_size} í”„ë ˆì„ì”©, ì´ {total_frames} í”„ë ˆì„")
    
    for chunk_start in range(0, total_frames, chunk_size):
        chunk_end = min(chunk_start + chunk_size, total_frames)
        current_chunk_size = chunk_end - chunk_start
        
        # í˜„ì¬ ì²­í¬ ë°ì´í„°
        chunk_obs = torch.softmax(
            torch.randn(1, current_chunk_size, num_states), dim=-1
        )
        
        # HMM ì²˜ë¦¬
        posteriors, forward, backward = hmm.forward_backward(chunk_obs)
        
        # ê²°ê³¼ ëˆ„ì 
        streaming_state['accumulated_posteriors'].append(posteriors)
        streaming_state['current_frame'] += current_chunk_size
        
        # ì‹¤ì‹œê°„ ì •ë ¬ (ìµœê·¼ ì²­í¬ë§Œ)
        current_alignment = torch.argmax(posteriors, dim=-1)[0]
        
        print(f"ì²­í¬ {chunk_start:3d}-{chunk_end:3d}: "
              f"ì •ë ¬ {current_alignment.tolist()}")
        
        # ì§€ì—° ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
        time.sleep(0.01)  # 10ms ì§€ì—°
    
    # ì „ì²´ ê²°ê³¼ ê²°í•©
    all_posteriors = torch.cat(streaming_state['accumulated_posteriors'], dim=1)
    final_alignment = torch.argmax(all_posteriors, dim=-1)[0]
    
    print(f"\nìµœì¢… ì •ë ¬ (ì´ {len(final_alignment)} í”„ë ˆì„):")
    print(f"ì²˜ìŒ 20ê°œ: {final_alignment[:20].tolist()}")
    print(f"ë§ˆì§€ë§‰ 20ê°œ: {final_alignment[-20:].tolist()}")
    
    return final_alignment

# ì‹¤í–‰
streaming_result = streaming_hmm_simulation()
```

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

ê¸°ë³¸ ì‚¬ìš©ë²•ì„ ìµí˜”ë‹¤ë©´, ì´ì œ ë” ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì„ íƒí—˜í•´ë³¼ ì‹œê°„ì…ë‹ˆë‹¤:

**ë‹¤ìŒ ë¬¸ì„œ**: [ê³ ê¸‰ ê¸°ëŠ¥](03_advanced_features.md) - Neural HMM, Semi-Markov, ì •ë ¬ ì•Œê³ ë¦¬ì¦˜

## ğŸ“ ìš”ì•½

ì´ ë¬¸ì„œì—ì„œ ë‹¤ë£¬ ì£¼ìš” ë‚´ìš©:

1. **ì„¤ì¹˜ ë° ì„¤ì •**: PyTorch HMM ë¼ì´ë¸ŒëŸ¬ë¦¬ í™˜ê²½ êµ¬ì„±
2. **ê¸°ë³¸ HMM**: ì „ì´ í–‰ë ¬, ê´€ì¸¡ ë°ì´í„°, Forward-Backward, Viterbi
3. **ì•Œê³ ë¦¬ì¦˜ ë¹„êµ**: Forward-Backward vs Viterbiì˜ ì°¨ì´ì ê³¼ ì‚¬ìš© ì‹œê¸°
4. **ì‹ ê²½ë§ í†µí•©**: HMMLayerë¥¼ ì‚¬ìš©í•œ end-to-end í•™ìŠµ
5. **ë°°ì¹˜ ì²˜ë¦¬**: íš¨ìœ¨ì ì¸ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
6. **GPU ê°€ì†**: CUDAë¥¼ í™œìš©í•œ ì„±ëŠ¥ ìµœì í™”
7. **ì‹¤ì œ ì‘ìš©**: ìŒì„± ì²˜ë¦¬ì—ì„œì˜ ì‹¤ì œ ì‚¬ìš© ì˜ˆì œ

## ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸

- **Forward-Backward**: í•™ìŠµê³¼ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”ì— ì‚¬ìš©
- **Viterbi**: ìµœì¢… ì¶”ë¡ ê³¼ ì‹¤ì‹œê°„ ì²˜ë¦¬ì— ì‚¬ìš©
- **HMMLayer**: ì‹ ê²½ë§ê³¼ì˜ ìì—°ìŠ¤ëŸ¬ìš´ í†µí•© ì œê³µ
- **ë°°ì¹˜ ì²˜ë¦¬**: íŒ¨ë”©ê³¼ ë§ˆìŠ¤í‚¹ìœ¼ë¡œ íš¨ìœ¨ì  ì²˜ë¦¬
- **GPU ê°€ì†**: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì„±ëŠ¥ í–¥ìƒ

ì´ì œ ì´ ê¸°ì´ˆë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” ë³µì¡í•œ ëª¨ë¸ê³¼ ì‘ìš©ì„ íƒí—˜í•´ë³´ì„¸ìš”!

---

**ë‹¤ìŒ**: [ê³ ê¸‰ ê¸°ëŠ¥](03_advanced_features.md)ì—ì„œ Neural HMM, Semi-Markov HMM, ì •ë ¬ ì•Œê³ ë¦¬ì¦˜ ë“±ì„ í•™ìŠµí•´ë³´ì„¸ìš”.

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

ê¸°ë³¸ ì‚¬ìš©ë²•ì„ ìµí˜”ë‹¤ë©´, ì´ì œ ë” ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì„ íƒí—˜í•´ë³¼ ì‹œê°„ì…ë‹ˆë‹¤:

**ë‹¤ìŒ ë¬¸ì„œ**: [ê³ ê¸‰ ê¸°ëŠ¥](03_advanced_features.md) - Neural HMM, Semi-Markov HMM, ì •ë ¬ ì•Œê³ ë¦¬ì¦˜

## ğŸ“ ìš”ì•½

ì´ ë¬¸ì„œì—ì„œ ë‹¤ë£¬ ì£¼ìš” ë‚´ìš©:

1. **ì„¤ì¹˜ ë° ì„¤ì •**: PyTorch HMM ë¼ì´ë¸ŒëŸ¬ë¦¬ í™˜ê²½ êµ¬ì„±
2. **ê¸°ë³¸ HMM**: ì „ì´ í–‰ë ¬, ê´€ì¸¡ ë°ì´í„°, Forward-Backward, Viterbi
3. **ì•Œê³ ë¦¬ì¦˜ ë¹„êµ**: Forward-Backward vs Viterbiì˜ ì°¨ì´ì ê³¼ ì‚¬ìš© ì‹œê¸°
4. **ì‹ ê²½ë§ í†µí•©**: HMMLayerë¥¼ ì‚¬ìš©í•œ end-to-end í•™ìŠµ
5. **ë°°ì¹˜ ì²˜ë¦¬**: íš¨ìœ¨ì ì¸ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
6. **GPU ê°€ì†**: CUDAë¥¼ í™œìš©í•œ ì„±ëŠ¥ ìµœì í™”
7. **ì‹¤ì œ ì‘ìš©**: ìŒì„± ì²˜ë¦¬ì—ì„œì˜ ì‹¤ì œ ì‚¬ìš© ì˜ˆì œ

## ğŸ’¡ í•µì‹¬ í¬ì¸íŠ¸

- **Forward-Backward**: í•™ìŠµê³¼ ë¶ˆí™•ì‹¤ì„± ì •ëŸ‰í™”ì— ì‚¬ìš©
- **Viterbi**: ìµœì¢… ì¶”ë¡ ê³¼ ì‹¤ì‹œê°„ ì²˜ë¦¬ì— ì‚¬ìš©
- **HMMLayer**: ì‹ ê²½ë§ê³¼ì˜ ìì—°ìŠ¤ëŸ¬ìš´ í†µí•© ì œê³µ
- **ë°°ì¹˜ ì²˜ë¦¬**: íŒ¨ë”©ê³¼ ë§ˆìŠ¤í‚¹ìœ¼ë¡œ íš¨ìœ¨ì  ì²˜ë¦¬
- **GPU ê°€ì†**: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì„±ëŠ¥ í–¥ìƒ

ì´ì œ ì´ ê¸°ì´ˆë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” ë³µì¡í•œ ëª¨ë¸ê³¼ ì‘ìš©ì„ íƒí—˜í•´ë³´ì„¸ìš”!

---

**ë‹¤ìŒ**: [ê³ ê¸‰ ê¸°ëŠ¥](03_advanced_features.md)ì—ì„œ Neural HMM, Semi-Markov HMM, ì •ë ¬ ì•Œê³ ë¦¬ì¦˜ ë“±ì„ í•™ìŠµí•´ë³´ì„¸ìš”. 