# PyTorch HMM

ğŸ¯ **Advanced PyTorch Hidden Markov Model Library for Speech Synthesis**

[![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 1.8+](https://img.shields.io/badge/PyTorch-1.8+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Version](https://img.shields.io/badge/version-0.2.0-green.svg)](https://github.com/crlotwhite/pytorch_hmm)

PyTorch ê¸°ë°˜ì˜ ì¢…í•©ì ì¸ Hidden Markov Model ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. **ìŒì„± í•©ì„±(TTS), ìŒì„± ì¸ì‹(ASR), ì‹œí€€ìŠ¤ ëª¨ë¸ë§**ì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë©°, ìµœì‹  ë”¥ëŸ¬ë‹ ê¸°ë²•ê³¼ ì „í†µì ì¸ HMMì„ ê²°í•©í•œ ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤.

## âœ¨ ìƒˆë¡œìš´ v0.2.0 ì£¼ìš” ê¸°ëŠ¥

### ğŸ§  **Neural HMM with Contextual Modeling**
- **Context-aware HMM**: ì–¸ì–´ì  ì»¨í…ìŠ¤íŠ¸ì™€ ìš´ìœ¨ ì •ë³´ë¥¼ í™œìš©
- **RNN/Transformer ê¸°ë°˜ ì „ì´ ëª¨ë¸**: ë™ì  ì „ì´ í™•ë¥  ê³„ì‚°
- **Mixture Gaussian ê´€ì¸¡ ëª¨ë¸**: ë³µì¡í•œ ìŒí–¥ íŠ¹ì§• ëª¨ë¸ë§

### â±ï¸ **Hidden Semi-Markov Model (HSMM)**
- **ëª…ì‹œì  ì§€ì†ì‹œê°„ ëª¨ë¸ë§**: Gamma, Poisson, Neural ë¶„í¬ ì§€ì›
- **ìì—°ìŠ¤ëŸ¬ìš´ ìŒì†Œ ì§€ì†ì‹œê°„**: ì‹¤ì œ ìŒì„± íŠ¹ì„± ë°˜ì˜
- **ì ì‘í˜• ì§€ì†ì‹œê°„**: ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥¸ ì§€ì†ì‹œê°„ ì¡°ì ˆ

### ğŸ¯ **Advanced Alignment Algorithms**
- **Dynamic Time Warping (DTW)**: ìœ ì—°í•œ ì‹œí€€ìŠ¤ ì •ë ¬, Soft-DTW ì§€ì›
- **CTC Alignment**: End-to-end í•™ìŠµ ê°€ëŠ¥í•œ ì •ë ¬
- **Constrained alignment**: Bandwidth ì œì•½, Monotonic ì •ë ¬

### ğŸ“Š **Comprehensive Evaluation Metrics**
- **Mel-Cepstral Distortion (MCD)**: ìŒì„± í’ˆì§ˆì˜ ê°ê´€ì  í‰ê°€
- **F0 RMSE**: ìš´ìœ¨ ëª¨ë¸ë§ ì„±ëŠ¥ í‰ê°€
- **Alignment Accuracy**: ì •ë ¬ ì •í™•ë„ ë° ê²½ê³„ íƒì§€ ì„±ëŠ¥
- **Duration Modeling**: ì§€ì†ì‹œê°„ ì˜ˆì¸¡ ì •í™•ë„

### ğŸš€ **Production-Ready Features**
- **GPU ê°€ì†**: CUDA ì§€ì›ìœ¼ë¡œ ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥ (>80fps)
- **ë°°ì¹˜ ì²˜ë¦¬**: íš¨ìœ¨ì ì¸ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
- **JIT í˜¸í™˜**: `torch.jit.script`ë¡œ ìµœì í™”
- **ë©”ëª¨ë¦¬ íš¨ìœ¨**: Log-space ê³„ì‚°ìœ¼ë¡œ ìˆ˜ì¹˜ ì•ˆì •ì„±

## ğŸ“¦ ì„¤ì¹˜

```bash
# ê¸°ë³¸ ì„¤ì¹˜
pip install pytorch-hmm

# ëª¨ë“  ê¸°ëŠ¥ í¬í•¨
pip install pytorch-hmm[all]

# ê°œë°œ ë²„ì „
git clone https://github.com/crlotwhite/pytorch_hmm.git
cd pytorch_hmm
pip install -e ".[all]"
```

### ì„ íƒì  ì˜ì¡´ì„±

```bash
# ì˜¤ë””ì˜¤ ì²˜ë¦¬
pip install pytorch-hmm[audio]

# ì‹œê°í™”
pip install pytorch-hmm[visualization]

# ê°œë°œ ë„êµ¬
pip install pytorch-hmm[dev]

# ë²¤ì¹˜ë§ˆí¬ ë„êµ¬
pip install pytorch-hmm[benchmarks]
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ê¸°ë³¸ HMM ì‚¬ìš©ë²•

```python
import torch
from pytorch_hmm import HMMPyTorch, create_left_to_right_matrix

# Left-to-right HMM ìƒì„± (ìŒì„± í•©ì„±ì— ì¼ë°˜ì )
num_states = 5
transition_matrix = create_left_to_right_matrix(num_states, self_loop_prob=0.8)
hmm = HMMPyTorch(transition_matrix)

# ê´€ì¸¡ ë°ì´í„° (batch_size=2, seq_len=10, num_states=5)
observations = torch.rand(2, 10, 5)

# Forward-backward ì•Œê³ ë¦¬ì¦˜
posterior, forward, backward = hmm.forward_backward(observations)

# Viterbi ë””ì½”ë”©
states, scores = hmm.viterbi_decode(observations)

print(f"Posterior shape: {posterior.shape}")  # (2, 10, 5)
print(f"Optimal states: {states[0]}")         # ì²« ë²ˆì§¸ ì‹œí€€ìŠ¤ì˜ ìµœì  ìƒíƒœ ê²½ë¡œ
```

### 2. Neural HMMìœ¼ë¡œ ê³ ê¸‰ ëª¨ë¸ë§

```python
from pytorch_hmm import NeuralHMM, ContextualNeuralHMM

# ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ Neural HMM
contextual_hmm = ContextualNeuralHMM(
    num_states=10,
    observation_dim=80,          # ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì°¨ì›
    phoneme_vocab_size=50,       # ìŒì†Œ ê°œìˆ˜
    linguistic_context_dim=32,   # ì–¸ì–´ì  ì»¨í…ìŠ¤íŠ¸
    prosody_dim=8               # ìš´ìœ¨ íŠ¹ì§•
)

# ì…ë ¥ ë°ì´í„° ì¤€ë¹„
batch_size, seq_len = 2, 100
observations = torch.randn(batch_size, seq_len, 80)           # ìŒí–¥ íŠ¹ì§•
phoneme_sequence = torch.randint(0, 50, (batch_size, seq_len)) # ìŒì†Œ ì‹œí€€ìŠ¤
prosody_features = torch.randn(batch_size, seq_len, 8)        # ìš´ìœ¨ íŠ¹ì§•

# ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì •ë ¬
posteriors, forward, backward = contextual_hmm.forward_with_context(
    observations, phoneme_sequence, prosody_features)

print(f"Context-aware posteriors: {posteriors.shape}")
```

### 3. Semi-Markov HMMìœ¼ë¡œ ì§€ì†ì‹œê°„ ëª¨ë¸ë§

```python
from pytorch_hmm import SemiMarkovHMM

# ì§€ì†ì‹œê°„ ëª¨ë¸ë§ì´ ê°€ëŠ¥í•œ HSMM
hsmm = SemiMarkovHMM(
    num_states=8,
    observation_dim=40,
    max_duration=20,
    duration_distribution='gamma',  # ê°ë§ˆ ë¶„í¬ë¡œ ì§€ì†ì‹œê°„ ëª¨ë¸ë§
    observation_model='gaussian'
)

# ì‹œí€€ìŠ¤ ìƒ˜í”Œë§
state_seq, duration_seq, observations = hsmm.sample(
    num_states=6, max_length=100)

print(f"Sampled states: {state_seq}")
print(f"State durations: {duration_seq}")
print(f"Total frames: {duration_seq.sum()} frames")

# Viterbi ë””ì½”ë”©ìœ¼ë¡œ ìµœì  ìƒíƒœ-ì§€ì†ì‹œê°„ ì‹œí€€ìŠ¤ ì°¾ê¸°
optimal_states, optimal_durations, log_prob = hsmm.viterbi_decode(observations)
```

### 4. DTW ì •ë ¬

```python
from pytorch_hmm import DTWAligner

# DTW ì •ë ¬ê¸° ìƒì„±
aligner = DTWAligner(
    distance_fn='cosine',           # ì½”ì‚¬ì¸ ê±°ë¦¬
    step_pattern='symmetric',       # ëŒ€ì¹­ ìŠ¤í… íŒ¨í„´
    soft_dtw=True                  # ë¯¸ë¶„ ê°€ëŠ¥í•œ Soft DTW
)

# ìŒì†Œ íŠ¹ì§•ê³¼ ìŒì„± íŠ¹ì§• ì •ë ¬
phoneme_features = torch.randn(5, 12)   # 5ê°œ ìŒì†Œ
audio_features = torch.randn(100, 12)   # 100 í”„ë ˆì„

# DTW ì •ë ¬ ìˆ˜í–‰
path_i, path_j, total_cost = aligner(phoneme_features, audio_features)

print(f"Alignment path length: {len(path_i)}")
print(f"DTW cost: {total_cost:.4f}")
```

### 5. CTC ì •ë ¬

```python
from pytorch_hmm import CTCAligner

# CTC ì •ë ¬ê¸° (ìŒì„± ì¸ì‹ìš©)
ctc_aligner = CTCAligner(
    num_classes=28,  # 26 letters + blank + space
    blank_id=0
)

# ìŒì„± ì¸ì‹ ì‹œë®¬ë ˆì´ì…˜
sequence_length, batch_size, vocab_size = 80, 2, 28
log_probs = torch.log_softmax(
    torch.randn(sequence_length, batch_size, vocab_size), dim=-1)

targets = torch.tensor([[8, 5, 12, 12, 15],   # "HELLO"
                       [23, 15, 18, 12, 4]])  # "WORLD"

input_lengths = torch.full((batch_size,), sequence_length)
target_lengths = torch.tensor([5, 5])

# CTC loss ê³„ì‚°
loss = ctc_aligner(log_probs, targets, input_lengths, target_lengths)

# Greedy ë””ì½”ë”©
decoded = ctc_aligner.decode(log_probs, input_lengths)
print(f"Decoded sequences: {decoded}")
```

## ğŸ“Š ì„±ëŠ¥ í‰ê°€

### ìŒì„± í’ˆì§ˆ ë©”íŠ¸ë¦­

```python
from pytorch_hmm import (
    mel_cepstral_distortion, f0_root_mean_square_error,
    comprehensive_speech_evaluation
)

# ì‹œë®¬ë ˆì´ì…˜ëœ TTS ê²°ê³¼
seq_len, mfcc_dim = 200, 13
ground_truth_mfcc = torch.randn(seq_len, mfcc_dim)
predicted_mfcc = ground_truth_mfcc + 0.1 * torch.randn(seq_len, mfcc_dim)

# MCD ê³„ì‚°
mcd = mel_cepstral_distortion(ground_truth_mfcc, predicted_mfcc)
print(f"Mel-Cepstral Distortion: {mcd:.2f} dB")

# F0 í‰ê°€
gt_f0 = torch.abs(torch.randn(seq_len)) * 100 + 120  # 120-220 Hz
pred_f0 = gt_f0 + 5 * torch.randn(seq_len)

f0_rmse = f0_root_mean_square_error(gt_f0, pred_f0)
print(f"F0 RMSE: {f0_rmse:.2f} Hz")

# ì¢…í•© í‰ê°€
predicted_features = {
    'mfcc': predicted_mfcc.unsqueeze(0),
    'f0': pred_f0.unsqueeze(0)
}
ground_truth_features = {
    'mfcc': ground_truth_mfcc.unsqueeze(0),
    'f0': gt_f0.unsqueeze(0)
}

metrics = comprehensive_speech_evaluation(predicted_features, ground_truth_features)
print(f"Comprehensive metrics: {metrics}")
```

## ğŸ¯ ì‹¤ì œ ì‘ìš© ì˜ˆì œ

### ì™„ì „í•œ TTS íŒŒì´í”„ë¼ì¸

```python
import torch.nn as nn
from pytorch_hmm import HMMLayer, DurationModel, DTWAligner

class AdvancedTTSModel(nn.Module):
    def __init__(self, vocab_size, num_phonemes, acoustic_dim):
        super().__init__()
        
        # í…ìŠ¤íŠ¸ ì¸ì½”ë”
        self.text_encoder = nn.Embedding(vocab_size, 256)
        
        # ì§€ì†ì‹œê°„ ì˜ˆì¸¡ê¸° (HSMM ê¸°ë°˜)
        self.duration_predictor = DurationModel(
            num_states=num_phonemes,
            max_duration=30,
            distribution_type='neural'
        )
        
        # HMM ì •ë ¬ ë ˆì´ì–´
        self.alignment_layer = HMMLayer(
            num_states=num_phonemes,
            learnable_transitions=True,
            transition_type="left_to_right"
        )
        
        # ìŒí–¥ ë””ì½”ë”
        self.acoustic_decoder = nn.Sequential(
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, acoustic_dim)
        )
        
        # DTW í›„ì²˜ë¦¬
        self.dtw_aligner = DTWAligner(distance_fn='cosine')
    
    def forward(self, text_sequence, target_length=None):
        # í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        text_emb = self.text_encoder(text_sequence)
        
        # ì§€ì†ì‹œê°„ ì˜ˆì¸¡
        phoneme_ids = text_sequence  # ê°„ë‹¨í™”
        predicted_durations = self.duration_predictor.sample(phoneme_ids)
        
        # HMM ì •ë ¬
        aligned_features = self.alignment_layer(text_emb)
        
        # ìŒí–¥ íŠ¹ì§• ìƒì„±
        acoustic_features = self.acoustic_decoder(aligned_features)
        
        # DTWë¡œ ê¸¸ì´ ì¡°ì • (ì˜µì…˜)
        if target_length is not None:
            target_features = torch.randn(target_length, acoustic_features.shape[-1])
            path_i, path_j, _ = self.dtw_aligner(acoustic_features[0], target_features)
            # ê¸¸ì´ ì¡°ì • ë¡œì§...
        
        return acoustic_features, predicted_durations

# ì‚¬ìš© ì˜ˆì œ
model = AdvancedTTSModel(vocab_size=1000, num_phonemes=50, acoustic_dim=80)
text = torch.randint(0, 1000, (1, 20))  # 20ê°œ í† í°
acoustic_output, durations = model(text)

print(f"Generated acoustic features: {acoustic_output.shape}")
print(f"Predicted durations: {durations}")
```

## ğŸ”§ ê³ ê¸‰ ê¸°ëŠ¥ ë° ì„¤ì •

### ì‚¬ìš©ì ì •ì˜ ì „ì´ í–‰ë ¬

```python
from pytorch_hmm import create_transition_matrix

# ë‹¤ì–‘í•œ ì „ì´ íŒ¨í„´
transition_types = [
    "ergodic",              # ì™„ì „ ì—°ê²°
    "left_to_right",        # ìˆœì°¨ ì§„í–‰  
    "left_to_right_skip",   # ê±´ë„ˆë›°ê¸° í—ˆìš©
    "circular"              # ìˆœí™˜ êµ¬ì¡°
]

for t_type in transition_types:
    P = create_transition_matrix(num_states=8, transition_type=t_type)
    print(f"{t_type}: {P.shape}")
```

### GPU ê°€ì† ë° ìµœì í™”

```python
# GPU ì‚¬ìš©
device = 'cuda' if torch.cuda.is_available() else 'cpu'
hmm = HMMPyTorch(transition_matrix).to(device)
observations = observations.to(device)

# JIT ì»´íŒŒì¼ë¡œ ì†ë„ í–¥ìƒ
@torch.jit.script
def fast_viterbi(hmm_model, obs):
    return hmm_model.viterbi_decode(obs)

# ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
def process_large_batch(hmm, data_loader):
    results = []
    for batch in data_loader:
        with torch.no_grad():
            batch = batch.to(device)
            posteriors, _, _ = hmm.forward_backward(batch)
            results.append(posteriors.cpu())
    return torch.cat(results, dim=0)
```

## ğŸ“ˆ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ë¹ ë¥¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰

```bash
# ê¸°ë³¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
python examples/benchmark.py --quick

# ì „ì²´ ë²¤ì¹˜ë§ˆí¬ (GPU í¬í•¨)
python examples/benchmark.py --save-results benchmark_results.json

# CPUë§Œ í…ŒìŠ¤íŠ¸
python examples/benchmark.py --no-gpu
```

### ì¼ë°˜ì ì¸ ì„±ëŠ¥ (RTX 3080 GPU ê¸°ì¤€)

| ì•Œê³ ë¦¬ì¦˜ | ì„±ëŠ¥ (fps) | ì‹¤ì‹œê°„ ì²˜ë¦¬ |
|---------|-----------|------------|
| **Basic HMM Forward-Backward** | ~15,000 | âœ… (188x faster) |
| **Basic HMM Viterbi** | ~25,000 | âœ… (312x faster) |
| **Neural HMM** | ~8,000 | âœ… (100x faster) |
| **DTW Alignment** | ~5,000 | âœ… (62x faster) |
| **CTC Decode** | ~12,000 | âœ… (150x faster) |

*ì‹¤ì‹œê°„ ì²˜ë¦¬ ê¸°ì¤€: 80fps (12.5ms/frame)*

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ëª¨ë“  ìƒˆ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
python tests/test_integration.py

# ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
python -m pytest tests/test_hmm.py -v

# ì„±ëŠ¥ íšŒê·€ í…ŒìŠ¤íŠ¸
python -m pytest tests/test_performance.py --benchmark-only
```

### ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬

```bash
# ì½”ë“œ í¬ë§·íŒ…
black pytorch_hmm/
isort pytorch_hmm/

# ë¦°íŒ…
flake8 pytorch_hmm/
mypy pytorch_hmm/
```

## ğŸ“– í•™ìŠµ ìë£Œ ë° ì˜ˆì œ

### ë‹¨ê³„ë³„ íŠœí† ë¦¬ì–¼

```bash
# 1. ê¸°ë³¸ ì‚¬ìš©ë²• í•™ìŠµ
python examples/basic_tutorial.py

# 2. ê³ ê¸‰ ê¸°ëŠ¥ ë°ëª¨
python examples/advanced_features_demo.py

# 3. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
python examples/benchmark.py
```

### ì»¤ë§¨ë“œë¼ì¸ ë„êµ¬

```bash
# ë¹ ë¥¸ ë°ëª¨ ì‹¤í–‰
pytorch-hmm-demo

# í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰  
pytorch-hmm-test
```

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

### ê°œë°œ í™˜ê²½ ì„¤ì •

```bash
git clone https://github.com/crlotwhite/pytorch_hmm.git
cd pytorch_hmm

# ê°œë°œ ì˜ì¡´ì„± ì„¤ì¹˜
pip install -e ".[dev]"

# pre-commit í›… ì„¤ì •
pre-commit install

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/ -v
```

### ê¸°ì—¬ ê°€ì´ë“œë¼ì¸

1. **ì´ìŠˆ ìƒì„±**: ìƒˆ ê¸°ëŠ¥ì´ë‚˜ ë²„ê·¸ ë¦¬í¬íŠ¸
2. **Fork & Branch**: `feature/your-feature-name`
3. **í…ŒìŠ¤íŠ¸ ì‘ì„±**: ìƒˆ ê¸°ëŠ¥ì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì¶”ê°€
4. **ë¬¸ì„œí™”**: docstringê³¼ README ì—…ë°ì´íŠ¸
5. **Pull Request**: ìƒì„¸í•œ ì„¤ëª…ê³¼ í•¨ê»˜ ì œì¶œ

## ğŸ“Š ë²„ì „ íˆìŠ¤í† ë¦¬ ë° ë¡œë“œë§µ

### âœ… v0.2.0 (í˜„ì¬) - Advanced Features
- âœ… Neural HMM with contextual modeling
- âœ… Hidden Semi-Markov Model (HSMM)
- âœ… DTW and CTC alignment algorithms
- âœ… Comprehensive evaluation metrics
- âœ… Performance optimization and GPU acceleration
- âœ… Production-ready features

### ğŸ”„ v0.3.0 (ê³„íš) - Real-world Integration
- [ ] LibriSpeech/KSS ë°ì´í„°ì…‹ ì§€ì›
- [ ] ONNX ë‚´ë³´ë‚´ê¸° ë° ì–‘ìí™”
- [ ] ì‹¤ì‹œê°„ ë§ˆì´í¬ ì…ë ¥ ì²˜ë¦¬
- [ ] Attention-based alignment
- [ ] ë©€í‹°í™”ì ëª¨ë¸ë§

### ğŸ¯ v1.0.0 (ëª©í‘œ) - Production Ready
- [ ] End-to-end TTS íŒŒì´í”„ë¼ì¸
- [ ] C++ ì¶”ë¡  ì—”ì§„
- [ ] ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- [ ] ì™„ì „í•œ ë¬¸ì„œí™” ë° API ì•ˆì •í™”

## ğŸ“š ì°¸ê³  ìë£Œ

### í•µì‹¬ ë…¼ë¬¸
- Rabiner, L. R. (1989). "A tutorial on hidden Markov models"
- Yu, K., et al. (2010). "Semi-Markov models for speech synthesis"  
- Cuturi, M., & Blondel, M. (2017). "Soft-DTW: a Differentiable Loss Function for Time-Series"
- Graves, A., et al. (2006). "Connectionist temporal classification"

### ì‹¤ë¬´ í™œìš© ì‚¬ë¡€
- ìŒì„± í•©ì„± ì‹œìŠ¤í…œì—ì„œì˜ ìŒì†Œ ì •ë ¬
- ìŒì„± ì¸ì‹ì—ì„œì˜ CTC ë””ì½”ë”©
- í™”ì ì ì‘ì„ ìœ„í•œ DTW ì •ë ¬
- ì§€ì†ì‹œê°„ ëª¨ë¸ë§ì„ í†µí•œ ìì—°ìŠ¤ëŸ¬ìš´ TTS

## ğŸ™‹â€â™‚ï¸ ì§€ì› ë° ì»¤ë®¤ë‹ˆí‹°

- **GitHub Issues**: [ë²„ê·¸ ë¦¬í¬íŠ¸ ë° ê¸°ëŠ¥ ìš”ì²­](https://github.com/crlotwhite/pytorch_hmm/issues)
- **GitHub Discussions**: [ì¼ë°˜ ì§ˆë¬¸ ë° í† ë¡ ](https://github.com/crlotwhite/pytorch_hmm/discussions)
- **Documentation**: [ìƒì„¸ ë¬¸ì„œ](https://github.com/crlotwhite/pytorch_hmm/wiki)

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

---

## ğŸ‰ v0.2.0 ì—…ë°ì´íŠ¸ í•˜ì´ë¼ì´íŠ¸

ì´ë²ˆ **v0.2.0** ì—…ë°ì´íŠ¸ëŠ” PyTorch HMM ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ **ì—°êµ¬ìš© ë„êµ¬ì—ì„œ í”„ë¡œë•ì…˜ ë ˆë”” ì†”ë£¨ì…˜**ìœ¼ë¡œ í¬ê²Œ ë°œì „ì‹œì¼°ìŠµë‹ˆë‹¤:

ğŸ§  **Neural HMM**: ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ëª¨ë¸ë§ìœ¼ë¡œ ê¸°ì¡´ HMMì˜ í•œê³„ ê·¹ë³µ  
â±ï¸ **Semi-Markov HMM**: ëª…ì‹œì  ì§€ì†ì‹œê°„ ëª¨ë¸ë§ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± í•©ì„±  
ğŸ¯ **ê³ ê¸‰ ì •ë ¬**: DTWì™€ CTCë¡œ ë‹¤ì–‘í•œ ì •ë ¬ needs ì§€ì›  
ğŸ“Š **ì¢…í•© í‰ê°€**: MCD, F0 RMSE ë“± í‘œì¤€ ìŒì„± í‰ê°€ ë©”íŠ¸ë¦­  
ğŸš€ **ì‹¤ì‹œê°„ ì„±ëŠ¥**: GPU ê°€ì†ìœ¼ë¡œ ì‹¤ì‹œê°„ ìŒì„± ì²˜ë¦¬ ê°€ëŠ¥

â­ **ì´ í”„ë¡œì íŠ¸ê°€ ë„ì›€ì´ ë˜ì—ˆë‹¤ë©´ Starë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”!**

ğŸ”— **ê´€ë ¨ í”„ë¡œì íŠ¸**: [Original TensorFlow HMM](https://github.com/crlotwhite/tensorflow_hmm)
