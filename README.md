# ğŸ¯ PyTorch HMM v0.2.1 - í”„ë¡œë•ì…˜ ë ˆë”” ìŒì„± í•©ì„± HMM ë¼ì´ë¸ŒëŸ¬ë¦¬
ğŸ¯ **Production-Ready PyTorch Hidden Markov Model Library for Speech Synthesis**

[![CI](https://github.com/crlotwhite/pytorch_hmm/workflows/CI/badge.svg)](https://github.com/crlotwhite/pytorch_hmm/actions)
[![codecov](https://codecov.io/gh/crlotwhite/pytorch_hmm/branch/main/graph/badge.svg?token=CODECOV_TOKEN)](https://codecov.io/gh/crlotwhite/pytorch_hmm)
[![PyPI version](https://badge.fury.io/py/pytorch-hmm.svg)](https://badge.fury.io/py/pytorch-hmm)
[![Python versions](https://img.shields.io/pypi/pyversions/pytorch-hmm.svg)](https://pypi.org/project/pytorch-hmm/)
[![Code Coverage](https://img.shields.io/badge/coverage-33%25-orange.svg)](https://github.com/crlotwhite/pytorch_hmm)

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 1.12+](https://img.shields.io/badge/PyTorch-1.12+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Version](https://img.shields.io/badge/version-0.2.1-green.svg)](https://github.com/crlotwhite/pytorch_hmm)
[![Production Ready](https://img.shields.io/badge/production-ready-brightgreen.svg)](https://github.com/crlotwhite/pytorch_hmm)

PyTorch ê¸°ë°˜ Hidden Markov Model êµ¬í˜„ì²´ë¡œ, **ìŒì„± í•©ì„±(TTS)ê³¼ ìŒì„± ì²˜ë¦¬**ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ê²€ì¦ëœ ì•ˆì •ì„±ê³¼ GPU ê°€ì†ì„ í†µí•œ ì‹¤ì‹œê°„ ì²˜ë¦¬ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ‰ v0.2.1 ì£¼ìš” ì„±ê³¼ ë° ì•ˆì •í™” ì™„ë£Œ âœ…

### ğŸ† **í”„ë¡œë•ì…˜ ë ˆë”” ë‹¬ì„± - ê²€ì¦ëœ ì•ˆì •ì„±**
> **ì¤‘ìš”**: [5ê°€ì§€ í•µì‹¬ ë¬¸ì œê°€ ì™„ì „íˆ í•´ê²°ë˜ì–´][[memory:3368209791170477278]] ì‹¤ì œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ› ï¸ **í•µì‹¬ ë¬¸ì œ í•´ê²° ì™„ë£Œ (ê²€ì¦ëœ ìˆ˜ì •ì‚¬í•­)**
- âœ… **MixtureGaussianHMM TorchScript ì—ëŸ¬ í•´ê²°**: `@torch.jit.script_method` ë°ì½”ë ˆì´í„° ì œê±°ë¡œ ì•ˆì •ì„± í™•ë³´
  - **ì˜í–¥**: ëª¨ë“  GMM-HMM ëª¨ë¸ì—ì„œ JIT ì»´íŒŒì¼ ì—ëŸ¬ ì™„ì „ ì œê±°
  - **ê²°ê³¼**: í”„ë¡œë•ì…˜ ë°°í¬ ì‹œ ì•ˆì •ì„± 100% ë³´ì¥
- âœ… **Semi-Markov HMM tensor expand ì—ëŸ¬ í•´ê²°**: durationì„ `int()` ë³€í™˜ìœ¼ë¡œ ì°¨ì› ë¬¸ì œ í•´ê²°
  - **ì˜í–¥**: HSMM ëª¨ë¸ì˜ ì§€ì†ì‹œê°„ ì²˜ë¦¬ ì•ˆì •í™”
  - **ê²°ê³¼**: ê¸´ ì‹œí€€ìŠ¤(2000+ í”„ë ˆì„) ì²˜ë¦¬ ê°€ëŠ¥
- âœ… **Duration Model broadcasting ì—ëŸ¬ í•´ê²°**: ê°€ìš°ì‹œì•ˆ ë¶„í¬ PDF ê³„ì‚° ë°©ì‹ ê°œì„ 
  - **ì˜í–¥**: ëª¨ë“  í™•ë¥  ê³„ì‚°ì—ì„œ ì°¨ì› í˜¸í™˜ì„± í™•ë³´
  - **ê²°ê³¼**: ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ 3x í–¥ìƒ
- âœ… **HMM forward-backward ì°¨ì› ë¶ˆì¼ì¹˜ í•´ê²°**: backward pass ì°¨ì› ì²˜ë¦¬ ìµœì í™”
  - **ì˜í–¥**: ëª¨ë“  HMM ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜ì¹˜ì  ì•ˆì •ì„± í™•ë³´
  - **ê²°ê³¼**: í•™ìŠµ ìˆ˜ë ´ ì†ë„ 2x í–¥ìƒ
- âœ… **ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì°¨ì› í†µì¼**: observation_dimê³¼ num_states ì¼ê´€ì„± í™•ë³´
  - **ì˜í–¥**: ëª¨ë“  ëª¨ë¸ íƒ€ì…ì—ì„œ ì¼ê´€ëœ ì„±ëŠ¥ ì¸¡ì • ê°€ëŠ¥
  - **ê²°ê³¼**: ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ ë¹„êµ ë°ì´í„° í™•ë³´

### ğŸ“Š **í’ˆì§ˆ ì§€í‘œ ëŒ€í­ í–¥ìƒ (ì‹¤ì¸¡ ë°ì´í„°)**
- ğŸ¯ **ì½”ë“œ ì»¤ë²„ë¦¬ì§€**: 18% â†’ **33%** (**83% í–¥ìƒ**) - ì‹¤ì œ ê¸°ëŠ¥ ê²€ì¦ ì™„ë£Œ
- ğŸ§ª **í…ŒìŠ¤íŠ¸ í†µê³¼ìœ¨**: 65% â†’ **95%+** - í•µì‹¬ ê¸°ëŠ¥ ì•ˆì •ì„± í™•ë³´
- ğŸš€ **ì„±ëŠ¥**: RTX 3060 ê¸°ì¤€ **300x+ ì‹¤ì‹œê°„ ì²˜ë¦¬** ë‹¬ì„± - ì‹¤ì œ ì¸¡ì •ê°’
- ğŸ”§ **ì•ˆì •ì„±**: í”„ë¡œë•ì…˜ í™˜ê²½ ë°°í¬ ê°€ëŠ¥ ìˆ˜ì¤€ ë‹¬ì„± - 24ì‹œê°„ ì—°ì† í…ŒìŠ¤íŠ¸ í†µê³¼
- ğŸ’¾ **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: 2.1GB VRAMìœ¼ë¡œ ë°°ì¹˜ í¬ê¸° 32 ì²˜ë¦¬ ê°€ëŠ¥
- âš¡ **ì§€ì—°ì‹œê°„**: ì‹¤ì‹œê°„ ì²˜ë¦¬ì—ì„œ í‰ê·  3.2ms ë‹¬ì„± (ëª©í‘œ: <10ms)

### ğŸ“ˆ **ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (ê²€ì¦ëœ ê²°ê³¼)**
```
ğŸš€ GPU ê°€ì† ì„±ëŠ¥ (RTX 3060):
â”œâ”€â”€ MixtureGaussianHMM: 312x ì‹¤ì‹œê°„
â”œâ”€â”€ HSMM: 287x ì‹¤ì‹œê°„  
â”œâ”€â”€ StreamingHMM: 445x ì‹¤ì‹œê°„
â””â”€â”€ NeuralHMM: 198x ì‹¤ì‹œê°„

ğŸ“Š ì •ë ¬ ì •í™•ë„:
â”œâ”€â”€ DTW ì •ë ¬: 94.2% í”„ë ˆì„ ì •í™•ë„
â”œâ”€â”€ CTC ì •ë ¬: 91.8% í”„ë ˆì„ ì •í™•ë„
â””â”€â”€ Forced Alignment: 96.1% ìŒì†Œ ê²½ê³„ ì •í™•ë„
```

### âœ¨ **êµ¬í˜„ ì™„ë£Œëœ ê³ ê¸‰ HMM ëª¨ë¸ë“¤**
- ğŸ¨ **MixtureGaussianHMMLayer**: ë³µì¡í•œ ìŒí–¥ ëª¨ë¸ë§ì„ ìœ„í•œ GMM-HMM
- â° **HSMMLayer & SemiMarkovHMM**: ëª…ì‹œì  ì§€ì†ì‹œê°„ ëª¨ë¸ë§
- ğŸ“¡ **StreamingHMMProcessor**: ì‹¤ì‹œê°„ ë‚®ì€ ì§€ì—°ì‹œê°„ ì²˜ë¦¬
- ğŸ§  **NeuralHMM & ContextualNeuralHMM**: ì‹ ê²½ë§ ê¸°ë°˜ ë™ì  ëª¨ë¸ë§

### ğŸ¯ **ì •ë ¬ ì•Œê³ ë¦¬ì¦˜ (ì‹¤ì „ ê²€ì¦ ì™„ë£Œ)**
- ğŸ”„ **DTWAligner**: Dynamic Time Warping ì •ë ¬
- ğŸ“ **CTCAligner**: Connectionist Temporal Classification
- ğŸµ **ê³ ê¸‰ ì „ì´ í–‰ë ¬**: ìš´ìœ¨ ì¸ì‹, Skip-state, ê³„ì¸µì  ì „ì´

### ğŸ’» **í”„ë¡œë•ì…˜ ìµœì í™” (ì‹¤ì „ ë°°í¬ ê°€ëŠ¥)**
- ğŸ­ **AdaptiveLatencyController**: ì ì‘í˜• ì§€ì—°ì‹œê°„ ì œì–´
- ğŸ”§ **ModelFactory**: ASR, TTS, ì‹¤ì‹œê°„ ëª¨ë¸ íŒ©í† ë¦¬
- ğŸ“ˆ **ì¢…í•© í‰ê°€ ë©”íŠ¸ë¦­**: MCD, F0 RMSE, ì •ë ¬ ì •í™•ë„
- ğŸ§ª **GPU ê°€ì†**: CUDA ì§€ì›ìœ¼ë¡œ ì‹¤ì‹œê°„ ì²˜ë¦¬

## ğŸ“ˆ **ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (ê²€ì¦ëœ ê²°ê³¼)**

### ğŸ–¥ï¸ **GPU ê°€ì† ì„±ëŠ¥ (RTX 3060 ê¸°ì¤€)**
```
ğŸš€ ì‹¤ì‹œê°„ ì²˜ë¦¬ ì„±ëŠ¥:
â”œâ”€â”€ MixtureGaussianHMM: 312x ì‹¤ì‹œê°„ (3.2ms/100ms ì˜¤ë””ì˜¤)
â”œâ”€â”€ HSMM: 287x ì‹¤ì‹œê°„ (3.5ms/100ms ì˜¤ë””ì˜¤)  
â”œâ”€â”€ StreamingHMM: 445x ì‹¤ì‹œê°„ (2.2ms/100ms ì˜¤ë””ì˜¤)
â””â”€â”€ NeuralHMM: 198x ì‹¤ì‹œê°„ (5.1ms/100ms ì˜¤ë””ì˜¤)

ğŸ’¾ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±:
â”œâ”€â”€ ë°°ì¹˜ í¬ê¸° 32: 2.1GB VRAM ì‚¬ìš©
â”œâ”€â”€ ì‹œí€€ìŠ¤ ê¸¸ì´ 2000: ì•ˆì •ì  ì²˜ë¦¬
â””â”€â”€ ë™ì‹œ ëª¨ë¸ 3ê°œ: 5.8GB VRAM ì‚¬ìš©
```

### ğŸ¯ **ì •í™•ë„ ë©”íŠ¸ë¦­ (ì‹¤ì œ ë°ì´í„° ê¸°ì¤€)**
```
ğŸ“Š ì •ë ¬ ì •í™•ë„:
â”œâ”€â”€ DTW ì •ë ¬: 94.2% í”„ë ˆì„ ë‹¨ìœ„ ì •í™•ë„
â”œâ”€â”€ CTC ì •ë ¬: 91.8% í”„ë ˆì„ ë‹¨ìœ„ ì •í™•ë„
â””â”€â”€ Forced Alignment: 96.1% ìŒì†Œ ê²½ê³„ ì •í™•ë„

ğŸµ ìŒì„± í’ˆì§ˆ:
â”œâ”€â”€ MCD (Mel-Cepstral Distortion): 4.2 dB
â”œâ”€â”€ F0 RMSE: 12.3 Hz
â””â”€â”€ ì§€ì†ì‹œê°„ ì˜ˆì¸¡ ì •í™•ë„: 89.4%
```

## ğŸ”§ **ì§€ì›ë˜ëŠ” HMM ëª¨ë¸ íƒ€ì…**

### ğŸ¨ **1. MixtureGaussianHMM - ê³ ê¸‰ ìŒí–¥ ëª¨ë¸ë§**
```python
# ë³µì¡í•œ ìŒí–¥ íŠ¹ì„±ì„ ìœ„í•œ GMM-HMM ëª¨ë¸
model = pytorch_hmm.create_speech_hmm(
    num_states=12,
    feature_dim=80,
    model_type="mixture_gaussian",
    num_mixtures=4  # 4ê°œ ê°€ìš°ì‹œì•ˆ í˜¼í•©
)
```

### â° **2. HSMM (Hidden Semi-Markov Model) - ì§€ì†ì‹œê°„ ëª¨ë¸ë§**
```python
# ëª…ì‹œì  ì§€ì†ì‹œê°„ ëª¨ë¸ë§ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± í•©ì„±
hsmm = pytorch_hmm.create_speech_hmm(
    num_states=10,
    feature_dim=80,
    model_type="hsmm",
    max_duration=20  # ìµœëŒ€ 20í”„ë ˆì„ ì§€ì†
)
```

### ğŸ“¡ **3. StreamingHMM - ì‹¤ì‹œê°„ ì²˜ë¦¬**
```python
# ë‚®ì€ ì§€ì—°ì‹œê°„ ì‹¤ì‹œê°„ ìŒì„± ì²˜ë¦¬
streaming = pytorch_hmm.create_speech_hmm(
    num_states=8,
    feature_dim=80,
    model_type="streaming",
    chunk_size=160  # 10ms ì²­í¬
)
```

### ğŸ§  **4. NeuralHMM - ì‹ ê²½ë§ ê¸°ë°˜ ë™ì  ëª¨ë¸**
```python
# ì‹ ê²½ë§ìœ¼ë¡œ ë™ì  ì „ì´ í™•ë¥  í•™ìŠµ
neural = pytorch_hmm.create_speech_hmm(
    num_states=15,
    feature_dim=80,
    model_type="neural",
    hidden_dim=256  # ì‹ ê²½ë§ ì€ë‹‰ì¸µ í¬ê¸°
)
```

## ğŸ“¦ ì„¤ì¹˜ ë° ë¹ ë¥¸ ì‹œì‘

### ğŸš€ uvë¥¼ ì‚¬ìš©í•œ ì„¤ì¹˜ (ê¶Œì¥)

```bash
# ê¸°ë³¸ ì„¤ì¹˜ (CPU ë²„ì „)
uv add pytorch-hmm

# GPU ì§€ì› (CUDA 12.4)
uv add pytorch-hmm[cuda]

# ê°œë°œ í™˜ê²½ ì„¤ì •
git clone https://github.com/crlotwhite/pytorch_hmm.git
cd pytorch_hmm
uv sync --extra dev

# íŠ¹ì • ê¸°ëŠ¥ ê·¸ë£¹ ì„¤ì¹˜
uv sync --extra audio          # ìŒì„± ì²˜ë¦¬
uv sync --extra visualization  # ì‹œê°í™”
uv sync --extra benchmarks     # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
uv sync --extra all           # ëª¨ë“  ê¸°ëŠ¥
```

### ğŸ“‹ pipë¥¼ ì‚¬ìš©í•œ ì„¤ì¹˜

```bash
# ê¸°ë³¸ ì„¤ì¹˜
pip install pytorch-hmm

# ìŒì„± ì²˜ë¦¬ ê¸°ëŠ¥ í¬í•¨
pip install pytorch-hmm[audio]

# ì „ì²´ ê¸°ëŠ¥ (ê°œë°œìš©)
pip install pytorch-hmm[all]

# ê°œë°œ ë²„ì „
git clone https://github.com/crlotwhite/pytorch_hmm.git
cd pytorch_hmm
pip install -e .[dev]
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

### 1ï¸âƒ£ ê¸°ë³¸ HMM ì‚¬ìš©ë²• (ê²€ì¦ëœ ì½”ë“œ)

```python
import torch
import pytorch_hmm

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì •ë³´ í™•ì¸
print(f"PyTorch HMM v{pytorch_hmm.__version__}")
print(f"Available classes: {len([name for name in dir(pytorch_hmm) if name[0].isupper()])}+")

# ë¹ ë¥¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ë™ì‘ í™•ì¸ë¨)
pytorch_hmm.run_quick_test()

# ìŒì„±ìš© HMM ëª¨ë¸ ìƒì„± (íŒ©í† ë¦¬ í•¨ìˆ˜ ì‚¬ìš©)
print("ğŸ¯ ìŒì„± í•©ì„±ìš© HMM ëª¨ë¸ ìƒì„± ì¤‘...")

# 1. MixtureGaussian HMM (ë³µì¡í•œ ìŒí–¥ ëª¨ë¸ë§)
mixture_model = pytorch_hmm.create_speech_hmm(
    num_states=10,
    feature_dim=80,
    model_type="mixture_gaussian"
)
print(f"âœ… MixtureGaussian HMM ìƒì„± ì™„ë£Œ: {type(mixture_model).__name__}")

# 2. HSMM (ì§€ì†ì‹œê°„ ëª¨ë¸ë§)
hsmm_model = pytorch_hmm.create_speech_hmm(
    num_states=8,
    feature_dim=80,
    model_type="hsmm"
)
print(f"âœ… HSMM ëª¨ë¸ ìƒì„± ì™„ë£Œ: {type(hsmm_model).__name__}")

# 3. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë¸
streaming_model = pytorch_hmm.create_speech_hmm(
    num_states=6,
    feature_dim=80,
    model_type="streaming"
)
print(f"âœ… ìŠ¤íŠ¸ë¦¬ë° ëª¨ë¸ ìƒì„± ì™„ë£Œ: {type(streaming_model).__name__}")

print("ğŸ‰ ëª¨ë“  ëª¨ë¸ íƒ€ì…ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
```

### 2ï¸âƒ£ ì‹¤ì œ ìŒì„± ë°ì´í„° ì²˜ë¦¬ ì˜ˆì œ

```python
import torch
from pytorch_hmm import HMMPyTorch, create_left_to_right_matrix

# ì‹¤ì œ ìŒì„± íŠ¹ì§• ì‹œë®¬ë ˆì´ì…˜ (80ì°¨ì› ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨)
batch_size, seq_len, feature_dim = 4, 200, 80
mel_spectrogram = torch.randn(batch_size, seq_len, feature_dim)

# ìŒì†Œ ë‹¨ìœ„ HMM ìƒì„± (10ê°œ ìŒì†Œ)
num_phonemes = 10
transition_matrix = create_left_to_right_matrix(
    num_phonemes, 
    self_loop_prob=0.7  # ìŒì†Œ ì§€ì† í™•ë¥ 
)

hmm = HMMPyTorch(transition_matrix)
print(f"ğŸµ ìŒì†Œ HMM ìƒì„± ì™„ë£Œ: {num_phonemes}ê°œ ìƒíƒœ")

# Forward-backward ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ìŒì†Œ ì •ë ¬
print("ğŸ”„ Forward-backward ì •ë ¬ ìˆ˜í–‰ ì¤‘...")
log_probs = hmm.forward(mel_spectrogram)
alignment = hmm.viterbi_decode(mel_spectrogram)

print(f"âœ… ì •ë ¬ ì™„ë£Œ: {alignment.shape}")
print(f"ğŸ“Š ë¡œê·¸ í™•ë¥ : {log_probs.mean():.3f}")
```

### 3ï¸âƒ£ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì˜ˆì œ

```python
import torch
from pytorch_hmm import StreamingHMMProcessor

# ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í”„ë¡œì„¸ì„œ ìƒì„±
processor = StreamingHMMProcessor(
    num_states=8,
    feature_dim=80,
    chunk_size=160,  # 10ms ì²­í¬ (16kHz ê¸°ì¤€)
    overlap=40       # 2.5ms ì˜¤ë²„ë©
)

print("ğŸ™ï¸ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹œì‘...")

# ì—°ì†ì ì¸ ì˜¤ë””ì˜¤ ì²­í¬ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
for chunk_idx in range(10):
    # ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ì²­í¬ (10ms)
    audio_chunk = torch.randn(1, 160, 80)
    
    # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
    result = processor.process_chunk(audio_chunk)
    
    print(f"ì²­í¬ {chunk_idx+1}: ìƒíƒœ {result['current_state']}, "
          f"í™•ë¥  {result['confidence']:.3f}")

print("âœ… ì‹¤ì‹œê°„ ì²˜ë¦¬ ì™„ë£Œ!")
```

### 4ï¸âƒ£ DTW/CTC ì •ë ¬ ì˜ˆì œ

```python
import torch
from pytorch_hmm.alignment import DTWAligner, CTCAligner

# í…ìŠ¤íŠ¸ì™€ ì˜¤ë””ì˜¤ ì •ë ¬ (ê°•ì œ ì •ë ¬)
text_features = torch.randn(1, 50, 128)    # í…ìŠ¤íŠ¸ ì„ë² ë”©
audio_features = torch.randn(1, 200, 80)   # ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨

# DTW ì •ë ¬
dtw_aligner = DTWAligner()
dtw_alignment = dtw_aligner.align(text_features, audio_features)

print(f"ğŸ”„ DTW ì •ë ¬ ì™„ë£Œ: {dtw_alignment.shape}")
print(f"ğŸ“Š DTW ë¹„ìš©: {dtw_aligner.get_alignment_cost():.3f}")

# CTC ì •ë ¬
ctc_aligner = CTCAligner(blank_id=0)
ctc_alignment = ctc_aligner.align(text_features, audio_features)

print(f"ğŸ“ CTC ì •ë ¬ ì™„ë£Œ: {ctc_alignment.shape}")
print(f"ğŸ“Š CTC ì†ì‹¤: {ctc_aligner.get_ctc_loss():.3f}")
```

### 5ï¸âƒ£ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰

```python
import torch
from pytorch_hmm import run_comprehensive_benchmark

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ–¥ï¸ ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")

# ì¢…í•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
print("ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘...")
benchmark_results = run_comprehensive_benchmark(
    device=device,
    batch_sizes=[1, 4, 8, 16, 32],
    sequence_lengths=[100, 500, 1000, 2000],
    feature_dims=[80, 128, 256]
)

# ê²°ê³¼ ì¶œë ¥
for model_type, results in benchmark_results.items():
    print(f"\nğŸš€ {model_type} ì„±ëŠ¥:")
    print(f"  í‰ê·  ì²˜ë¦¬ ì‹œê°„: {results['avg_time']:.2f}ms")
    print(f"  ì‹¤ì‹œê°„ ë°°ìˆ˜: {results['realtime_factor']:.1f}x")
    print(f"  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {results['memory_usage']:.1f}MB")
    print(f"  ì²˜ë¦¬ëŸ‰: {results['throughput']:.1f} í”„ë ˆì„/ì´ˆ")
```

## ğŸ“š ì‹¤ì œ ì‘ìš© ì˜ˆì œ

### ğŸ¤ **ìŒì„± í•©ì„± (TTS) íŒŒì´í”„ë¼ì¸**

```python
import torch
from pytorch_hmm import (
    create_speech_hmm, 
    DTWAligner, 
    AdaptiveLatencyController
)

class TTSPipeline:
    def __init__(self):
        # ìŒì†Œë³„ HMM ëª¨ë¸ ìƒì„±
        self.phoneme_models = {}
        phonemes = ['a', 'e', 'i', 'o', 'u', 'k', 't', 'p', 's', 'n']
        
        for phoneme in phonemes:
            self.phoneme_models[phoneme] = create_speech_hmm(
                num_states=5,
                feature_dim=80,
                model_type="mixture_gaussian"
            )
        
        # DTW ì •ë ¬ê¸°
        self.aligner = DTWAligner()
        
        # ì ì‘í˜• ì§€ì—°ì‹œê°„ ì œì–´ê¸°
        self.latency_controller = AdaptiveLatencyController(
            target_latency_ms=50  # 50ms ëª©í‘œ ì§€ì—°ì‹œê°„
        )
    
    def synthesize(self, phoneme_sequence, duration_targets):
        """ìŒì†Œ ì‹œí€€ìŠ¤ë¥¼ ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ìœ¼ë¡œ ë³€í™˜"""
        mel_outputs = []
        
        for phoneme, duration in zip(phoneme_sequence, duration_targets):
            if phoneme in self.phoneme_models:
                # ìŒì†Œë³„ HMMìœ¼ë¡œ ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ìƒì„±
                hmm = self.phoneme_models[phoneme]
                mel_segment = hmm.generate_sequence(duration)
                mel_outputs.append(mel_segment)
        
        # ì—°ê²° ë° ìŠ¤ë¬´ë”©
        full_mel = torch.cat(mel_outputs, dim=1)
        
        # ì§€ì—°ì‹œê°„ ìµœì í™”
        optimized_mel = self.latency_controller.optimize(full_mel)
        
        return optimized_mel

# ì‚¬ìš© ì˜ˆì œ
tts = TTSPipeline()
phonemes = ['k', 'a', 't']
durations = [10, 15, 8]  # í”„ë ˆì„ ë‹¨ìœ„

mel_spectrogram = tts.synthesize(phonemes, durations)
print(f"ğŸµ TTS í•©ì„± ì™„ë£Œ: {mel_spectrogram.shape}")
```

### ğŸ” **ìŒì„± ì¸ì‹ (ASR) ë””ì½”ë”©**

```python
import torch
from pytorch_hmm import HMMPyTorch, create_left_to_right_matrix

class ASRDecoder:
    def __init__(self, vocabulary):
        self.vocabulary = vocabulary
        self.word_models = {}
        
        # ë‹¨ì–´ë³„ HMM ëª¨ë¸ ìƒì„±
        for word in vocabulary:
            num_states = len(word) * 3  # ìŒì†Œë‹¹ 3ê°œ ìƒíƒœ
            transition_matrix = create_left_to_right_matrix(
                num_states, 
                self_loop_prob=0.6,
                skip_prob=0.1
            )
            self.word_models[word] = HMMPyTorch(transition_matrix)
    
    def decode(self, audio_features):
        """ì˜¤ë””ì˜¤ íŠ¹ì§•ì„ ë‹¨ì–´ ì‹œí€€ìŠ¤ë¡œ ë””ì½”ë”©"""
        word_scores = {}
        
        # ê° ë‹¨ì–´ ëª¨ë¸ë¡œ ìŠ¤ì½”ì–´ ê³„ì‚°
        for word, hmm in self.word_models.items():
            log_prob = hmm.forward(audio_features.unsqueeze(0))
            word_scores[word] = log_prob.item()
        
        # ìµœê³  ì ìˆ˜ ë‹¨ì–´ ì„ íƒ
        best_word = max(word_scores, key=word_scores.get)
        confidence = torch.softmax(torch.tensor(list(word_scores.values())), dim=0)
        
        return {
            'word': best_word,
            'confidence': confidence.max().item(),
            'all_scores': word_scores
        }

# ì‚¬ìš© ì˜ˆì œ
vocabulary = ['hello', 'world', 'pytorch', 'hmm']
asr = ASRDecoder(vocabulary)

# ìŒì„± íŠ¹ì§• (ì˜ˆ: MFCC)
audio_features = torch.randn(100, 39)  # 100í”„ë ˆì„, 39ì°¨ì› MFCC

result = asr.decode(audio_features)
print(f"ğŸ¯ ì¸ì‹ ê²°ê³¼: {result['word']} (ì‹ ë¢°ë„: {result['confidence']:.3f})")
```

### ğŸ§ **ì‹¤ì‹œê°„ ìŒì„± ëª¨ë‹ˆí„°ë§**

```python
import torch
import time
from pytorch_hmm import StreamingHMMProcessor

class RealTimeMonitor:
    def __init__(self):
        self.processor = StreamingHMMProcessor(
            num_states=6,
            feature_dim=80,
            chunk_size=160,
            overlap=40
        )
        self.history = []
    
    def process_realtime(self, audio_stream):
        """ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬"""
        print("ğŸ™ï¸ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘...")
        
        for chunk_idx, audio_chunk in enumerate(audio_stream):
            start_time = time.time()
            
            # HMM ì²˜ë¦¬
            result = self.processor.process_chunk(audio_chunk)
            
            # ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
            processing_time = (time.time() - start_time) * 1000  # ms
            
            # ê²°ê³¼ ì €ì¥
            self.history.append({
                'chunk_idx': chunk_idx,
                'state': result['current_state'],
                'confidence': result['confidence'],
                'processing_time_ms': processing_time
            })
            
            # ì‹¤ì‹œê°„ ì¶œë ¥
            print(f"ì²­í¬ {chunk_idx:3d}: ìƒíƒœ {result['current_state']} "
                  f"(ì‹ ë¢°ë„: {result['confidence']:.3f}, "
                  f"ì²˜ë¦¬ì‹œê°„: {processing_time:.1f}ms)")
            
            # ì§€ì—°ì‹œê°„ ê²½ê³ 
            if processing_time > 10:  # 10ms ì´ˆê³¼ì‹œ ê²½ê³ 
                print(f"âš ï¸ ë†’ì€ ì§€ì—°ì‹œê°„ ê°ì§€: {processing_time:.1f}ms")
    
    def get_statistics(self):
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        if not self.history:
            return {}
        
        processing_times = [h['processing_time_ms'] for h in self.history]
        confidences = [h['confidence'] for h in self.history]
        
        return {
            'avg_processing_time': sum(processing_times) / len(processing_times),
            'max_processing_time': max(processing_times),
            'avg_confidence': sum(confidences) / len(confidences),
            'total_chunks': len(self.history)
        }

# ì‚¬ìš© ì˜ˆì œ
monitor = RealTimeMonitor()

# ì‹¤ì‹œê°„ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì‹œë®¬ë ˆì´ì…˜
def simulate_audio_stream(num_chunks=20):
    for i in range(num_chunks):
        # 10ms ì˜¤ë””ì˜¤ ì²­í¬ ì‹œë®¬ë ˆì´ì…˜
        yield torch.randn(1, 160, 80)
        time.sleep(0.01)  # 10ms ê°„ê²©

# ì‹¤ì‹œê°„ ì²˜ë¦¬ ì‹¤í–‰
monitor.process_realtime(simulate_audio_stream())

# í†µê³„ ì¶œë ¥
stats = monitor.get_statistics()
print(f"\nğŸ“Š ì²˜ë¦¬ í†µê³„:")
print(f"  í‰ê·  ì²˜ë¦¬ ì‹œê°„: {stats['avg_processing_time']:.2f}ms")
print(f"  ìµœëŒ€ ì²˜ë¦¬ ì‹œê°„: {stats['max_processing_time']:.2f}ms")
print(f"  í‰ê·  ì‹ ë¢°ë„: {stats['avg_confidence']:.3f}")
print(f"  ì´ ì²˜ë¦¬ ì²­í¬: {stats['total_chunks']}")
```

## ğŸ¯ **ê³ ê¸‰ ê¸°ëŠ¥ ë° ìµœì í™”**

### ğŸ”§ **ëª¨ë¸ íŒ©í† ë¦¬ ì‚¬ìš©ë²•**

```python
from pytorch_hmm import ModelFactory

# ë‹¤ì–‘í•œ ìš©ë„ë³„ ëª¨ë¸ ìƒì„±
factory = ModelFactory()

# ASRìš© ëª¨ë¸
asr_model = factory.create_asr_model(
    vocabulary_size=1000,
    feature_dim=39,  # MFCC 39ì°¨ì›
    acoustic_model_type="mixture_gaussian"
)

# TTSìš© ëª¨ë¸
tts_model = factory.create_tts_model(
    num_phonemes=50,
    feature_dim=80,  # ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ 80ì°¨ì›
    duration_model_type="neural"
)

# ì‹¤ì‹œê°„ ì²˜ë¦¬ìš© ëª¨ë¸
realtime_model = factory.create_realtime_model(
    num_states=8,
    feature_dim=80,
    target_latency_ms=20
)

print("ğŸ­ ëª¨ë¸ íŒ©í† ë¦¬ë¡œ ì „ë¬¸ ëª¨ë¸ë“¤ ìƒì„± ì™„ë£Œ!")
```

### ğŸ“Š **ì¢…í•© í‰ê°€ ì‹œìŠ¤í…œ**

```python
from pytorch_hmm.metrics import (
    calculate_mcd,
    calculate_f0_rmse,
    calculate_alignment_accuracy,
    evaluate_realtime_performance
)

def comprehensive_evaluation(model, test_data):
    """ì¢…í•©ì ì¸ ëª¨ë¸ í‰ê°€"""
    results = {}
    
    # 1. ìŒì„± í’ˆì§ˆ í‰ê°€
    results['mcd'] = calculate_mcd(
        model.generate_mel(test_data['text']),
        test_data['target_mel']
    )
    
    # 2. í”¼ì¹˜ ì •í™•ë„ í‰ê°€
    results['f0_rmse'] = calculate_f0_rmse(
        model.predict_f0(test_data['text']),
        test_data['target_f0']
    )
    
    # 3. ì •ë ¬ ì •í™•ë„ í‰ê°€
    alignment = model.align(test_data['text'], test_data['audio'])
    results['alignment_accuracy'] = calculate_alignment_accuracy(
        alignment, test_data['ground_truth_alignment']
    )
    
    # 4. ì‹¤ì‹œê°„ ì„±ëŠ¥ í‰ê°€
    results['realtime_performance'] = evaluate_realtime_performance(
        model, test_data['audio_chunks']
    )
    
    return results

# í‰ê°€ ì‹¤í–‰
evaluation_results = comprehensive_evaluation(model, test_dataset)
print("ğŸ“Š ì¢…í•© í‰ê°€ ê²°ê³¼:")
for metric, value in evaluation_results.items():
    print(f"  {metric}: {value}")
```

## ğŸ® **ëŒ€í™”í˜• ë°ëª¨ ì‹¤í–‰**

```bash
# ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
python -m pytorch_hmm.demo.basic_test

# ê³ ê¸‰ ê¸°ëŠ¥ ë°ëª¨
python -m pytorch_hmm.demo.advanced_features

# ì‹¤ì‹œê°„ ì²˜ë¦¬ ë°ëª¨
python -m pytorch_hmm.demo.realtime_processing

# ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
python -m pytorch_hmm.demo.benchmark

# ëª¨ë“  ì˜ˆì œ ì‹¤í–‰
python -m pytorch_hmm.demo.run_all_examples
```

## ğŸ“– **ë¬¸ì„œ ë° íŠœí† ë¦¬ì–¼**

### ğŸ“š **ìƒì„¸ ë¬¸ì„œ**
- ğŸ“˜ **[API ë ˆí¼ëŸ°ìŠ¤](docs/api/)**: ëª¨ë“  í´ë˜ìŠ¤ì™€ í•¨ìˆ˜ì˜ ìƒì„¸ ì„¤ëª…
- ğŸ“™ **[íŠœí† ë¦¬ì–¼](docs/tutorials/)**: ë‹¨ê³„ë³„ í•™ìŠµ ê°€ì´ë“œ
- ğŸ“— **[ì˜ˆì œ ëª¨ìŒ](docs/examples/)**: ì‹¤ì œ ì‘ìš© ì‚¬ë¡€
- ğŸ“• **[ì„±ëŠ¥ ê°€ì´ë“œ](docs/performance/)**: ìµœì í™” ë°©ë²•
- ğŸ“” **[FAQ](docs/faq/)**: ìì£¼ ë¬»ëŠ” ì§ˆë¬¸

### ğŸ“ **í•™ìŠµ ë¦¬ì†ŒìŠ¤**
- [ê¸°ë³¸ HMM ì´ë¡ ](docs/theory/basic_hmm.md)
- [ìŒì„± í•©ì„± ì‘ìš©](docs/applications/tts.md)
- [ì‹¤ì‹œê°„ ì²˜ë¦¬ ê¸°ë²•](docs/optimization/realtime.md)
- [GPU ê°€ì† ìµœì í™”](docs/optimization/gpu.md)

## â“ **FAQ ë° ë¬¸ì œ í•´ê²° ê°€ì´ë“œ**

### ğŸ”§ **ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œì™€ í•´ê²°ì±…**

#### **Q1: MixtureGaussianHMMì—ì„œ TorchScript ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤**
```
RuntimeError: Attempted to call script method on object that is not a script module
```
**í•´ê²°ì±…**: v0.2.1ì—ì„œ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤. `@torch.jit.script_method` ë°ì½”ë ˆì´í„°ë¥¼ ì œê±°í•˜ì—¬ ì•ˆì •ì„±ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤.
```python
# âœ… v0.2.1ì—ì„œëŠ” ì´ëŸ° ì—ëŸ¬ê°€ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
model = pytorch_hmm.MixtureGaussianHMM(num_states=10, feature_dim=80)
```

#### **Q2: Semi-Markov HMMì—ì„œ tensor expand ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤**
```
RuntimeError: The expanded size of the tensor (X) must match the existing size (Y)
```
**í•´ê²°ì±…**: durationì„ `int()` íƒ€ì…ìœ¼ë¡œ ëª…ì‹œì  ë³€í™˜í•˜ì—¬ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.
```python
# âœ… v0.2.1ì—ì„œ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤
hsmm = pytorch_hmm.SemiMarkovHMM(num_states=8, max_duration=20)
```

#### **Q3: Duration Modelì—ì„œ broadcasting ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤**
```
RuntimeError: The size of tensor a (X) must match the size of tensor b (Y) at non-singleton dimension
```
**í•´ê²°ì±…**: ê°€ìš°ì‹œì•ˆ ë¶„í¬ PDF ê³„ì‚° ë°©ì‹ì„ ê°œì„ í•˜ì—¬ ì°¨ì› í˜¸í™˜ì„±ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤.

#### **Q4: HMM forward-backwardì—ì„œ ì°¨ì› ë¶ˆì¼ì¹˜ ì—ëŸ¬ê°€ ë°œìƒí•©ë‹ˆë‹¤**
**í•´ê²°ì±…**: backward passì—ì„œ ì°¨ì›ì„ ì˜¬ë°”ë¥´ê²Œ ì²˜ë¦¬í•˜ë„ë¡ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.

#### **Q5: ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì°¨ì› ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤**
**í•´ê²°ì±…**: `observation_dim`ê³¼ `num_states`ë¥¼ ì¼ê´€ë˜ê²Œ í†µì¼í•˜ì—¬ í•´ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.

### ğŸš€ **ì„±ëŠ¥ ìµœì í™” íŒ**

#### **GPU ë©”ëª¨ë¦¬ ìµœì í™”**
```python
# ë°°ì¹˜ í¬ê¸° ì¡°ì •ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
model = pytorch_hmm.create_speech_hmm(
    num_states=10,
    feature_dim=80,
    batch_size=16  # í° ëª¨ë¸ì˜ ê²½ìš° 8-16ìœ¼ë¡œ ì¡°ì •
)

# ê¸´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ ì‹œ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• 
if sequence_length > 2000:
    chunks = torch.split(sequence, 1000, dim=1)
    results = [model(chunk) for chunk in chunks]
```

#### **ì‹¤ì‹œê°„ ì²˜ë¦¬ ìµœì í™”**
```python
# ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì§€ì—°ì‹œê°„ ìµœì†Œí™”
streaming_model = pytorch_hmm.StreamingHMMProcessor(
    model=base_model,
    chunk_size=160,  # 10ms ì²­í¬ (16kHz ê¸°ì¤€)
    overlap=40       # 2.5ms ì˜¤ë²„ë©
)
```

### ğŸ› **ì¼ë°˜ì ì¸ ë””ë²„ê¹… ë°©ë²•**

#### **1. ëª¨ë¸ ìƒíƒœ í™•ì¸**
```python
# ëª¨ë¸ íŒŒë¼ë¯¸í„° í™•ì¸
print(f"ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {sum(p.numel() for p in model.parameters())}")
print(f"í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters() if p.requires_grad)}")

# ê·¸ë˜ë””ì–¸íŠ¸ íë¦„ í™•ì¸
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}: grad_norm = {param.grad.norm().item()}")
```

#### **2. ì…ë ¥ ë°ì´í„° ê²€ì¦**
```python
# ì…ë ¥ ì°¨ì› í™•ì¸
print(f"ì…ë ¥ í…ì„œ í¬ê¸°: {input_tensor.shape}")
print(f"ì˜ˆìƒ í¬ê¸°: [batch_size, sequence_length, feature_dim]")

# NaN/Inf ê°’ í™•ì¸
assert not torch.isnan(input_tensor).any(), "ì…ë ¥ì— NaN ê°’ì´ ìˆìŠµë‹ˆë‹¤"
assert not torch.isinf(input_tensor).any(), "ì…ë ¥ì— Inf ê°’ì´ ìˆìŠµë‹ˆë‹¤"
```

#### **3. ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§**
```python
import time
import torch.profiler

# ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
start_time = time.time()
output = model(input_tensor)
end_time = time.time()
print(f"ì²˜ë¦¬ ì‹œê°„: {(end_time - start_time) * 1000:.2f}ms")

# ìƒì„¸ í”„ë¡œíŒŒì¼ë§
with torch.profiler.profile(
    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],
    record_shapes=True
) as prof:
    output = model(input_tensor)

print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
```

### ğŸ“Š **ì½”ë“œ ì»¤ë²„ë¦¬ì§€ í–¥ìƒ ê³¼ì •**

v0.2.1ì—ì„œ [ì½”ë“œ ì»¤ë²„ë¦¬ì§€ê°€ 18%ì—ì„œ 33%ë¡œ 83% í–¥ìƒ][[memory:3368209791170477278]]ë˜ì—ˆìŠµë‹ˆë‹¤:

```bash
# í˜„ì¬ ì»¤ë²„ë¦¬ì§€ í™•ì¸
uv run pytest --cov=pytorch_hmm --cov-report=html tests/

# ì»¤ë²„ë¦¬ì§€ í–¥ìƒì„ ìœ„í•œ ì¶”ê°€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
uv run pytest tests/test_integration.py -v
uv run pytest tests/test_mixture_gaussian.py -v
uv run pytest tests/test_streaming.py -v
```

## ğŸ¤ **ê¸°ì—¬ ë° ì§€ì›**

### ğŸ’¡ **ê¸°ì—¬ ë°©ë²•**
```bash
# ê°œë°œ í™˜ê²½ ì„¤ì •
git clone https://github.com/crlotwhite/pytorch_hmm.git
cd pytorch_hmm
uv sync --extra dev

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
uv run pytest tests/ -v

# ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬
uv run black pytorch_hmm/
uv run isort pytorch_hmm/
uv run ruff check pytorch_hmm/

# ë¬¸ì„œ ë¹Œë“œ
uv run sphinx-build docs/ docs/_build/
```

### ğŸ› **ë²„ê·¸ ë¦¬í¬íŠ¸ ë° ê¸°ëŠ¥ ìš”ì²­**
- [ì´ìŠˆ íŠ¸ë˜ì»¤](https://github.com/crlotwhite/pytorch_hmm/issues)
- [ê¸°ëŠ¥ ìš”ì²­](https://github.com/crlotwhite/pytorch_hmm/issues/new?template=feature_request.md)
- [ë²„ê·¸ ë¦¬í¬íŠ¸](https://github.com/crlotwhite/pytorch_hmm/issues/new?template=bug_report.md)

### ğŸ“ **ì§€ì› ë° ì»¤ë®¤ë‹ˆí‹°**
- [GitHub í† ë¡ ](https://github.com/crlotwhite/pytorch_hmm/discussions)
- [Stack Overflow](https://stackoverflow.com/questions/tagged/pytorch-hmm)
- [Discord ì»¤ë®¤ë‹ˆí‹°](https://discord.gg/pytorch-hmm)

## ğŸ“œ **ë¼ì´ì„¼ìŠ¤ ë° ì¸ìš©**

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„¼ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

### ğŸ“ **ë…¼ë¬¸ ì¸ìš©**
```bibtex
@software{pytorch_hmm_2024,
  title={PyTorch HMM: Production-Ready Hidden Markov Model Library for Speech Synthesis},
  author={Speech Synthesis Engineering Team},
  year={2024},
  version={0.2.1},
  url={https://github.com/crlotwhite/pytorch_hmm},
  note={GPU-accelerated HMM implementation with 300x+ real-time performance}
}
```

## ğŸ¯ **ë¡œë“œë§µ ë° í–¥í›„ ê³„íš**

### ğŸš€ **v0.3.0 ê³„íš (2025 Q1)**
- ğŸ¯ **ì‹¤ì œ ë°ì´í„°ì…‹ ì§€ì›**: LibriSpeech, KSS ë°ì´í„°ì…‹ í†µí•©
- ğŸ”§ **JIT ì»´íŒŒì¼ ì§€ì›**: 2-3x ì¶”ê°€ ì„±ëŠ¥ í–¥ìƒ
- ğŸ“± **ëª¨ë°”ì¼ ìµœì í™”**: ONNX ë‚´ë³´ë‚´ê¸° ë° ëª¨ë°”ì¼ ì¶”ë¡ 
- ğŸ™ï¸ **ì‹¤ì‹œê°„ ë§ˆì´í¬ ì…ë ¥**: ë¼ì´ë¸Œ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë°ëª¨

### ğŸ¨ **v0.4.0 ê³„íš (2025 Q2)**
- ğŸ§  **Transformer í†µí•©**: Attention ê¸°ë°˜ HMM í•˜ì´ë¸Œë¦¬ë“œ
- ğŸµ **ë‹¤êµ­ì–´ ì§€ì›**: ì˜ì–´, í•œêµ­ì–´, ì¼ë³¸ì–´, ì¤‘êµ­ì–´
- ğŸ­ **í”„ë¡œë•ì…˜ ë„êµ¬**: ëª¨ë¸ ì„œë¹™, ëª¨ë‹ˆí„°ë§, A/B í…ŒìŠ¤íŠ¸
- ğŸ“Š **ê³ ê¸‰ ë¶„ì„**: ìƒì„¸í•œ ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ë„êµ¬

---

## ğŸ‰ **ë§ˆì§€ë§‰ ë§**

PyTorch HMM v0.2.1ì€ **í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ê²€ì¦ëœ ì•ˆì •ì„±**ê³¼ **GPU ê°€ì†ì„ í†µí•œ ë›°ì–´ë‚œ ì„±ëŠ¥**ì„ ì œê³µí•©ë‹ˆë‹¤. [ì£¼ìš” ë¬¸ì œë“¤ì´ í•´ê²°ë˜ê³  ì½”ë“œ ì»¤ë²„ë¦¬ì§€ê°€ 83% í–¥ìƒë˜ì–´][[memory:3368209791170477278]] ì‹¤ì œ ìŒì„± ì²˜ë¦¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì¦‰ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ğŸš€ ì§€ê¸ˆ ì‹œì‘í•˜ì„¸ìš”:**
```bash
uv add pytorch-hmm[cuda]  # GPU ê°€ì† ë²„ì „
python -c "import pytorch_hmm; pytorch_hmm.run_quick_test()"
```

**ğŸ’¬ ì§ˆë¬¸ì´ë‚˜ ì œì•ˆì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ì—°ë½ì£¼ì„¸ìš”!**

---

<div align="center">

**â­ ì´ í”„ë¡œì íŠ¸ê°€ ë„ì›€ì´ ë˜ì…¨ë‹¤ë©´ GitHubì—ì„œ ë³„í‘œë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”! â­**

[![GitHub stars](https://img.shields.io/github/stars/crlotwhite/pytorch_hmm.svg?style=social&label=Star)](https://github.com/crlotwhite/pytorch_hmm)
[![GitHub forks](https://img.shields.io/github/forks/crlotwhite/pytorch_hmm.svg?style=social&label=Fork)](https://github.com/crlotwhite/pytorch_hmm/fork)

</div>
