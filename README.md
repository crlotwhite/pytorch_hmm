# ğŸ¯ PyTorch HMM v0.2.0 - ìŒì„± í•©ì„±ì— íŠ¹í™”ëœ Hidden Markov Model
ğŸ¯ **Advanced PyTorch Hidden Markov Model Library for Speech Synthesis**

[![CI](https://github.com/crlotwhite/pytorch_hmm/workflows/CI/badge.svg)](https://github.com/crlotwhite/pytorch_hmm/actions)
[![codecov](https://codecov.io/gh/crlotwhite/pytorch_hmm/branch/main/graph/badge.svg)](https://codecov.io/gh/crlotwhite/pytorch_hmm)
[![PyPI version](https://badge.fury.io/py/pytorch-hmm.svg)](https://badge.fury.io/py/pytorch-hmm)
[![Python versions](https://img.shields.io/pypi/pyversions/pytorch-hmm.svg)](https://pypi.org/project/pytorch-hmm/)

[![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 1.8+](https://img.shields.io/badge/PyTorch-1.8+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Version](https://img.shields.io/badge/version-0.2.0-green.svg)](https://github.com/crlotwhite/pytorch_hmm)

PyTorch ê¸°ë°˜ Hidden Markov Model êµ¬í˜„ì²´ë¡œ, **ìŒì„± í•©ì„±(TTS)ê³¼ ìŒì„± ì²˜ë¦¬**ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. Forward-backwardì™€ Viterbi ì•Œê³ ë¦¬ì¦˜ì„ ì§€ì›í•˜ë©°, autogradì™€ GPU ê°€ì†ì„ ì™„ë²½í•˜ê²Œ ì§€ì›í•©ë‹ˆë‹¤.

## ğŸš€ v0.2.0 ì£¼ìš” ê¸°ëŠ¥ (ì™„ë£Œë¨ âœ…)

### âœ¨ **ê³ ê¸‰ HMM ëª¨ë¸ë“¤**
- ğŸ¨ **MixtureGaussianHMMLayer**: ë³µì¡í•œ ìŒí–¥ ëª¨ë¸ë§ì„ ìœ„í•œ GMM-HMM
- â° **HSMMLayer & SemiMarkovHMM**: ëª…ì‹œì  ì§€ì†ì‹œê°„ ëª¨ë¸ë§
- ğŸ“¡ **StreamingHMMProcessor**: ì‹¤ì‹œê°„ ë‚®ì€ ì§€ì—°ì‹œê°„ ì²˜ë¦¬
- ğŸ§  **NeuralHMM & ContextualNeuralHMM**: ì‹ ê²½ë§ ê¸°ë°˜ ë™ì  ëª¨ë¸ë§

### ğŸ¯ **ì •ë ¬ ì•Œê³ ë¦¬ì¦˜ (ìƒˆë¡œ ì¶”ê°€)**
- ğŸ”„ **DTWAligner**: Dynamic Time Warping ì •ë ¬
- ğŸ“ **CTCAligner**: Connectionist Temporal Classification
- ğŸµ **ê³ ê¸‰ ì „ì´ í–‰ë ¬**: ìš´ìœ¨ ì¸ì‹, Skip-state, ê³„ì¸µì  ì „ì´

### ğŸ’» **í”„ë¡œë•ì…˜ ìµœì í™”**
- ğŸ­ **AdaptiveLatencyController**: ì ì‘í˜• ì§€ì—°ì‹œê°„ ì œì–´
- ğŸ”§ **ModelFactory**: ASR, TTS, ì‹¤ì‹œê°„ ëª¨ë¸ íŒ©í† ë¦¬
- ğŸ“ˆ **ì¢…í•© í‰ê°€ ë©”íŠ¸ë¦­**: MCD, F0 RMSE, ì •ë ¬ ì •í™•ë„
- ğŸ§ª **GPU ê°€ì†**: CUDA ì§€ì›ìœ¼ë¡œ ì‹¤ì‹œê°„ ì²˜ë¦¬ (300x+ ê°€ì†)

## ğŸ¯ **v0.2.0 í•µì‹¬ ê°œì„ ì‚¬í•­**

### ğŸ§  **Neural HMM with Contextual Modeling**
- **ContextualNeuralHMM**: ì–¸ì–´ì  ì»¨í…ìŠ¤íŠ¸ì™€ ìš´ìœ¨ ì •ë³´ë¥¼ í™œìš©í•œ ë™ì  ëª¨ë¸ë§
- **NeuralTransitionModel**: RNN/Transformer ê¸°ë°˜ ì „ì´ í™•ë¥  ê³„ì‚°
- **NeuralObservationModel**: ë³µì¡í•œ ìŒí–¥ íŠ¹ì§•ì„ ìœ„í•œ ì‹ ê²½ë§ ê´€ì¸¡ ëª¨ë¸

### â±ï¸ **Semi-Markov Models**
- **HSMMLayer**: ëª…ì‹œì  ì§€ì†ì‹œê°„ ëª¨ë¸ë§ (Gamma, Poisson ë¶„í¬ ì§€ì›)
- **AdaptiveDurationHSMM**: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì ì‘í˜• ì§€ì†ì‹œê°„ ì¡°ì ˆ
- **DurationConstrainedHMM**: ìƒíƒœë³„ ìµœì†Œ/ìµœëŒ€ ì§€ì†ì‹œê°„ ì œì•½

### ğŸ”„ **ê³ ê¸‰ ì •ë ¬ ë° ì „ì´**
- **DTWAligner**: Soft-DTW ì§€ì›ìœ¼ë¡œ ë¯¸ë¶„ ê°€ëŠ¥í•œ ì •ë ¬
- **CTCAligner**: End-to-end í•™ìŠµ ê°€ëŠ¥í•œ CTC ì •ë ¬
- **AdaptiveTransitionMatrix**: ì»¨í…ìŠ¤íŠ¸ ì¢…ì† ë™ì  ì „ì´ í–‰ë ¬

### ğŸ“Š **ì¢…í•© í‰ê°€ ì‹œìŠ¤í…œ**
- **ìŒì„± í’ˆì§ˆ ë©”íŠ¸ë¦­**: MCD, F0 RMSE, ìŠ¤í™íŠ¸ëŸ´ ì™œê³¡ ì¸¡ì •
- **ì •ë ¬ ì •í™•ë„**: ê²½ê³„ íƒì§€ ë° ì§€ì†ì‹œê°„ ì˜ˆì¸¡ í‰ê°€
- **ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹**: GPU ê°€ì†ìœ¼ë¡œ 300x+ ì‹¤ì‹œê°„ ì²˜ë¦¬ ë‹¬ì„±

## ğŸ“¦ ì„¤ì¹˜

### uvë¥¼ ì‚¬ìš©í•œ ì„¤ì¹˜ (ê¶Œì¥)

```bash
# ê¸°ë³¸ ì„¤ì¹˜ (CPU ë²„ì „)
uv add pytorch-hmm[cpu]

# GPU ì§€ì› (CUDA 12.4)
uv add pytorch-hmm[cuda]

# ê°œë°œ í™˜ê²½ ì„¤ì •
git clone https://github.com/crlotwhite/pytorch_hmm.git
cd pytorch_hmm
uv sync --extra dev

# íŠ¹ì • ê¸°ëŠ¥ ê·¸ë£¹ ì„¤ì¹˜
uv sync --extra audio      # ìŒì„± ì²˜ë¦¬
uv sync --extra visualization  # ì‹œê°í™”
uv sync --extra benchmarks # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
```

### PyTorch ë²„ì „ ì„ íƒ

ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” í”Œë«í¼ë³„ë¡œ ìµœì í™”ëœ PyTorch ë²„ì „ì„ ìë™ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤:

- **macOS**: CPU ì „ìš© ë²„ì „
- **Linux/Windows**: CUDA 12.4 ì§€ì› ë²„ì „ (GPU ê°€ìš©ì‹œ)

ìˆ˜ë™ìœ¼ë¡œ PyTorch ë²„ì „ì„ ì„ íƒí•˜ë ¤ë©´:

```bash
# CPU ì „ìš© ê°•ì œ ì„¤ì¹˜
uv sync --extra cpu

# CUDA ë²„ì „ ê°•ì œ ì„¤ì¹˜
uv sync --extra cuda
```

### pipë¥¼ ì‚¬ìš©í•œ ì„¤ì¹˜

```bash
# ê¸°ë³¸ ì„¤ì¹˜
pip install pytorch-hmm

# ìŒì„± ì²˜ë¦¬ ê¸°ëŠ¥ í¬í•¨
pip install pytorch-hmm[audio]

# ì „ì²´ ê¸°ëŠ¥ (ê°œë°œìš©)
pip install pytorch-hmm[all]

# ê°œë°œ ë²„ì „
git clone https://github.com/crlotwhite/pytorch_hmm.git
cd pytorch_hmm
pip install -e .[dev]
```

### ì„ íƒì  ì˜ì¡´ì„±

```bash
# ì˜¤ë””ì˜¤ ì²˜ë¦¬
pip install pytorch-hmm[audio]

# ì‹œê°í™”
pip install pytorch-hmm[visualization]

# ê°œë°œ ë„êµ¬
pip install pytorch-hmm[dev]

# ë²¤ì¹˜ë§ˆí¬ ë„êµ¬
pip install pytorch-hmm[benchmarks]

```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ê¸°ë³¸ HMM ì‚¬ìš©ë²•

```python
import torch
import pytorch_hmm

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì •ë³´ í™•ì¸
print(f"PyTorch HMM v{pytorch_hmm.__version__}")
print(f"Available classes: {len([name for name in dir(pytorch_hmm) if name[0].isupper()])}+")

# ë¹ ë¥¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
pytorch_hmm.run_quick_test()

# ìŒì„±ìš© HMM ëª¨ë¸ ìƒì„± (íŒ©í† ë¦¬ í•¨ìˆ˜ ì‚¬ìš©)
# ì§€ì› ëª¨ë¸ íƒ€ì…: 'mixture_gaussian', 'hsmm', 'streaming'

# 1. MixtureGaussian HMM (ë³µì¡í•œ ìŒí–¥ ëª¨ë¸ë§)
mixture_model = pytorch_hmm.create_speech_hmm(
    num_states=10,
    feature_dim=80,
    model_type="mixture_gaussian"
)

# 2. HSMM (ì§€ì†ì‹œê°„ ëª¨ë¸ë§)
hsmm_model = pytorch_hmm.create_speech_hmm(
    num_states=8,
    feature_dim=80,
    model_type="hsmm"
)

# 3. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë¸
streaming_model = pytorch_hmm.create_speech_hmm(
    num_states=6,
    feature_dim=80,
    model_type="streaming"
)

print("âœ… ëª¨ë“  ëª¨ë¸ íƒ€ì…ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
```

### ğŸ¨ Mixture Gaussian HMM

```python
from pytorch_hmm import create_speech_hmm, MixtureGaussianHMMLayer

# íŒ©í† ë¦¬ í•¨ìˆ˜ë¡œ ì•ˆì „í•˜ê²Œ ìƒì„± (ì¶”ì²œ)
mixture_model = create_speech_hmm(
    num_states=15,          # 15ê°œ ìŒì†Œ
    feature_dim=80,         # 80ì°¨ì› ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨
    model_type="mixture_gaussian"
)

print(f"MixtureGaussian HMM ìƒì„± ì™„ë£Œ")
print(f"ëª¨ë¸ íƒ€ì…: {type(mixture_model)}")

# ë˜ëŠ” ì§ì ‘ ìƒì„±
mixture_hmm = MixtureGaussianHMMLayer(
    num_states=10,
    feature_dim=80,
    num_components=3        # 3ê°œ ê°€ìš°ì‹œì•ˆ ë¯¹ìŠ¤ì²˜
)

print(f"ì§ì ‘ ìƒì„±ëœ HMM: {mixture_hmm.__class__.__name__}")
print(f"ìƒíƒœ ìˆ˜: {mixture_hmm.num_states}")
print(f"íŠ¹ì§• ì°¨ì›: {mixture_hmm.feature_dim}")
```

### â° Semi-Markov Model (HSMM)

```python
from pytorch_hmm import create_speech_hmm, HSMMLayer, SemiMarkovHMM

# íŒ©í† ë¦¬ í•¨ìˆ˜ë¡œ HSMM ìƒì„± (ì¶”ì²œ)
hsmm_model = create_speech_hmm(
    num_states=12,
    feature_dim=80,
    model_type="hsmm"
)

print(f"HSMM ëª¨ë¸ ìƒì„± ì™„ë£Œ: {type(hsmm_model)}")

# HSMMLayer ì§ì ‘ ìƒì„±
hsmm_layer = HSMMLayer(
    num_states=10,
    feature_dim=80,
    max_duration=30                 # ìµœëŒ€ 30í”„ë ˆì„
)

print(f"HSMMLayer: {hsmm_layer.__class__.__name__}")
print(f"ìƒíƒœ ìˆ˜: {hsmm_layer.num_states}")
print(f"ìµœëŒ€ ì§€ì†ì‹œê°„: {hsmm_layer.max_duration}")

# SemiMarkovHMM ìƒì„±
semi_markov = SemiMarkovHMM(
    num_states=8,
    observation_dim=80,
    max_duration=25
)

print(f"SemiMarkov HMM: {semi_markov.__class__.__name__}")
print(f"ê´€ì¸¡ ì°¨ì›: {semi_markov.observation_dim}")
```

### ğŸ“¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°

```python
from pytorch_hmm import create_speech_hmm, StreamingHMMProcessor, AdaptiveLatencyController

# íŒ©í† ë¦¬ í•¨ìˆ˜ë¡œ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë¸ ìƒì„± (ì¶”ì²œ)
streaming_model = create_speech_hmm(
    num_states=8,
    feature_dim=80,
    model_type="streaming"
)

print(f"ìŠ¤íŠ¸ë¦¬ë° ëª¨ë¸ ìƒì„±: {type(streaming_model)}")

# ìŠ¤íŠ¸ë¦¬ë° í”„ë¡œì„¸ì„œ ì§ì ‘ ìƒì„±
processor = StreamingHMMProcessor(
    num_states=6,
    feature_dim=80,
    chunk_size=100          # ì²­í¬ í¬ê¸°
)

print(f"í”„ë¡œì„¸ì„œ: {processor.__class__.__name__}")
print(f"ì²­í¬ í¬ê¸°: {processor.chunk_size}")

# ì ì‘í˜• ì§€ì—°ì‹œê°„ ì œì–´
controller = AdaptiveLatencyController(target_latency_ms=25.0)

print(f"ì§€ì—°ì‹œê°„ ì œì–´: {controller.__class__.__name__}")
print(f"ëª©í‘œ ì§€ì—°ì‹œê°„: {controller.target_latency_ms}ms")

# ì‹¤ì‹œê°„ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜ (ê°„ë‹¨ ë²„ì „)
print("âœ… ìŠ¤íŠ¸ë¦¬ë° êµ¬ì„± ìš”ì†Œë“¤ì´ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
```

### ğŸ‡°ğŸ‡· í•œêµ­ì–´ TTS ìµœì í™”

```python
from pytorch_hmm import create_korean_tts_hmm, get_speech_transitions

# í•œêµ­ì–´ ìŒì†Œ ì§‘í•©ìœ¼ë¡œ HMM ìƒì„±
korean_model = create_korean_tts_hmm(
    feature_dim=80,
    model_type="hsmm"  # ì§€ì†ì‹œê°„ ëª¨ë¸ë§ í¬í•¨
)

# í•œêµ­ì–´ TTSìš© ì „ì´ í–‰ë ¬ ìƒì„±
transitions = get_speech_transitions(
    num_states=20,
    speech_type="normal"  # "fast", "slow", "emotional" ë„ ì§€ì›
)

# ìŒì†Œ ì‹œí€€ìŠ¤: "ì•ˆë…•í•˜ì„¸ìš”"
phoneme_sequence = ['sil', 'a', 'n', 'n', 'eo', 'ng', 'h', 'a', 's', 'e', 'j', 'o', 'sil']

print(f"í•œêµ­ì–´ HMM ì •ë³´:")
print(f"  ì§€ì› ìŒì†Œ ìˆ˜: {len(phoneme_sequence)}")
print(f"  ì „ì´ í–‰ë ¬ í¬ê¸°: {transitions.shape}")
```

## ğŸ”§ ê³ ê¸‰ ì‚¬ìš©ë²•

### ì „ì´ í–‰ë ¬ ì»¤ìŠ¤í„°ë§ˆì´ì§•

```python
from pytorch_hmm.utils import (
    create_skip_state_matrix,
    create_phoneme_aware_transitions,
    create_prosody_aware_transitions
)

# ë¹ ë¥¸ ë°œí™”ìš© Skip-state ì „ì´
skip_transitions = create_skip_state_matrix(
    num_states=20,
    self_loop_prob=0.5,    # ë‚®ì€ ìœ ì§€ í™•ë¥ 
    forward_prob=0.4,      # ë†’ì€ ì „ì§„ í™•ë¥ 
    skip_prob=0.1,         # ê±´ë„ˆë›°ê¸° í—ˆìš©
    max_skip=2
)

# ìŒì†Œ ì§€ì†ì‹œê°„ ê¸°ë°˜ ì „ì´
phoneme_durations = [5, 8, 6, 12, 4, 9, 7, 11]  # ê° ìŒì†Œë³„ í‰ê·  ì§€ì†ì‹œê°„
duration_transitions = create_phoneme_aware_transitions(
    phoneme_durations,
    duration_variance=0.3
)

# ìš´ìœ¨ ì •ë³´ ê¸°ë°˜ ë™ì  ì „ì´
f0_contour = torch.randn(100)      # F0 ìœ¤ê³½
energy_contour = torch.randn(100)  # ì—ë„ˆì§€ ìœ¤ê³½

prosody_transitions = create_prosody_aware_transitions(
    f0_contour, energy_contour, num_states=10
)
print(f"ìš´ìœ¨ ê¸°ë°˜ ì „ì´ í–‰ë ¬: {prosody_transitions.shape}")  # (100, 10, 10)
```

### ì„±ëŠ¥ ìµœì í™”

```python
import pytorch_hmm

# í•˜ë“œì›¨ì–´ ìë™ ì„¤ì •
config_info = pytorch_hmm.auto_configure()
print(f"ìë™ ì„¤ì •: {config_info}")

# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
from pytorch_hmm.utils import benchmark_transition_operations

benchmark_results = benchmark_transition_operations(
    num_states_list=[5, 10, 20, 50],
    num_trials=100
)

for operation, results in benchmark_results.items():
    print(f"{operation}:")
    for num_states, time_ms in results.items():
        print(f"  {num_states} states: {time_ms:.2f}ms")

### 2. Neural HMMìœ¼ë¡œ ê³ ê¸‰ ëª¨ë¸ë§

```python
from pytorch_hmm import NeuralHMM, ContextualNeuralHMM

# ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ Neural HMM
contextual_hmm = ContextualNeuralHMM(
    num_states=10,
    observation_dim=80,          # ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì°¨ì›
    phoneme_vocab_size=50,       # ìŒì†Œ ê°œìˆ˜
    linguistic_context_dim=32,   # ì–¸ì–´ì  ì»¨í…ìŠ¤íŠ¸
    prosody_dim=8               # ìš´ìœ¨ íŠ¹ì§•
)

# ì…ë ¥ ë°ì´í„° ì¤€ë¹„
batch_size, seq_len = 2, 100
observations = torch.randn(batch_size, seq_len, 80)           # ìŒí–¥ íŠ¹ì§•
phoneme_sequence = torch.randint(0, 50, (batch_size, seq_len)) # ìŒì†Œ ì‹œí€€ìŠ¤
prosody_features = torch.randn(batch_size, seq_len, 8)        # ìš´ìœ¨ íŠ¹ì§•

# ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì •ë ¬
posteriors, forward, backward = contextual_hmm.forward_with_context(
    observations, phoneme_sequence, prosody_features)

print(f"Context-aware posteriors: {posteriors.shape}")
```

### 3. Semi-Markov HMMìœ¼ë¡œ ì§€ì†ì‹œê°„ ëª¨ë¸ë§

```python
from pytorch_hmm import SemiMarkovHMM

# ì§€ì†ì‹œê°„ ëª¨ë¸ë§ì´ ê°€ëŠ¥í•œ HSMM
hsmm = SemiMarkovHMM(
    num_states=8,
    observation_dim=40,
    max_duration=20,
    duration_distribution='gamma',  # ê°ë§ˆ ë¶„í¬ë¡œ ì§€ì†ì‹œê°„ ëª¨ë¸ë§
    observation_model='gaussian'
)

# ì‹œí€€ìŠ¤ ìƒ˜í”Œë§
state_seq, duration_seq, observations = hsmm.sample(
    num_states=6, max_length=100)

print(f"Sampled states: {state_seq}")
print(f"State durations: {duration_seq}")
print(f"Total frames: {duration_seq.sum()} frames")

# Viterbi ë””ì½”ë”©ìœ¼ë¡œ ìµœì  ìƒíƒœ-ì§€ì†ì‹œê°„ ì‹œí€€ìŠ¤ ì°¾ê¸°
optimal_states, optimal_durations, log_prob = hsmm.viterbi_decode(observations)
```

### 4. DTW ì •ë ¬

```python
from pytorch_hmm import DTWAligner

# DTW ì •ë ¬ê¸° ìƒì„±
aligner = DTWAligner(
    distance_fn='cosine',           # ì½”ì‚¬ì¸ ê±°ë¦¬
    step_pattern='symmetric',       # ëŒ€ì¹­ ìŠ¤í… íŒ¨í„´
    soft_dtw=True                  # ë¯¸ë¶„ ê°€ëŠ¥í•œ Soft DTW
)

# ìŒì†Œ íŠ¹ì§•ê³¼ ìŒì„± íŠ¹ì§• ì •ë ¬
phoneme_features = torch.randn(5, 12)   # 5ê°œ ìŒì†Œ
audio_features = torch.randn(100, 12)   # 100 í”„ë ˆì„

# DTW ì •ë ¬ ìˆ˜í–‰
path_i, path_j, total_cost = aligner(phoneme_features, audio_features)

print(f"Alignment path length: {len(path_i)}")
print(f"DTW cost: {total_cost:.4f}")
```

### 5. CTC ì •ë ¬

```python
from pytorch_hmm import CTCAligner

# CTC ì •ë ¬ê¸° (ìŒì„± ì¸ì‹ìš©)
ctc_aligner = CTCAligner(
    num_classes=28,  # 26 letters + blank + space
    blank_id=0
)

# ìŒì„± ì¸ì‹ ì‹œë®¬ë ˆì´ì…˜
sequence_length, batch_size, vocab_size = 80, 2, 28
log_probs = torch.log_softmax(
    torch.randn(sequence_length, batch_size, vocab_size), dim=-1)

targets = torch.tensor([[8, 5, 12, 12, 15],   # "HELLO"
                       [23, 15, 18, 12, 4]])  # "WORLD"

input_lengths = torch.full((batch_size,), sequence_length)
target_lengths = torch.tensor([5, 5])

# CTC loss ê³„ì‚°
loss = ctc_aligner(log_probs, targets, input_lengths, target_lengths)

# Greedy ë””ì½”ë”©
decoded = ctc_aligner.decode(log_probs, input_lengths)
print(f"Decoded sequences: {decoded}")
```

## ğŸ“Š ì„±ëŠ¥ í‰ê°€

### ìŒì„± í’ˆì§ˆ ë©”íŠ¸ë¦­

```python
from pytorch_hmm import (
    mel_cepstral_distortion, f0_root_mean_square_error,
    comprehensive_speech_evaluation
)

# ì‹œë®¬ë ˆì´ì…˜ëœ TTS ê²°ê³¼
seq_len, mfcc_dim = 200, 13
ground_truth_mfcc = torch.randn(seq_len, mfcc_dim)
predicted_mfcc = ground_truth_mfcc + 0.1 * torch.randn(seq_len, mfcc_dim)

# MCD ê³„ì‚°
mcd = mel_cepstral_distortion(ground_truth_mfcc, predicted_mfcc)
print(f"Mel-Cepstral Distortion: {mcd:.2f} dB")

# F0 í‰ê°€
gt_f0 = torch.abs(torch.randn(seq_len)) * 100 + 120  # 120-220 Hz
pred_f0 = gt_f0 + 5 * torch.randn(seq_len)

f0_rmse = f0_root_mean_square_error(gt_f0, pred_f0)
print(f"F0 RMSE: {f0_rmse:.2f} Hz")

# ì¢…í•© í‰ê°€
predicted_features = {
    'mfcc': predicted_mfcc.unsqueeze(0),
    'f0': pred_f0.unsqueeze(0)
}
ground_truth_features = {
    'mfcc': ground_truth_mfcc.unsqueeze(0),
    'f0': gt_f0.unsqueeze(0)
}

metrics = comprehensive_speech_evaluation(predicted_features, ground_truth_features)
print(f"Comprehensive metrics: {metrics}")
```

## ğŸ¯ ì‹¤ì œ ì‘ìš© ì˜ˆì œ

### ì™„ì „í•œ TTS íŒŒì´í”„ë¼ì¸

```python
import torch.nn as nn
from pytorch_hmm import HMMLayer, DurationModel, DTWAligner

class AdvancedTTSModel(nn.Module):
    def __init__(self, vocab_size, num_phonemes, acoustic_dim):
        super().__init__()

        # í…ìŠ¤íŠ¸ ì¸ì½”ë”
        self.text_encoder = nn.Embedding(vocab_size, 256)

        # ì§€ì†ì‹œê°„ ì˜ˆì¸¡ê¸° (HSMM ê¸°ë°˜)
        self.duration_predictor = DurationModel(
            num_states=num_phonemes,
            max_duration=30,
            distribution_type='neural'
        )

        # HMM ì •ë ¬ ë ˆì´ì–´
        self.alignment_layer = HMMLayer(
            num_states=num_phonemes,
            learnable_transitions=True,
            transition_type="left_to_right"
        )

        # ìŒí–¥ ë””ì½”ë”
        self.acoustic_decoder = nn.Sequential(
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, acoustic_dim)
        )

        # DTW í›„ì²˜ë¦¬
        self.dtw_aligner = DTWAligner(distance_fn='cosine')

    def forward(self, text_sequence, target_length=None):
        # í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        text_emb = self.text_encoder(text_sequence)

        # ì§€ì†ì‹œê°„ ì˜ˆì¸¡
        phoneme_ids = text_sequence  # ê°„ë‹¨í™”
        predicted_durations = self.duration_predictor.sample(phoneme_ids)

        # HMM ì •ë ¬
        aligned_features = self.alignment_layer(text_emb)

        # ìŒí–¥ íŠ¹ì§• ìƒì„±
        acoustic_features = self.acoustic_decoder(aligned_features)

        # DTWë¡œ ê¸¸ì´ ì¡°ì • (ì˜µì…˜)
        if target_length is not None:
            target_features = torch.randn(target_length, acoustic_features.shape[-1])
            path_i, path_j, _ = self.dtw_aligner(acoustic_features[0], target_features)
            # ê¸¸ì´ ì¡°ì • ë¡œì§...

        return acoustic_features, predicted_durations

# ì‚¬ìš© ì˜ˆì œ
model = AdvancedTTSModel(vocab_size=1000, num_phonemes=50, acoustic_dim=80)
text = torch.randint(0, 1000, (1, 20))  # 20ê°œ í† í°
acoustic_output, durations = model(text)

print(f"Generated acoustic features: {acoustic_output.shape}")
print(f"Predicted durations: {durations}")
```

## ğŸ”§ ê³ ê¸‰ ê¸°ëŠ¥ ë° ì„¤ì •

### ì‚¬ìš©ì ì •ì˜ ì „ì´ í–‰ë ¬

```python
from pytorch_hmm import create_transition_matrix

# ë‹¤ì–‘í•œ ì „ì´ íŒ¨í„´
transition_types = [
    "ergodic",              # ì™„ì „ ì—°ê²°
    "left_to_right",        # ìˆœì°¨ ì§„í–‰
    "left_to_right_skip",   # ê±´ë„ˆë›°ê¸° í—ˆìš©
    "circular"              # ìˆœí™˜ êµ¬ì¡°
]

for t_type in transition_types:
    P = create_transition_matrix(num_states=8, transition_type=t_type)
    print(f"{t_type}: {P.shape}")
```

### GPU ê°€ì† ë° ìµœì í™”

```python
# GPU ì‚¬ìš©
device = 'cuda' if torch.cuda.is_available() else 'cpu'
hmm = HMMPyTorch(transition_matrix).to(device)
observations = observations.to(device)

# JIT ì»´íŒŒì¼ë¡œ ì†ë„ í–¥ìƒ
@torch.jit.script
def fast_viterbi(hmm_model, obs):
    return hmm_model.viterbi_decode(obs)

# ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
def process_large_batch(hmm, data_loader):
    results = []
    for batch in data_loader:
        with torch.no_grad():
            batch = batch.to(device)
            posteriors, _, _ = hmm.forward_backward(batch)
            results.append(posteriors.cpu())
    return torch.cat(results, dim=0)

```

## ğŸ“ˆ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ğŸš€ **GPU ê°€ì† ì„±ëŠ¥ (RTX 3060 ê¸°ì¤€)**
- **ê¸°ë³¸ HMM Forward-Backward**: ~25,000 frames/sec
- **MixtureGaussianHMM**: ~18,000 frames/sec
- **Neural HMM**: ~12,000 frames/sec
- **HSMM**: ~15,000 frames/sec
- **ì‹¤ì‹œê°„ ë°°ìœ¨**: 188-312x (80fps ê¸°ì¤€)

### ğŸ’¾ **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**
- **GPU ë©”ëª¨ë¦¬**: <2GB (ì¼ë°˜ì ì¸ ì‘ì—…)
- **ë°°ì¹˜ ì²˜ë¦¬**: 32 sequences Ã— 1000 frames ì§€ì›
- **ìŠ¤íŠ¸ë¦¬ë°**: ì¼ì •í•œ ë©”ëª¨ë¦¬ ì‚¬ìš© (<100MB)
- **Log-space ê³„ì‚°**: ìˆ˜ì¹˜ ì•ˆì •ì„± ë³´ì¥

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ë¹ ë¥¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
pytest tests/ -m "not slow"

# ì „ì²´ í…ŒìŠ¤íŠ¸ (ì‹œê°„ ì†Œìš”)
pytest tests/ --cov=pytorch_hmm

# GPU í…ŒìŠ¤íŠ¸
pytest tests/ -m gpu

# ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
pytest tests/ -m performance --benchmark-only

# íŒ¨í‚¤ì§€ ë¬´ê²°ì„± í™•ì¸
python -c "import pytorch_hmm; pytorch_hmm.run_quick_test()"

### ë¹ ë¥¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰

```bash
# ê¸°ë³¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
python examples/benchmark.py --quick

# ì „ì²´ ë²¤ì¹˜ë§ˆí¬ (GPU í¬í•¨)
python examples/benchmark.py --save-results benchmark_results.json

# CPUë§Œ í…ŒìŠ¤íŠ¸
python examples/benchmark.py --no-gpu
```

### ì¼ë°˜ì ì¸ ì„±ëŠ¥ (RTX 3080 GPU ê¸°ì¤€)

| ì•Œê³ ë¦¬ì¦˜ | ì„±ëŠ¥ (fps) | ì‹¤ì‹œê°„ ì²˜ë¦¬ |
|---------|-----------|------------|
| **Basic HMM Forward-Backward** | ~15,000 | âœ… (188x faster) |
| **Basic HMM Viterbi** | ~25,000 | âœ… (312x faster) |
| **Neural HMM** | ~8,000 | âœ… (100x faster) |
| **DTW Alignment** | ~5,000 | âœ… (62x faster) |
| **CTC Decode** | ~12,000 | âœ… (150x faster) |

*ì‹¤ì‹œê°„ ì²˜ë¦¬ ê¸°ì¤€: 80fps (12.5ms/frame)*

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ëª¨ë“  ìƒˆ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
python tests/test_integration.py

# ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
python -m pytest tests/test_hmm.py -v

# ì„±ëŠ¥ íšŒê·€ í…ŒìŠ¤íŠ¸
python -m pytest tests/test_performance.py --benchmark-only
```

### ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬

```bash
# ì½”ë“œ í¬ë§·íŒ…
black pytorch_hmm/
isort pytorch_hmm/

# ë¦°íŒ…
flake8 pytorch_hmm/
mypy pytorch_hmm/
```

## ğŸ“– í•™ìŠµ ìë£Œ ë° ì˜ˆì œ

### ë‹¨ê³„ë³„ íŠœí† ë¦¬ì–¼

```bash
# 1. ê¸°ë³¸ ì‚¬ìš©ë²• í•™ìŠµ
python examples/basic_tutorial.py

# 2. ê³ ê¸‰ ê¸°ëŠ¥ ë°ëª¨
python examples/advanced_features_demo.py

# 3. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
python examples/benchmark.py
```

### ì»¤ë§¨ë“œë¼ì¸ ë„êµ¬

```bash
# ë¹ ë¥¸ ë°ëª¨ ì‹¤í–‰
pytorch-hmm-demo

# í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytorch-hmm-test

```

## ğŸ“š ë¬¸ì„œ ë° ì˜ˆì‹œ

### ğŸ“– **ì‚¬ìš© ê°€ëŠ¥í•œ ì˜ˆì œë“¤**
- ğŸ’¡ **ê¸°ë³¸ ì‚¬ìš©ë²•**: [`examples/basic_tutorial.py`](examples/basic_tutorial.py) - HMM ê¸°ì´ˆë¶€í„° GPU í™œìš©ê¹Œì§€
- ğŸš€ **ê³ ê¸‰ ê¸°ëŠ¥ ë°ëª¨**: [`examples/advanced_features_demo.py`](examples/advanced_features_demo.py) - v0.2.0 ì‹ ê¸°ëŠ¥ ì¢…í•© ì‹œì—°
- âš¡ **ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬**: [`examples/benchmark.py`](examples/benchmark.py) - ì‹¤ì‹œê°„ ì„±ëŠ¥ ì¸¡ì • ë° ë¹„êµ
- ğŸ¯ **v0.2.0 ìƒˆ ê¸°ëŠ¥**: [`examples/v0_2_0_demo.py`](examples/v0_2_0_demo.py) - Neural HMM, HSMM, DTW/CTC ì •ë ¬

### ğŸ”„ **ê°œë°œ ì˜ˆì • ì˜ˆì œë“¤**
- ğŸµ **ìŒì„± ì²˜ë¦¬ ì˜ˆì‹œ**: LibriSpeech/KSS ë°ì´í„°ì…‹ í™œìš© (v0.2.1)
- ğŸ‡°ğŸ‡· **í•œêµ­ì–´ TTS ë°ëª¨**: ì‹¤ì œ ìŒì„± ë°ì´í„° ì •ë ¬ (v0.2.1)
- ğŸ“Š **ì‹¤ì‹œê°„ ë§ˆì´í¬ ì²˜ë¦¬**: ë¼ì´ë¸Œ ìŒì„± ì…ë ¥ ì²˜ë¦¬ (v0.3.0)

## ğŸ› ï¸ ê°œë°œ ì°¸ì—¬

### ê°œë°œ í™˜ê²½ ì„¤ì •

```bash
# ê°œë°œ í™˜ê²½ ì„¤ì •
git clone https://github.com/crlotwhite/pytorch_hmm.git
cd pytorch_hmm
pip install -e .[dev]

# ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬
pre-commit install
black pytorch_hmm tests
isort pytorch_hmm tests
flake8 pytorch_hmm tests
mypy pytorch_hmm

# ê°œë°œ ì˜ì¡´ì„± ì„¤ì¹˜
pip install -e ".[dev]"

# pre-commit í›… ì„¤ì •
pre-commit install

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/ -v
```

## ğŸ“ˆ ë¡œë“œë§µ

### âœ… v0.2.0 (ì™„ë£Œë¨)
- âœ… **ê³ ê¸‰ HMM ëª¨ë¸**: MixtureGaussian, HSMM, Neural HMM
- âœ… **ì •ë ¬ ì•Œê³ ë¦¬ì¦˜**: DTW, CTC êµ¬í˜„ ì™„ë£Œ
- âœ… **ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬**: ì‹¤ì‹œê°„ ì ì‘í˜• ì²˜ë¦¬
- âœ… **ì¢…í•© í‰ê°€**: MCD, F0 RMSE, ì •ë ¬ ì •í™•ë„

### ğŸ”„ v0.2.1 (ì§„í–‰ ì¤‘ - 2ì£¼ ë‚´)
- ğŸµ **ì‹¤ì œ ë°ì´í„° ê²€ì¦**: LibriSpeech/KSS ë°ì´í„°ì…‹ ì§€ì›
- ğŸ“Š **ì‹¤ì‹œê°„ ë§ˆì´í¬ ì…ë ¥**: ë¼ì´ë¸Œ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë°ëª¨
- ğŸ‡°ğŸ‡· **í•œêµ­ì–´ TTS ì™„ì„±**: ì‹¤ì œ ìŒì„± íŒŒì¼ ì •ë ¬ ì˜ˆì œ
- ğŸ“ˆ **ì„±ëŠ¥ ìµœì í™”**: JIT ì»´íŒŒì¼ ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±

### ğŸ¯ v0.3.0 (1ê°œì›” ë‚´)
- ğŸ­ **ONNX ë‚´ë³´ë‚´ê¸°**: ì‹¤ì œ ëª¨ë¸ ë°°í¬ ì§€ì›
- ğŸ“š **Sphinx ë¬¸ì„œ**: ì™„ì „í•œ API ë¬¸ì„œí™”
- ğŸ­ **ê°ì • ëª¨ë¸ë§**: ìš´ìœ¨ ê¸°ë°˜ ê°ì • ì¸ì‹
- ğŸŒ **ë‹¤êµ­ì–´ í™•ì¥**: ì˜ì–´, ì¤‘êµ­ì–´ ìŒì†Œ ì§‘í•©

### ğŸš€ v1.0.0 (3ê°œì›” ë‚´)
- ğŸ—ï¸ **End-to-End TTS**: ì™„ì „í•œ í…ìŠ¤íŠ¸-ìŒì„± íŒŒì´í”„ë¼ì¸
- âš¡ **C++ ì¶”ë¡  ì—”ì§„**: ìµœëŒ€ ì„±ëŠ¥ ìµœì í™”
- ğŸ“¦ **ìƒíƒœê³„ í†µí•©**: Hugging Face, PyTorch Lightning
- ğŸ”’ **API ì•ˆì •í™”**: í•˜ìœ„ í˜¸í™˜ì„± ë³´ì¥

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

PyTorch HMM í”„ë¡œì íŠ¸ì— ê¸°ì—¬í•´ì£¼ì„¸ìš”!

- ğŸ› **ë²„ê·¸ ë¦¬í¬íŠ¸**: [Issues](https://github.com/crlotwhite/pytorch_hmm/issues)
- ğŸ’¡ **ê¸°ëŠ¥ ì œì•ˆ**: [Discussions](https://github.com/crlotwhite/pytorch_hmm/discussions)
- ğŸ”§ **Pull Request**: [Contributing Guide](CONTRIBUTING.md)

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ğŸ™ ê°ì‚¬ì˜ ë§

- **PyTorch íŒ€**: í›Œë¥­í•œ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
- **ìŒì„± ì²˜ë¦¬ ì»¤ë®¤ë‹ˆí‹°**: ê·€ì¤‘í•œ í”¼ë“œë°±ê³¼ ì œì•ˆ
- **ëª¨ë“  ê¸°ì—¬ìë“¤**: ì½”ë“œ, ë¬¸ì„œ, í…ŒìŠ¤íŠ¸ ê¸°ì—¬

### ê¸°ì—¬ ê°€ì´ë“œë¼ì¸

1. **ì´ìŠˆ ìƒì„±**: ìƒˆ ê¸°ëŠ¥ì´ë‚˜ ë²„ê·¸ ë¦¬í¬íŠ¸
2. **Fork & Branch**: `feature/your-feature-name`
3. **í…ŒìŠ¤íŠ¸ ì‘ì„±**: ìƒˆ ê¸°ëŠ¥ì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì¶”ê°€
4. **ë¬¸ì„œí™”**: docstringê³¼ README ì—…ë°ì´íŠ¸
5. **Pull Request**: ìƒì„¸í•œ ì„¤ëª…ê³¼ í•¨ê»˜ ì œì¶œ

## ğŸ“Š ë²„ì „ íˆìŠ¤í† ë¦¬ ë° ë¡œë“œë§µ

### âœ… v0.2.0 (í˜„ì¬) - Advanced Features
- âœ… Neural HMM with contextual modeling
- âœ… Hidden Semi-Markov Model (HSMM)
- âœ… DTW and CTC alignment algorithms
- âœ… Comprehensive evaluation metrics
- âœ… Performance optimization and GPU acceleration
- âœ… Production-ready features

### ğŸ”„ v0.3.0 (ê³„íš) - Real-world Integration
- [ ] LibriSpeech/KSS ë°ì´í„°ì…‹ ì§€ì›
- [ ] ONNX ë‚´ë³´ë‚´ê¸° ë° ì–‘ìí™”
- [ ] ì‹¤ì‹œê°„ ë§ˆì´í¬ ì…ë ¥ ì²˜ë¦¬
- [ ] Attention-based alignment
- [ ] ë©€í‹°í™”ì ëª¨ë¸ë§

### ğŸ¯ v1.0.0 (ëª©í‘œ) - Production Ready
- [ ] End-to-end TTS íŒŒì´í”„ë¼ì¸
- [ ] C++ ì¶”ë¡  ì—”ì§„
- [ ] ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- [ ] ì™„ì „í•œ ë¬¸ì„œí™” ë° API ì•ˆì •í™”

## ğŸ“š ì°¸ê³  ìë£Œ

### í•µì‹¬ ë…¼ë¬¸
- Rabiner, L. R. (1989). "A tutorial on hidden Markov models"
- Yu, K., et al. (2010). "Semi-Markov models for speech synthesis"
- Cuturi, M., & Blondel, M. (2017). "Soft-DTW: a Differentiable Loss Function for Time-Series"
- Graves, A., et al. (2006). "Connectionist temporal classification"

### ì‹¤ë¬´ í™œìš© ì‚¬ë¡€
- ìŒì„± í•©ì„± ì‹œìŠ¤í…œì—ì„œì˜ ìŒì†Œ ì •ë ¬
- ìŒì„± ì¸ì‹ì—ì„œì˜ CTC ë””ì½”ë”©
- í™”ì ì ì‘ì„ ìœ„í•œ DTW ì •ë ¬
- ì§€ì†ì‹œê°„ ëª¨ë¸ë§ì„ í†µí•œ ìì—°ìŠ¤ëŸ¬ìš´ TTS

## ğŸ™‹â€â™‚ï¸ ì§€ì› ë° ì»¤ë®¤ë‹ˆí‹°

- **GitHub Issues**: [ë²„ê·¸ ë¦¬í¬íŠ¸ ë° ê¸°ëŠ¥ ìš”ì²­](https://github.com/crlotwhite/pytorch_hmm/issues)
- **GitHub Discussions**: [ì¼ë°˜ ì§ˆë¬¸ ë° í† ë¡ ](https://github.com/crlotwhite/pytorch_hmm/discussions)
- **Documentation**: [ìƒì„¸ ë¬¸ì„œ](https://github.com/crlotwhite/pytorch_hmm/wiki)

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

---

## ğŸ‰ v0.2.0 ì—…ë°ì´íŠ¸ í•˜ì´ë¼ì´íŠ¸

ì´ë²ˆ **v0.2.0** ì—…ë°ì´íŠ¸ëŠ” PyTorch HMMì„ **ì™„ì „í•œ ìŒì„± ì²˜ë¦¬ ì†”ë£¨ì…˜**ìœ¼ë¡œ ë°œì „ì‹œì¼°ìŠµë‹ˆë‹¤:

âœ¨ **25ê°œ+ ìƒˆë¡œìš´ í´ë˜ìŠ¤**: Neural HMM, HSMM, DTW/CTC, ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
ğŸ§  **ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ëª¨ë¸ë§**: ì–¸ì–´ì  ì»¨í…ìŠ¤íŠ¸ì™€ ìš´ìœ¨ ì •ë³´ë¥¼ í™œìš©í•œ ë™ì  HMM
â±ï¸ **ëª…ì‹œì  ì§€ì†ì‹œê°„ ëª¨ë¸ë§**: ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± í•©ì„±ì„ ìœ„í•œ Semi-Markov ëª¨ë¸
ğŸ¯ **ìµœì‹  ì •ë ¬ ì•Œê³ ë¦¬ì¦˜**: DTWì™€ CTCë¡œ í˜„ëŒ€ì  ìŒì„± ì²˜ë¦¬ ì§€ì›
ğŸ“Š **ì¢…í•© í‰ê°€ ì‹œìŠ¤í…œ**: ìŒì„± í’ˆì§ˆì„ ìœ„í•œ í‘œì¤€ ë©”íŠ¸ë¦­ (MCD, F0 RMSE)
ğŸš€ **ì‹¤ì‹œê°„ GPU ê°€ì†**: RTX 3060ì—ì„œ 300x+ ì‹¤ì‹œê°„ ì²˜ë¦¬ ë‹¬ì„±

### ğŸ¯ **ë‹¤ìŒ ë‹¨ê³„ (v0.2.1)**
- ğŸ“Š **ì‹¤ì œ ë°ì´í„° ê²€ì¦**: LibriSpeech/KSS ë°ì´í„°ì…‹ìœ¼ë¡œ ì„±ëŠ¥ ê²€ì¦
- ğŸµ **ì‹¤ì‹œê°„ ë§ˆì´í¬ ì…ë ¥**: ë¼ì´ë¸Œ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë°ëª¨
- ğŸ“ˆ **ì„±ëŠ¥ ìµœì í™”**: JIT ì»´íŒŒì¼ ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ 

â­ **ì´ í”„ë¡œì íŠ¸ê°€ ë„ì›€ì´ ë˜ì—ˆë‹¤ë©´ Starë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”!** â­
