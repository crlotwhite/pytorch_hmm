# ğŸ¯ PyTorch HMM v0.2.0 - ìŒì„± í•©ì„±ì— íŠ¹í™”ëœ Hidden Markov Model
ğŸ¯ **Advanced PyTorch Hidden Markov Model Library for Speech Synthesis**

[![CI](https://github.com/crlotwhite/pytorch_hmm/workflows/CI/badge.svg)](https://github.com/crlotwhite/pytorch_hmm/actions)
[![codecov](https://codecov.io/gh/crlotwhite/pytorch_hmm/branch/main/graph/badge.svg)](https://codecov.io/gh/crlotwhite/pytorch_hmm)
[![PyPI version](https://badge.fury.io/py/pytorch-hmm.svg)](https://badge.fury.io/py/pytorch-hmm)
[![Python versions](https://img.shields.io/pypi/pyversions/pytorch-hmm.svg)](https://pypi.org/project/pytorch-hmm/)

[![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 1.8+](https://img.shields.io/badge/PyTorch-1.8+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Version](https://img.shields.io/badge/version-0.2.0-green.svg)](https://github.com/crlotwhite/pytorch_hmm)

PyTorch ê¸°ë°˜ Hidden Markov Model êµ¬í˜„ì²´ë¡œ, **ìŒì„± í•©ì„±(TTS)ê³¼ ìŒì„± ì²˜ë¦¬**ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. Forward-backwardì™€ Viterbi ì•Œê³ ë¦¬ì¦˜ì„ ì§€ì›í•˜ë©°, autogradì™€ GPU ê°€ì†ì„ ì™„ë²½í•˜ê²Œ ì§€ì›í•©ë‹ˆë‹¤.

## ğŸš€ v0.2.0 ì£¼ìš” ê¸°ëŠ¥

### âœ¨ ìƒˆë¡œìš´ ëª¨ë¸ë“¤
- ğŸ¨ **MixtureGaussianHMM**: ë³µì¡í•œ ìŒí–¥ ëª¨ë¸ë§ì„ ìœ„í•œ GMM-HMM
- â° **Semi-Markov Model (HSMM)**: ëª…ì‹œì  ì§€ì†ì‹œê°„ ëª¨ë¸ë§
- ğŸ“¡ **StreamingHMM**: ì‹¤ì‹œê°„ ë‚®ì€ ì§€ì—°ì‹œê°„ ì²˜ë¦¬
- ğŸ”„ **AdaptiveTransitions**: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë™ì  ì „ì´

### ğŸ¯ ìŒì„± ì²˜ë¦¬ íŠ¹í™” ê¸°ëŠ¥
- ğŸ‡°ğŸ‡· **í•œêµ­ì–´ TTS ì§€ì›**: ìŒì†Œ ì •ë ¬ê³¼ ì§€ì†ì‹œê°„ ì œì–´
- ğŸµ **ìš´ìœ¨ ì¸ì‹ ì „ì´**: F0ì™€ ì—ë„ˆì§€ ê¸°ë°˜ ì „ì´ í–‰ë ¬
- âš¡ **Skip-state ì „ì´**: ë¹ ë¥¸ ë°œí™” ì²˜ë¦¬
- ğŸ“Š **ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹**: ì¢…í•©ì ì¸ ì„±ëŠ¥ ë¶„ì„ ë„êµ¬

### ğŸ’» í”„ë¡œë•ì…˜ ì¤€ë¹„
- ğŸ­ **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ëŒ€ìš©ëŸ‰ ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
- ğŸ”§ **TorchScript ì§€ì›**: ë°°í¬ ìµœì í™”
- ğŸ“ˆ **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: ì„±ëŠ¥ í†µê³„ ë° ì ì‘í˜• ì œì–´
- ğŸ§ª **ì¢…í•© í…ŒìŠ¤íŠ¸**: 95%+ ì½”ë“œ ì»¤ë²„ë¦¬ì§€

PyTorch ê¸°ë°˜ì˜ ì¢…í•©ì ì¸ Hidden Markov Model ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. **ìŒì„± í•©ì„±(TTS), ìŒì„± ì¸ì‹(ASR), ì‹œí€€ìŠ¤ ëª¨ë¸ë§**ì— ìµœì í™”ë˜ì–´ ìˆìœ¼ë©°, ìµœì‹  ë”¥ëŸ¬ë‹ ê¸°ë²•ê³¼ ì „í†µì ì¸ HMMì„ ê²°í•©í•œ ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤.

## âœ¨ ìƒˆë¡œìš´ v0.2.0 ì£¼ìš” ê¸°ëŠ¥

### ğŸ§  **Neural HMM with Contextual Modeling**
- **Context-aware HMM**: ì–¸ì–´ì  ì»¨í…ìŠ¤íŠ¸ì™€ ìš´ìœ¨ ì •ë³´ë¥¼ í™œìš©
- **RNN/Transformer ê¸°ë°˜ ì „ì´ ëª¨ë¸**: ë™ì  ì „ì´ í™•ë¥  ê³„ì‚°
- **Mixture Gaussian ê´€ì¸¡ ëª¨ë¸**: ë³µì¡í•œ ìŒí–¥ íŠ¹ì§• ëª¨ë¸ë§

### â±ï¸ **Hidden Semi-Markov Model (HSMM)**
- **ëª…ì‹œì  ì§€ì†ì‹œê°„ ëª¨ë¸ë§**: Gamma, Poisson, Neural ë¶„í¬ ì§€ì›
- **ìì—°ìŠ¤ëŸ¬ìš´ ìŒì†Œ ì§€ì†ì‹œê°„**: ì‹¤ì œ ìŒì„± íŠ¹ì„± ë°˜ì˜
- **ì ì‘í˜• ì§€ì†ì‹œê°„**: ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥¸ ì§€ì†ì‹œê°„ ì¡°ì ˆ

### ğŸ¯ **Advanced Alignment Algorithms**
- **Dynamic Time Warping (DTW)**: ìœ ì—°í•œ ì‹œí€€ìŠ¤ ì •ë ¬, Soft-DTW ì§€ì›
- **CTC Alignment**: End-to-end í•™ìŠµ ê°€ëŠ¥í•œ ì •ë ¬
- **Constrained alignment**: Bandwidth ì œì•½, Monotonic ì •ë ¬

### ğŸ“Š **Comprehensive Evaluation Metrics**
- **Mel-Cepstral Distortion (MCD)**: ìŒì„± í’ˆì§ˆì˜ ê°ê´€ì  í‰ê°€
- **F0 RMSE**: ìš´ìœ¨ ëª¨ë¸ë§ ì„±ëŠ¥ í‰ê°€
- **Alignment Accuracy**: ì •ë ¬ ì •í™•ë„ ë° ê²½ê³„ íƒì§€ ì„±ëŠ¥
- **Duration Modeling**: ì§€ì†ì‹œê°„ ì˜ˆì¸¡ ì •í™•ë„

### ğŸš€ **Production-Ready Features**
- **GPU ê°€ì†**: CUDA ì§€ì›ìœ¼ë¡œ ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥ (>80fps)
- **ë°°ì¹˜ ì²˜ë¦¬**: íš¨ìœ¨ì ì¸ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
- **JIT í˜¸í™˜**: `torch.jit.script`ë¡œ ìµœì í™”
- **ë©”ëª¨ë¦¬ íš¨ìœ¨**: Log-space ê³„ì‚°ìœ¼ë¡œ ìˆ˜ì¹˜ ì•ˆì •ì„±

## ğŸ“¦ ì„¤ì¹˜

### uvë¥¼ ì‚¬ìš©í•œ ì„¤ì¹˜ (ê¶Œì¥)

```bash
# ê¸°ë³¸ ì„¤ì¹˜ (CPU ë²„ì „)
uv add pytorch-hmm[cpu]

# GPU ì§€ì› (CUDA 12.4)
uv add pytorch-hmm[cuda]

# ê°œë°œ í™˜ê²½ ì„¤ì •
git clone https://github.com/crlotwhite/pytorch_hmm.git
cd pytorch_hmm
uv sync --extra dev

# íŠ¹ì • ê¸°ëŠ¥ ê·¸ë£¹ ì„¤ì¹˜
uv sync --extra audio      # ìŒì„± ì²˜ë¦¬
uv sync --extra visualization  # ì‹œê°í™”
uv sync --extra benchmarks # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
```

### PyTorch ë²„ì „ ì„ íƒ

ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” í”Œë«í¼ë³„ë¡œ ìµœì í™”ëœ PyTorch ë²„ì „ì„ ìë™ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤:

- **macOS**: CPU ì „ìš© ë²„ì „
- **Linux/Windows**: CUDA 12.4 ì§€ì› ë²„ì „ (GPU ê°€ìš©ì‹œ)

ìˆ˜ë™ìœ¼ë¡œ PyTorch ë²„ì „ì„ ì„ íƒí•˜ë ¤ë©´:

```bash
# CPU ì „ìš© ê°•ì œ ì„¤ì¹˜
uv sync --extra cpu

# CUDA ë²„ì „ ê°•ì œ ì„¤ì¹˜
uv sync --extra cuda
```

### pipë¥¼ ì‚¬ìš©í•œ ì„¤ì¹˜

```bash
# ê¸°ë³¸ ì„¤ì¹˜
pip install pytorch-hmm

# ìŒì„± ì²˜ë¦¬ ê¸°ëŠ¥ í¬í•¨
pip install pytorch-hmm[audio]

# ì „ì²´ ê¸°ëŠ¥ (ê°œë°œìš©)
pip install pytorch-hmm[all]

# ê°œë°œ ë²„ì „
git clone https://github.com/crlotwhite/pytorch_hmm.git
cd pytorch_hmm
pip install -e .[dev]
```

### ì„ íƒì  ì˜ì¡´ì„±

```bash
# ì˜¤ë””ì˜¤ ì²˜ë¦¬
pip install pytorch-hmm[audio]

# ì‹œê°í™”
pip install pytorch-hmm[visualization]

# ê°œë°œ ë„êµ¬
pip install pytorch-hmm[dev]

# ë²¤ì¹˜ë§ˆí¬ ë„êµ¬
pip install pytorch-hmm[benchmarks]

```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ê¸°ë³¸ HMM ì‚¬ìš©ë²•

```python
import torch
from pytorch_hmm import create_speech_hmm, get_speech_transitions

# ìŒì„±ìš© HMM ìƒì„±
model = create_speech_hmm(
    num_states=10,      # ìŒì†Œ ê°œìˆ˜
    feature_dim=80,     # ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì°¨ì›
    model_type="mixture_gaussian"
)

# ìŒì„± íŠ¹ì§• ë°ì´í„°
features = torch.randn(4, 100, 80)  # (batch, time, features)

# ë””ì½”ë”©
decoded_states, log_probs = model(features, return_log_probs=True)
print(f"ë””ì½”ë”©ëœ ìƒíƒœ: {decoded_states.shape}")  # (4, 100)
```

### ğŸ¨ Mixture Gaussian HMM

```python
from pytorch_hmm import MixtureGaussianHMMLayer

# ë³µì¡í•œ ìŒí–¥ ëª¨ë¸ë§
model = MixtureGaussianHMMLayer(
    num_states=20,          # 20ê°œ ìŒì†Œ
    feature_dim=80,         # 80ì°¨ì› ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨
    num_components=4,       # 4ê°œ ê°€ìš°ì‹œì•ˆ ë¯¹ìŠ¤ì²˜
    covariance_type='diag'  # ëŒ€ê° ê³µë¶„ì‚°
)

# ìŒí–¥ íŠ¹ì§• ì²˜ë¦¬
audio_features = torch.randn(2, 150, 80)
states, confidence = model(audio_features, return_log_probs=True)

print(f"ëª¨ë¸ ì •ë³´: {model.get_model_info()}")
```

### â° Semi-Markov Model (HSMM)

```python
from pytorch_hmm import HSMMLayer

# ì§€ì†ì‹œê°„ ëª¨ë¸ë§
hsmm = HSMMLayer(
    num_states=15,
    feature_dim=80,
    duration_distribution='gamma',  # Gamma ë¶„í¬
    max_duration=50                 # ìµœëŒ€ 50í”„ë ˆì„
)

# ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± ì‹œí€€ìŠ¤ ìƒì„±
states, observations = hsmm.generate_sequence(length=200)

# ì§€ì†ì‹œê°„ ë¶„ì„
expected_durations = hsmm.get_expected_durations()
print(f"ê° ìƒíƒœì˜ ì˜ˆìƒ ì§€ì†ì‹œê°„: {expected_durations}")
```

### ğŸ“¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°

```python
from pytorch_hmm import StreamingHMMProcessor, AdaptiveLatencyController

# ì‹¤ì‹œê°„ í”„ë¡œì„¸ì„œ
processor = StreamingHMMProcessor(
    num_states=10,
    feature_dim=80,
    chunk_size=160,         # 10ms ì²­í¬
    use_beam_search=True,
    beam_width=4
)

# ì ì‘í˜• ì§€ì—°ì‹œê°„ ì œì–´
controller = AdaptiveLatencyController(target_latency_ms=30.0)

# ì‹¤ì‹œê°„ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
for i in range(100):
    # 10ms ì˜¤ë””ì˜¤ ì²­í¬
    audio_chunk = torch.randn(160, 80)

    # ì²˜ë¦¬
    result = processor.process_chunk(audio_chunk)

    if result.status == 'decoded':
        print(f"ìƒíƒœ: {result.decoded_states}")
        print(f"ì²˜ë¦¬ ì‹œê°„: {result.processing_time_ms:.1f}ms")

        # ì„±ëŠ¥ ì ì‘
        recommendations = controller.update(
            result.processing_time_ms,
            result.buffer_size
        )
```

### ğŸ‡°ğŸ‡· í•œêµ­ì–´ TTS ì˜ˆì‹œ

```python
from pytorch_hmm import create_korean_tts_hmm

# í•œêµ­ì–´ ìŒì†Œ ì§‘í•©ìœ¼ë¡œ HMM ìƒì„±
korean_model = create_korean_tts_hmm(
    feature_dim=80,
    model_type="hsmm"  # ì§€ì†ì‹œê°„ ëª¨ë¸ë§ í¬í•¨
)

# ìŒì†Œ ì‹œí€€ìŠ¤: "ì•ˆë…•í•˜ì„¸ìš”"
phoneme_sequence = ['sil', 'a', 'n', 'n', 'eo', 'ng', 'h', 'a', 's', 'e', 'j', 'o', 'sil']

print(f"í•œêµ­ì–´ HMM ì •ë³´:")
print(f"  ìƒíƒœ ìˆ˜: {korean_model.num_states}")
print(f"  íŠ¹ì§• ì°¨ì›: {korean_model.feature_dim}")
```

## ğŸ”§ ê³ ê¸‰ ì‚¬ìš©ë²•

### ì „ì´ í–‰ë ¬ ì»¤ìŠ¤í„°ë§ˆì´ì§•

```python
from pytorch_hmm.utils import (
    create_skip_state_matrix,
    create_phoneme_aware_transitions,
    create_prosody_aware_transitions
)

# ë¹ ë¥¸ ë°œí™”ìš© Skip-state ì „ì´
skip_transitions = create_skip_state_matrix(
    num_states=20,
    self_loop_prob=0.5,    # ë‚®ì€ ìœ ì§€ í™•ë¥ 
    forward_prob=0.4,      # ë†’ì€ ì „ì§„ í™•ë¥ 
    skip_prob=0.1,         # ê±´ë„ˆë›°ê¸° í—ˆìš©
    max_skip=2
)

# ìŒì†Œ ì§€ì†ì‹œê°„ ê¸°ë°˜ ì „ì´
phoneme_durations = [5, 8, 6, 12, 4, 9, 7, 11]  # ê° ìŒì†Œë³„ í‰ê·  ì§€ì†ì‹œê°„
duration_transitions = create_phoneme_aware_transitions(
    phoneme_durations,
    duration_variance=0.3
)

# ìš´ìœ¨ ì •ë³´ ê¸°ë°˜ ë™ì  ì „ì´
f0_contour = torch.randn(100)      # F0 ìœ¤ê³½
energy_contour = torch.randn(100)  # ì—ë„ˆì§€ ìœ¤ê³½

prosody_transitions = create_prosody_aware_transitions(
    f0_contour, energy_contour, num_states=10
)
print(f"ìš´ìœ¨ ê¸°ë°˜ ì „ì´ í–‰ë ¬: {prosody_transitions.shape}")  # (100, 10, 10)
```

### ì„±ëŠ¥ ìµœì í™”

```python
import pytorch_hmm

# í•˜ë“œì›¨ì–´ ìë™ ì„¤ì •
config_info = pytorch_hmm.auto_configure()
print(f"ìë™ ì„¤ì •: {config_info}")

# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
from pytorch_hmm.utils import benchmark_transition_operations

benchmark_results = benchmark_transition_operations(
    num_states_list=[5, 10, 20, 50],
    num_trials=100
)

for operation, results in benchmark_results.items():
    print(f"{operation}:")
    for num_states, time_ms in results.items():
        print(f"  {num_states} states: {time_ms:.2f}ms")

### 2. Neural HMMìœ¼ë¡œ ê³ ê¸‰ ëª¨ë¸ë§

```python
from pytorch_hmm import NeuralHMM, ContextualNeuralHMM

# ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ Neural HMM
contextual_hmm = ContextualNeuralHMM(
    num_states=10,
    observation_dim=80,          # ë©œ ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì°¨ì›
    phoneme_vocab_size=50,       # ìŒì†Œ ê°œìˆ˜
    linguistic_context_dim=32,   # ì–¸ì–´ì  ì»¨í…ìŠ¤íŠ¸
    prosody_dim=8               # ìš´ìœ¨ íŠ¹ì§•
)

# ì…ë ¥ ë°ì´í„° ì¤€ë¹„
batch_size, seq_len = 2, 100
observations = torch.randn(batch_size, seq_len, 80)           # ìŒí–¥ íŠ¹ì§•
phoneme_sequence = torch.randint(0, 50, (batch_size, seq_len)) # ìŒì†Œ ì‹œí€€ìŠ¤
prosody_features = torch.randn(batch_size, seq_len, 8)        # ìš´ìœ¨ íŠ¹ì§•

# ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì •ë ¬
posteriors, forward, backward = contextual_hmm.forward_with_context(
    observations, phoneme_sequence, prosody_features)

print(f"Context-aware posteriors: {posteriors.shape}")
```

### 3. Semi-Markov HMMìœ¼ë¡œ ì§€ì†ì‹œê°„ ëª¨ë¸ë§

```python
from pytorch_hmm import SemiMarkovHMM

# ì§€ì†ì‹œê°„ ëª¨ë¸ë§ì´ ê°€ëŠ¥í•œ HSMM
hsmm = SemiMarkovHMM(
    num_states=8,
    observation_dim=40,
    max_duration=20,
    duration_distribution='gamma',  # ê°ë§ˆ ë¶„í¬ë¡œ ì§€ì†ì‹œê°„ ëª¨ë¸ë§
    observation_model='gaussian'
)

# ì‹œí€€ìŠ¤ ìƒ˜í”Œë§
state_seq, duration_seq, observations = hsmm.sample(
    num_states=6, max_length=100)

print(f"Sampled states: {state_seq}")
print(f"State durations: {duration_seq}")
print(f"Total frames: {duration_seq.sum()} frames")

# Viterbi ë””ì½”ë”©ìœ¼ë¡œ ìµœì  ìƒíƒœ-ì§€ì†ì‹œê°„ ì‹œí€€ìŠ¤ ì°¾ê¸°
optimal_states, optimal_durations, log_prob = hsmm.viterbi_decode(observations)
```

### 4. DTW ì •ë ¬

```python
from pytorch_hmm import DTWAligner

# DTW ì •ë ¬ê¸° ìƒì„±
aligner = DTWAligner(
    distance_fn='cosine',           # ì½”ì‚¬ì¸ ê±°ë¦¬
    step_pattern='symmetric',       # ëŒ€ì¹­ ìŠ¤í… íŒ¨í„´
    soft_dtw=True                  # ë¯¸ë¶„ ê°€ëŠ¥í•œ Soft DTW
)

# ìŒì†Œ íŠ¹ì§•ê³¼ ìŒì„± íŠ¹ì§• ì •ë ¬
phoneme_features = torch.randn(5, 12)   # 5ê°œ ìŒì†Œ
audio_features = torch.randn(100, 12)   # 100 í”„ë ˆì„

# DTW ì •ë ¬ ìˆ˜í–‰
path_i, path_j, total_cost = aligner(phoneme_features, audio_features)

print(f"Alignment path length: {len(path_i)}")
print(f"DTW cost: {total_cost:.4f}")
```

### 5. CTC ì •ë ¬

```python
from pytorch_hmm import CTCAligner

# CTC ì •ë ¬ê¸° (ìŒì„± ì¸ì‹ìš©)
ctc_aligner = CTCAligner(
    num_classes=28,  # 26 letters + blank + space
    blank_id=0
)

# ìŒì„± ì¸ì‹ ì‹œë®¬ë ˆì´ì…˜
sequence_length, batch_size, vocab_size = 80, 2, 28
log_probs = torch.log_softmax(
    torch.randn(sequence_length, batch_size, vocab_size), dim=-1)

targets = torch.tensor([[8, 5, 12, 12, 15],   # "HELLO"
                       [23, 15, 18, 12, 4]])  # "WORLD"

input_lengths = torch.full((batch_size,), sequence_length)
target_lengths = torch.tensor([5, 5])

# CTC loss ê³„ì‚°
loss = ctc_aligner(log_probs, targets, input_lengths, target_lengths)

# Greedy ë””ì½”ë”©
decoded = ctc_aligner.decode(log_probs, input_lengths)
print(f"Decoded sequences: {decoded}")
```

## ğŸ“Š ì„±ëŠ¥ í‰ê°€

### ìŒì„± í’ˆì§ˆ ë©”íŠ¸ë¦­

```python
from pytorch_hmm import (
    mel_cepstral_distortion, f0_root_mean_square_error,
    comprehensive_speech_evaluation
)

# ì‹œë®¬ë ˆì´ì…˜ëœ TTS ê²°ê³¼
seq_len, mfcc_dim = 200, 13
ground_truth_mfcc = torch.randn(seq_len, mfcc_dim)
predicted_mfcc = ground_truth_mfcc + 0.1 * torch.randn(seq_len, mfcc_dim)

# MCD ê³„ì‚°
mcd = mel_cepstral_distortion(ground_truth_mfcc, predicted_mfcc)
print(f"Mel-Cepstral Distortion: {mcd:.2f} dB")

# F0 í‰ê°€
gt_f0 = torch.abs(torch.randn(seq_len)) * 100 + 120  # 120-220 Hz
pred_f0 = gt_f0 + 5 * torch.randn(seq_len)

f0_rmse = f0_root_mean_square_error(gt_f0, pred_f0)
print(f"F0 RMSE: {f0_rmse:.2f} Hz")

# ì¢…í•© í‰ê°€
predicted_features = {
    'mfcc': predicted_mfcc.unsqueeze(0),
    'f0': pred_f0.unsqueeze(0)
}
ground_truth_features = {
    'mfcc': ground_truth_mfcc.unsqueeze(0),
    'f0': gt_f0.unsqueeze(0)
}

metrics = comprehensive_speech_evaluation(predicted_features, ground_truth_features)
print(f"Comprehensive metrics: {metrics}")
```

## ğŸ¯ ì‹¤ì œ ì‘ìš© ì˜ˆì œ

### ì™„ì „í•œ TTS íŒŒì´í”„ë¼ì¸

```python
import torch.nn as nn
from pytorch_hmm import HMMLayer, DurationModel, DTWAligner

class AdvancedTTSModel(nn.Module):
    def __init__(self, vocab_size, num_phonemes, acoustic_dim):
        super().__init__()

        # í…ìŠ¤íŠ¸ ì¸ì½”ë”
        self.text_encoder = nn.Embedding(vocab_size, 256)

        # ì§€ì†ì‹œê°„ ì˜ˆì¸¡ê¸° (HSMM ê¸°ë°˜)
        self.duration_predictor = DurationModel(
            num_states=num_phonemes,
            max_duration=30,
            distribution_type='neural'
        )

        # HMM ì •ë ¬ ë ˆì´ì–´
        self.alignment_layer = HMMLayer(
            num_states=num_phonemes,
            learnable_transitions=True,
            transition_type="left_to_right"
        )

        # ìŒí–¥ ë””ì½”ë”
        self.acoustic_decoder = nn.Sequential(
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, acoustic_dim)
        )

        # DTW í›„ì²˜ë¦¬
        self.dtw_aligner = DTWAligner(distance_fn='cosine')

    def forward(self, text_sequence, target_length=None):
        # í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        text_emb = self.text_encoder(text_sequence)

        # ì§€ì†ì‹œê°„ ì˜ˆì¸¡
        phoneme_ids = text_sequence  # ê°„ë‹¨í™”
        predicted_durations = self.duration_predictor.sample(phoneme_ids)

        # HMM ì •ë ¬
        aligned_features = self.alignment_layer(text_emb)

        # ìŒí–¥ íŠ¹ì§• ìƒì„±
        acoustic_features = self.acoustic_decoder(aligned_features)

        # DTWë¡œ ê¸¸ì´ ì¡°ì • (ì˜µì…˜)
        if target_length is not None:
            target_features = torch.randn(target_length, acoustic_features.shape[-1])
            path_i, path_j, _ = self.dtw_aligner(acoustic_features[0], target_features)
            # ê¸¸ì´ ì¡°ì • ë¡œì§...

        return acoustic_features, predicted_durations

# ì‚¬ìš© ì˜ˆì œ
model = AdvancedTTSModel(vocab_size=1000, num_phonemes=50, acoustic_dim=80)
text = torch.randint(0, 1000, (1, 20))  # 20ê°œ í† í°
acoustic_output, durations = model(text)

print(f"Generated acoustic features: {acoustic_output.shape}")
print(f"Predicted durations: {durations}")
```

## ğŸ”§ ê³ ê¸‰ ê¸°ëŠ¥ ë° ì„¤ì •

### ì‚¬ìš©ì ì •ì˜ ì „ì´ í–‰ë ¬

```python
from pytorch_hmm import create_transition_matrix

# ë‹¤ì–‘í•œ ì „ì´ íŒ¨í„´
transition_types = [
    "ergodic",              # ì™„ì „ ì—°ê²°
    "left_to_right",        # ìˆœì°¨ ì§„í–‰
    "left_to_right_skip",   # ê±´ë„ˆë›°ê¸° í—ˆìš©
    "circular"              # ìˆœí™˜ êµ¬ì¡°
]

for t_type in transition_types:
    P = create_transition_matrix(num_states=8, transition_type=t_type)
    print(f"{t_type}: {P.shape}")
```

### GPU ê°€ì† ë° ìµœì í™”

```python
# GPU ì‚¬ìš©
device = 'cuda' if torch.cuda.is_available() else 'cpu'
hmm = HMMPyTorch(transition_matrix).to(device)
observations = observations.to(device)

# JIT ì»´íŒŒì¼ë¡œ ì†ë„ í–¥ìƒ
@torch.jit.script
def fast_viterbi(hmm_model, obs):
    return hmm_model.viterbi_decode(obs)

# ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
def process_large_batch(hmm, data_loader):
    results = []
    for batch in data_loader:
        with torch.no_grad():
            batch = batch.to(device)
            posteriors, _, _ = hmm.forward_backward(batch)
            results.append(posteriors.cpu())
    return torch.cat(results, dim=0)

```

## ğŸ“ˆ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### ì²˜ë¦¬ ì†ë„ (RTX 3080 ê¸°ì¤€)
- **MixtureGaussianHMM**: ~18,000 frames/sec
- **HSMM**: ~12,000 frames/sec
- **StreamingHMM**: ~25,000 frames/sec
- **ì‹¤ì‹œê°„ ë°°ìœ¨**: 150-400x (ì‹¤ì‹œê°„ 80fps ê¸°ì¤€)

### ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
- **ëŒ€ìš©ëŸ‰ ë°°ì¹˜**: 32 sequences Ã— 2000 frames ì²˜ë¦¬ ê°€ëŠ¥
- **GPU ë©”ëª¨ë¦¬**: 2GB ì´í•˜ (ëŒ€ë¶€ë¶„ì˜ ì‘ì—…)
- **ìŠ¤íŠ¸ë¦¬ë°**: ì¼ì •í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (<100MB)

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ë¹ ë¥¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
pytest tests/ -m "not slow"

# ì „ì²´ í…ŒìŠ¤íŠ¸ (ì‹œê°„ ì†Œìš”)
pytest tests/ --cov=pytorch_hmm

# GPU í…ŒìŠ¤íŠ¸
pytest tests/ -m gpu

# ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
pytest tests/ -m performance --benchmark-only

# íŒ¨í‚¤ì§€ ë¬´ê²°ì„± í™•ì¸
python -c "import pytorch_hmm; pytorch_hmm.run_quick_test()"

### ë¹ ë¥¸ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰

```bash
# ê¸°ë³¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
python examples/benchmark.py --quick

# ì „ì²´ ë²¤ì¹˜ë§ˆí¬ (GPU í¬í•¨)
python examples/benchmark.py --save-results benchmark_results.json

# CPUë§Œ í…ŒìŠ¤íŠ¸
python examples/benchmark.py --no-gpu
```

### ì¼ë°˜ì ì¸ ì„±ëŠ¥ (RTX 3080 GPU ê¸°ì¤€)

| ì•Œê³ ë¦¬ì¦˜ | ì„±ëŠ¥ (fps) | ì‹¤ì‹œê°„ ì²˜ë¦¬ |
|---------|-----------|------------|
| **Basic HMM Forward-Backward** | ~15,000 | âœ… (188x faster) |
| **Basic HMM Viterbi** | ~25,000 | âœ… (312x faster) |
| **Neural HMM** | ~8,000 | âœ… (100x faster) |
| **DTW Alignment** | ~5,000 | âœ… (62x faster) |
| **CTC Decode** | ~12,000 | âœ… (150x faster) |

*ì‹¤ì‹œê°„ ì²˜ë¦¬ ê¸°ì¤€: 80fps (12.5ms/frame)*

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ëª¨ë“  ìƒˆ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
python tests/test_integration.py

# ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
python -m pytest tests/test_hmm.py -v

# ì„±ëŠ¥ íšŒê·€ í…ŒìŠ¤íŠ¸
python -m pytest tests/test_performance.py --benchmark-only
```

### ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬

```bash
# ì½”ë“œ í¬ë§·íŒ…
black pytorch_hmm/
isort pytorch_hmm/

# ë¦°íŒ…
flake8 pytorch_hmm/
mypy pytorch_hmm/
```

## ğŸ“– í•™ìŠµ ìë£Œ ë° ì˜ˆì œ

### ë‹¨ê³„ë³„ íŠœí† ë¦¬ì–¼

```bash
# 1. ê¸°ë³¸ ì‚¬ìš©ë²• í•™ìŠµ
python examples/basic_tutorial.py

# 2. ê³ ê¸‰ ê¸°ëŠ¥ ë°ëª¨
python examples/advanced_features_demo.py

# 3. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
python examples/benchmark.py
```

### ì»¤ë§¨ë“œë¼ì¸ ë„êµ¬

```bash
# ë¹ ë¥¸ ë°ëª¨ ì‹¤í–‰
pytorch-hmm-demo

# í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytorch-hmm-test

```

## ğŸ“š ë¬¸ì„œ ë° ì˜ˆì‹œ

- ğŸ“– **API ë¬¸ì„œ**: [pytorch-hmm.readthedocs.io](https://pytorch-hmm.readthedocs.io)
- ğŸ’¡ **íŠœí† ë¦¬ì–¼**: [examples/](examples/) ë””ë ‰í† ë¦¬
- ğŸµ **ìŒì„± ì²˜ë¦¬ ì˜ˆì‹œ**: [examples/speech_synthesis_examples.py](examples/speech_synthesis_examples.py)
- ğŸ‡°ğŸ‡· **í•œêµ­ì–´ TTS ë°ëª¨**: [examples/korean_tts_demo.py](examples/korean_tts_demo.py)
- âš¡ **ì‹¤ì‹œê°„ ì²˜ë¦¬**: [examples/real_time_processing.py](examples/real_time_processing.py)

## ğŸ› ï¸ ê°œë°œ ì°¸ì—¬

### ê°œë°œ í™˜ê²½ ì„¤ì •

```bash
# ê°œë°œ í™˜ê²½ ì„¤ì •
git clone https://github.com/crlotwhite/pytorch_hmm.git
cd pytorch_hmm
pip install -e .[dev]

# ì½”ë“œ í’ˆì§ˆ ê²€ì‚¬
pre-commit install
black pytorch_hmm tests
isort pytorch_hmm tests
flake8 pytorch_hmm tests
mypy pytorch_hmm

# ê°œë°œ ì˜ì¡´ì„± ì„¤ì¹˜
pip install -e ".[dev]"

# pre-commit í›… ì„¤ì •
pre-commit install

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/ -v
```

## ğŸ“ˆ ë¡œë“œë§µ

### v0.3.0 (ì˜ˆì •)
- ğŸ­ **ê°ì • ìŒì„± í•©ì„±**: ê°ì • ìƒíƒœ ê¸°ë°˜ HMM
- ğŸŒ **ë‹¤êµ­ì–´ ì§€ì›**: ì˜ì–´, ì¤‘êµ­ì–´, ì¼ë³¸ì–´ ìŒì†Œ ì§‘í•©
- ğŸ”Š **í™”ì ì ì‘**: ë‹¤ì¤‘ í™”ì ëª¨ë¸ë§
- ğŸ¯ **ê³ ê¸‰ ì •ë ¬**: DTWì™€ HMM ê²°í•© ì •ë ¬

### v1.0.0 (ëª©í‘œ)
- ğŸ­ **í”„ë¡œë•ì…˜ ì•ˆì •ì„±**: API ê³ ì • ë° í•˜ìœ„ í˜¸í™˜ì„±
- ğŸ“¦ **íŒ¨í‚¤ì§€ ìƒíƒœê³„**: Hugging Face, PyTorch Lightning í†µí•©
- ğŸš€ **ë°°í¬ ìµœì í™”**: ONNX, TensorRT ì§€ì›
- ğŸ“š **ì™„ì „í•œ ë¬¸ì„œí™”**: ì¢…í•© ê°€ì´ë“œ ë° íŠœí† ë¦¬ì–¼

## ğŸ¤ ê¸°ì—¬í•˜ê¸°

PyTorch HMM í”„ë¡œì íŠ¸ì— ê¸°ì—¬í•´ì£¼ì„¸ìš”!

- ğŸ› **ë²„ê·¸ ë¦¬í¬íŠ¸**: [Issues](https://github.com/crlotwhite/pytorch_hmm/issues)
- ğŸ’¡ **ê¸°ëŠ¥ ì œì•ˆ**: [Discussions](https://github.com/crlotwhite/pytorch_hmm/discussions)
- ğŸ”§ **Pull Request**: [Contributing Guide](CONTRIBUTING.md)

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ğŸ™ ê°ì‚¬ì˜ ë§

- **PyTorch íŒ€**: í›Œë¥­í•œ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
- **ìŒì„± ì²˜ë¦¬ ì»¤ë®¤ë‹ˆí‹°**: ê·€ì¤‘í•œ í”¼ë“œë°±ê³¼ ì œì•ˆ
- **ëª¨ë“  ê¸°ì—¬ìë“¤**: ì½”ë“œ, ë¬¸ì„œ, í…ŒìŠ¤íŠ¸ ê¸°ì—¬

### ê¸°ì—¬ ê°€ì´ë“œë¼ì¸

1. **ì´ìŠˆ ìƒì„±**: ìƒˆ ê¸°ëŠ¥ì´ë‚˜ ë²„ê·¸ ë¦¬í¬íŠ¸
2. **Fork & Branch**: `feature/your-feature-name`
3. **í…ŒìŠ¤íŠ¸ ì‘ì„±**: ìƒˆ ê¸°ëŠ¥ì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì¶”ê°€
4. **ë¬¸ì„œí™”**: docstringê³¼ README ì—…ë°ì´íŠ¸
5. **Pull Request**: ìƒì„¸í•œ ì„¤ëª…ê³¼ í•¨ê»˜ ì œì¶œ

## ğŸ“Š ë²„ì „ íˆìŠ¤í† ë¦¬ ë° ë¡œë“œë§µ

### âœ… v0.2.0 (í˜„ì¬) - Advanced Features
- âœ… Neural HMM with contextual modeling
- âœ… Hidden Semi-Markov Model (HSMM)
- âœ… DTW and CTC alignment algorithms
- âœ… Comprehensive evaluation metrics
- âœ… Performance optimization and GPU acceleration
- âœ… Production-ready features

### ğŸ”„ v0.3.0 (ê³„íš) - Real-world Integration
- [ ] LibriSpeech/KSS ë°ì´í„°ì…‹ ì§€ì›
- [ ] ONNX ë‚´ë³´ë‚´ê¸° ë° ì–‘ìí™”
- [ ] ì‹¤ì‹œê°„ ë§ˆì´í¬ ì…ë ¥ ì²˜ë¦¬
- [ ] Attention-based alignment
- [ ] ë©€í‹°í™”ì ëª¨ë¸ë§

### ğŸ¯ v1.0.0 (ëª©í‘œ) - Production Ready
- [ ] End-to-end TTS íŒŒì´í”„ë¼ì¸
- [ ] C++ ì¶”ë¡  ì—”ì§„
- [ ] ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- [ ] ì™„ì „í•œ ë¬¸ì„œí™” ë° API ì•ˆì •í™”

## ğŸ“š ì°¸ê³  ìë£Œ

### í•µì‹¬ ë…¼ë¬¸
- Rabiner, L. R. (1989). "A tutorial on hidden Markov models"
- Yu, K., et al. (2010). "Semi-Markov models for speech synthesis"
- Cuturi, M., & Blondel, M. (2017). "Soft-DTW: a Differentiable Loss Function for Time-Series"
- Graves, A., et al. (2006). "Connectionist temporal classification"

### ì‹¤ë¬´ í™œìš© ì‚¬ë¡€
- ìŒì„± í•©ì„± ì‹œìŠ¤í…œì—ì„œì˜ ìŒì†Œ ì •ë ¬
- ìŒì„± ì¸ì‹ì—ì„œì˜ CTC ë””ì½”ë”©
- í™”ì ì ì‘ì„ ìœ„í•œ DTW ì •ë ¬
- ì§€ì†ì‹œê°„ ëª¨ë¸ë§ì„ í†µí•œ ìì—°ìŠ¤ëŸ¬ìš´ TTS

## ğŸ™‹â€â™‚ï¸ ì§€ì› ë° ì»¤ë®¤ë‹ˆí‹°

- **GitHub Issues**: [ë²„ê·¸ ë¦¬í¬íŠ¸ ë° ê¸°ëŠ¥ ìš”ì²­](https://github.com/crlotwhite/pytorch_hmm/issues)
- **GitHub Discussions**: [ì¼ë°˜ ì§ˆë¬¸ ë° í† ë¡ ](https://github.com/crlotwhite/pytorch_hmm/discussions)
- **Documentation**: [ìƒì„¸ ë¬¸ì„œ](https://github.com/crlotwhite/pytorch_hmm/wiki)

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [LICENSE](LICENSE) íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

---

## ğŸ‰ v0.2.0 ì—…ë°ì´íŠ¸ í•˜ì´ë¼ì´íŠ¸

ì´ë²ˆ **v0.2.0** ì—…ë°ì´íŠ¸ëŠ” PyTorch HMM ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ **ì—°êµ¬ìš© ë„êµ¬ì—ì„œ í”„ë¡œë•ì…˜ ë ˆë”” ì†”ë£¨ì…˜**ìœ¼ë¡œ í¬ê²Œ ë°œì „ì‹œì¼°ìŠµë‹ˆë‹¤:

ğŸ§  **Neural HMM**: ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ëª¨ë¸ë§ìœ¼ë¡œ ê¸°ì¡´ HMMì˜ í•œê³„ ê·¹ë³µ
â±ï¸ **Semi-Markov HMM**: ëª…ì‹œì  ì§€ì†ì‹œê°„ ëª¨ë¸ë§ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± í•©ì„±
ğŸ¯ **ê³ ê¸‰ ì •ë ¬**: DTWì™€ CTCë¡œ ë‹¤ì–‘í•œ ì •ë ¬ needs ì§€ì›
ğŸ“Š **ì¢…í•© í‰ê°€**: MCD, F0 RMSE ë“± í‘œì¤€ ìŒì„± í‰ê°€ ë©”íŠ¸ë¦­
ğŸš€ **ì‹¤ì‹œê°„ ì„±ëŠ¥**: GPU ê°€ì†ìœ¼ë¡œ ì‹¤ì‹œê°„ ìŒì„± ì²˜ë¦¬ ê°€ëŠ¥

â­ **ì´ í”„ë¡œì íŠ¸ê°€ ë„ì›€ì´ ë˜ì—ˆë‹¤ë©´ Starë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”!** â­
