import torch
import numpy as np
import pytest
import sys
import os

# Add parent directory to path for importing
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from pytorch_hmm import (
    # Core HMM
    HMM, HMMPyTorch, HMMLayer, GaussianHMMLayer,
    # Neural HMM
    NeuralHMM, ContextualNeuralHMM,
    # Semi-Markov HMM
    SemiMarkovHMM, DurationModel,
    # Alignment
    DTWAligner, CTCAligner,
    # Metrics
    mel_cepstral_distortion, f0_root_mean_square_error, alignment_accuracy,
    comprehensive_speech_evaluation,
    # Utilities
    create_left_to_right_matrix, create_transition_matrix,
    compute_state_durations
)


class TestAdvancedFeatures:
    """ìƒˆë¡œ êµ¬í˜„ëœ ê³ ê¸‰ ê¸°ëŠ¥ë“¤ì˜ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    def setup_method(self):
        """ê° í…ŒìŠ¤íŠ¸ ì „ ì‹¤í–‰"""
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        torch.manual_seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ì„¤ì •
    
    def test_dtw_alignment(self):
        """DTW ì •ë ¬ í…ŒìŠ¤íŠ¸"""
        print("Testing DTW Alignment...")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        seq_len1, seq_len2, feature_dim = 20, 25, 10
        x = torch.randn(seq_len1, feature_dim)
        y = torch.randn(seq_len2, feature_dim)
        
        # DTW ì •ë ¬ ìˆ˜í–‰
        aligner = DTWAligner(distance_fn='euclidean', step_pattern='symmetric')
        path_i, path_j, total_cost = aligner(x, y)
        
        # ê²°ê³¼ ê²€ì¦
        assert len(path_i) == len(path_j), "Path lengths must match"
        assert path_i[0] == 0 and path_i[-1] == seq_len1 - 1, "Path must start and end correctly"
        assert path_j[0] == 0 and path_j[-1] == seq_len2 - 1, "Path must start and end correctly"
        assert torch.isfinite(total_cost), "Total cost must be finite"
        
        print(f"âœ“ DTW alignment successful. Path length: {len(path_i)}, Cost: {total_cost:.4f}")
    
    def test_ctc_alignment(self):
        """CTC ì •ë ¬ í…ŒìŠ¤íŠ¸"""
        print("Testing CTC Alignment...")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        batch_size, seq_len, num_classes = 2, 50, 10
        log_probs = torch.log_softmax(torch.randn(seq_len, batch_size, num_classes), dim=-1)
        
        targets = torch.randint(1, num_classes, (batch_size, 5))  # blank=0 ì œì™¸
        input_lengths = torch.full((batch_size,), seq_len)
        target_lengths = torch.full((batch_size,), 5)
        
        # CTC ì •ë ¬ ìˆ˜í–‰
        aligner = CTCAligner(num_classes=num_classes, blank_id=0)
        loss = aligner(log_probs, targets, input_lengths, target_lengths)
        
        # ë””ì½”ë”© í…ŒìŠ¤íŠ¸
        decoded = aligner.decode(log_probs, input_lengths)
        
        # ê²°ê³¼ ê²€ì¦
        assert torch.isfinite(loss), "CTC loss must be finite"
        assert len(decoded) == batch_size, "Must decode for each batch item"
        
        print(f"âœ“ CTC alignment successful. Loss: {loss:.4f}, Decoded lengths: {[len(d) for d in decoded]}")
    
    def test_neural_hmm(self):
        """Neural HMM í…ŒìŠ¤íŠ¸"""
        print("Testing Neural HMM...")
        
        # í…ŒìŠ¤íŠ¸ íŒŒë¼ë¯¸í„°
        num_states, observation_dim, context_dim = 5, 8, 12
        batch_size, seq_len = 2, 20
        
        # Neural HMM ìƒì„±
        neural_hmm = NeuralHMM(
            num_states=num_states,
            observation_dim=observation_dim,
            context_dim=context_dim,
            hidden_dim=64,
            transition_type='mlp',
            observation_type='gaussian'
        )
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        observations = torch.randn(batch_size, seq_len, observation_dim)
        context = torch.randn(batch_size, seq_len, context_dim)
        
        # Forward pass
        posteriors, forward, backward = neural_hmm(observations, context)
        
        # Viterbi ë””ì½”ë”©
        states, scores = neural_hmm.viterbi_decode(observations, context)
        
        # ê²°ê³¼ ê²€ì¦
        assert posteriors.shape == (batch_size, seq_len, num_states), f"Wrong posterior shape: {posteriors.shape}"
        assert torch.allclose(posteriors.sum(dim=-1), torch.ones(batch_size, seq_len), atol=1e-5), "Posteriors must sum to 1"
        assert states.shape == (batch_size, seq_len), f"Wrong states shape: {states.shape}"
        assert torch.all(states >= 0) and torch.all(states < num_states), "Invalid state indices"
        
        print(f"âœ“ Neural HMM successful. Posterior shape: {posteriors.shape}, States shape: {states.shape}")
    
    def test_contextual_neural_hmm(self):
        """Contextual Neural HMM í…ŒìŠ¤íŠ¸"""
        print("Testing Contextual Neural HMM...")
        
        # í…ŒìŠ¤íŠ¸ íŒŒë¼ë¯¸í„°
        num_states, observation_dim = 6, 10
        phoneme_vocab_size = 50
        batch_size, seq_len = 2, 15
        
        # Contextual Neural HMM ìƒì„±
        contextual_hmm = ContextualNeuralHMM(
            num_states=num_states,
            observation_dim=observation_dim,
            phoneme_vocab_size=phoneme_vocab_size,
            linguistic_context_dim=32,
            prosody_dim=8
        )
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        observations = torch.randn(batch_size, seq_len, observation_dim)
        phoneme_sequence = torch.randint(0, phoneme_vocab_size, (batch_size, seq_len))
        prosody_features = torch.randn(batch_size, seq_len, 8)
        
        # Forward pass with context
        posteriors, forward, backward = contextual_hmm.forward_with_context(
            observations, phoneme_sequence, prosody_features)
        
        # ê²°ê³¼ ê²€ì¦
        assert posteriors.shape == (batch_size, seq_len, num_states), f"Wrong posterior shape: {posteriors.shape}"
        assert torch.allclose(posteriors.sum(dim=-1), torch.ones(batch_size, seq_len), atol=1e-5), "Posteriors must sum to 1"
        
        print(f"âœ“ Contextual Neural HMM successful. Posterior shape: {posteriors.shape}")
    
    def test_semi_markov_hmm(self):
        """Semi-Markov HMM í…ŒìŠ¤íŠ¸"""
        print("Testing Semi-Markov HMM...")
        
        # í…ŒìŠ¤íŠ¸ íŒŒë¼ë¯¸í„°
        num_states, observation_dim, max_duration = 4, 6, 10
        
        # HSMM ìƒì„±
        hsmm = SemiMarkovHMM(
            num_states=num_states,
            observation_dim=observation_dim,
            max_duration=max_duration,
            duration_distribution='gamma',
            observation_model='gaussian'
        )
        
        # ì§€ì†ì‹œê°„ ëª¨ë¸ í…ŒìŠ¤íŠ¸
        state_indices = torch.randint(0, num_states, (3,))
        duration_probs = hsmm.duration_model(state_indices)
        
        assert duration_probs.shape == (3, max_duration), f"Wrong duration prob shape: {duration_probs.shape}"
        
        # ìƒ˜í”Œë§ í…ŒìŠ¤íŠ¸
        sampled_durations = hsmm.duration_model.sample(state_indices)
        assert len(sampled_durations) == len(state_indices), "Wrong number of sampled durations"
        assert torch.all(sampled_durations >= 1), "Durations must be at least 1"
        
        # ì‹œí€€ìŠ¤ ìƒ˜í”Œë§ í…ŒìŠ¤íŠ¸
        state_seq, duration_seq, obs_seq = hsmm.sample(num_states=5, max_length=50)
        
        assert len(state_seq) == len(duration_seq), "State and duration sequences must have same length"
        assert obs_seq.shape[1] == observation_dim, f"Wrong observation dimension: {obs_seq.shape[1]}"
        
        print(f"âœ“ Semi-Markov HMM successful. Sampled {len(state_seq)} states, {obs_seq.shape[0]} observations")
    
    def test_duration_model(self):
        """Duration Model í…ŒìŠ¤íŠ¸"""
        print("Testing Duration Model...")
        
        # ë‹¤ì–‘í•œ ë¶„í¬ íƒ€ì… í…ŒìŠ¤íŠ¸
        for distribution_type in ['gamma', 'poisson', 'gaussian', 'neural']:
            duration_model = DurationModel(
                num_states=5,
                max_duration=20,
                distribution_type=distribution_type
            )
            
            state_indices = torch.randint(0, 5, (3,))
            
            # ë¶„í¬ ê³„ì‚°
            log_probs = duration_model(state_indices)
            assert log_probs.shape == (3, 20), f"Wrong shape for {distribution_type}: {log_probs.shape}"
            
            # ìƒ˜í”Œë§
            samples = duration_model.sample(state_indices)
            assert len(samples) == 3, f"Wrong number of samples for {distribution_type}"
            assert torch.all(samples >= 1), f"Invalid samples for {distribution_type}"
            
            print(f"  âœ“ {distribution_type} distribution working")
        
        print("âœ“ Duration Model tests completed")
    
    def test_metrics(self):
        """Metrics í…ŒìŠ¤íŠ¸"""
        print("Testing Speech Quality Metrics...")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        seq_len, mfcc_dim = 100, 13
        mfcc_true = torch.randn(seq_len, mfcc_dim)
        mfcc_pred = mfcc_true + 0.1 * torch.randn(seq_len, mfcc_dim)  # ì•½ê°„ì˜ ë…¸ì´ì¦ˆ ì¶”ê°€
        
        # MCD í…ŒìŠ¤íŠ¸
        mcd = mel_cepstral_distortion(mfcc_true, mfcc_pred)
        assert torch.isfinite(mcd), "MCD must be finite"
        assert mcd >= 0, "MCD must be non-negative"
        
        # F0 RMSE í…ŒìŠ¤íŠ¸
        f0_true = torch.abs(torch.randn(seq_len)) * 200 + 100  # 100-300 Hz ë²”ìœ„
        f0_pred = f0_true + 10 * torch.randn(seq_len)  # 10Hz ë…¸ì´ì¦ˆ
        f0_rmse = f0_root_mean_square_error(f0_true, f0_pred)
        assert torch.isfinite(f0_rmse), "F0 RMSE must be finite"
        assert f0_rmse >= 0, "F0 RMSE must be non-negative"
        
        # ì •ë ¬ ì •í™•ë„ í…ŒìŠ¤íŠ¸
        alignment_true = torch.randint(0, 5, (seq_len,))
        alignment_pred = alignment_true.clone()
        alignment_pred[::10] = (alignment_pred[::10] + 1) % 5  # 10%ì˜ ì˜¤ë¥˜ ì¶”ê°€
        
        acc = alignment_accuracy(alignment_pred, alignment_true)
        assert 0 <= acc <= 1, "Accuracy must be between 0 and 1"
        assert acc > 0.8, "Accuracy should be reasonably high with small errors"
        
        # ì¢…í•© í‰ê°€ í…ŒìŠ¤íŠ¸
        predicted_features = {
            'mfcc': mfcc_pred.unsqueeze(0),
            'f0': f0_pred.unsqueeze(0),
            'alignment': alignment_pred
        }
        ground_truth_features = {
            'mfcc': mfcc_true.unsqueeze(0),
            'f0': f0_true.unsqueeze(0),
            'alignment': alignment_true
        }
        
        metrics = comprehensive_speech_evaluation(predicted_features, ground_truth_features)
        
        assert 'mcd' in metrics, "MCD metric missing"
        assert 'f0_rmse' in metrics, "F0 RMSE metric missing"
        assert 'alignment_accuracy' in metrics, "Alignment accuracy metric missing"
        
        print(f"âœ“ Metrics test successful. MCD: {mcd:.4f}, F0 RMSE: {f0_rmse:.4f}, Accuracy: {acc:.4f}")
    
    def test_integration_workflow(self):
        """ì „ì²´ ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸"""
        print("Testing Complete Integration Workflow...")
        
        # 1. ê¸°ë³¸ HMMìœ¼ë¡œ ì‹œì‘
        num_states = 5
        transition_matrix = create_left_to_right_matrix(num_states, self_loop_prob=0.7)
        basic_hmm = HMMPyTorch(transition_matrix)
        
        # 2. ê´€ì¸¡ ë°ì´í„° ìƒì„±
        batch_size, seq_len, obs_dim = 2, 30, num_states
        observations = torch.softmax(torch.randn(batch_size, seq_len, obs_dim), dim=-1)
        
        # 3. ê¸°ë³¸ HMMìœ¼ë¡œ ì •ë ¬
        posteriors, _, _ = basic_hmm.forward_backward(observations)
        basic_states, _ = basic_hmm.viterbi_decode(observations)
        
        # 4. DTWë¡œ ì¬ì •ë ¬
        dtw_aligner = DTWAligner()
        ref_features = torch.randn(num_states, obs_dim)
        for b in range(batch_size):
            path_i, path_j, _ = dtw_aligner(ref_features, observations[b])
            assert len(path_i) > 0, "DTW must produce valid path"
        
        # 5. Neural HMMìœ¼ë¡œ ê³ ê¸‰ ëª¨ë¸ë§
        neural_hmm = NeuralHMM(
            num_states=num_states,
            observation_dim=num_states,
            context_dim=8,
            hidden_dim=32
        )
        
        context = torch.randn(batch_size, seq_len, 8)
        neural_posteriors, _, _ = neural_hmm(observations, context)
        neural_states, _ = neural_hmm.viterbi_decode(observations, context)
        
        # 6. ê²°ê³¼ í‰ê°€
        for b in range(batch_size):
            acc = alignment_accuracy(neural_states[b], basic_states[b], tolerance=1)
            assert 0 <= acc <= 1, "Accuracy must be valid"
        
        # 7. ì§€ì†ì‹œê°„ ë¶„ì„
        for b in range(batch_size):
            durations = compute_state_durations(basic_states[b])
            assert len(durations) > 0, "Must have some state durations"
            assert torch.all(durations > 0), "All durations must be positive"
        
        print("âœ“ Complete integration workflow successful!")
    
    def test_batch_processing(self):
        """ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        print("Testing Batch Processing...")
        
        # ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸°ë¡œ í…ŒìŠ¤íŠ¸
        for batch_size in [1, 4, 8]:
            seq_len, num_states, obs_dim = 25, 6, 10
            
            # Neural HMM
            neural_hmm = NeuralHMM(
                num_states=num_states,
                observation_dim=obs_dim,
                context_dim=5,
                hidden_dim=32
            )
            
            observations = torch.randn(batch_size, seq_len, obs_dim)
            context = torch.randn(batch_size, seq_len, 5)
            
            # ë°°ì¹˜ ì²˜ë¦¬
            posteriors, _, _ = neural_hmm(observations, context)
            states, _ = neural_hmm.viterbi_decode(observations, context)
            
            assert posteriors.shape == (batch_size, seq_len, num_states), f"Wrong shape for batch_size={batch_size}"
            assert states.shape == (batch_size, seq_len), f"Wrong states shape for batch_size={batch_size}"
            
            print(f"  âœ“ Batch size {batch_size} successful")
        
        print("âœ“ Batch processing tests completed")
    
    def test_device_compatibility(self):
        """Device í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
        print("Testing Device Compatibility...")
        
        for device in ['cpu'] + (['cuda'] if torch.cuda.is_available() else []):
            print(f"  Testing on {device}...")
            
            # Neural HMM
            neural_hmm = NeuralHMM(
                num_states=4,
                observation_dim=6,
                context_dim=4,
                hidden_dim=16
            ).to(device)
            
            observations = torch.randn(2, 15, 6, device=device)
            context = torch.randn(2, 15, 4, device=device)
            
            posteriors, _, _ = neural_hmm(observations, context)
            states, _ = neural_hmm.viterbi_decode(observations, context)
            
            assert posteriors.device.type == device, f"Wrong device for posteriors"
            assert states.device.type == device, f"Wrong device for states"
            
            print(f"  âœ“ {device.upper()} device successful")
        
        print("âœ“ Device compatibility tests completed")
    
    def test_error_handling(self):
        """ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        print("Testing Error Handling...")
        
        # ì˜ëª»ëœ ì…ë ¥ ì°¨ì›
        neural_hmm = NeuralHMM(num_states=5, observation_dim=10, context_dim=8)
        
        try:
            wrong_obs = torch.randn(2, 20, 15)  # Wrong observation dim
            context = torch.randn(2, 20, 8)
            neural_hmm(wrong_obs, context)
            assert False, "Should have raised an error"
        except:
            print("  âœ“ Correctly caught dimension mismatch error")
        
        # DTW with empty sequences
        dtw_aligner = DTWAligner()
        try:
            empty_seq = torch.empty(0, 5)
            normal_seq = torch.randn(10, 5)
            dtw_aligner(empty_seq, normal_seq)
            print("  âœ“ DTW handles empty sequences gracefully")
        except:
            print("  âœ“ DTW correctly rejects empty sequences")
        
        print("âœ“ Error handling tests completed")


def test_performance_benchmark():
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*50)
    print("PERFORMANCE BENCHMARK")
    print("="*50)
    
    import time
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Running benchmarks on: {device.upper()}")
    
    # í…ŒìŠ¤íŠ¸ ì„¤ì •
    configs = [
        (2, 50, 10, 5),    # Small: batch=2, seq=50, obs=10, states=5
        (4, 100, 20, 10),  # Medium: batch=4, seq=100, obs=20, states=10
        (8, 200, 40, 15),  # Large: batch=8, seq=200, obs=40, states=15
    ]
    
    for batch_size, seq_len, obs_dim, num_states in configs:
        print(f"\nTesting: batch={batch_size}, seq={seq_len}, obs={obs_dim}, states={num_states}")
        
        # ë°ì´í„° ì¤€ë¹„
        observations = torch.randn(batch_size, seq_len, num_states, device=device)
        context = torch.randn(batch_size, seq_len, obs_dim//2, device=device)
        
        # Basic HMM ë²¤ì¹˜ë§ˆí¬
        transition_matrix = create_left_to_right_matrix(num_states)
        basic_hmm = HMMPyTorch(transition_matrix).to(device)
        
        start_time = time.time()
        with torch.no_grad():
            posteriors, _, _ = basic_hmm.forward_backward(observations)
        basic_time = time.time() - start_time
        
        # Neural HMM ë²¤ì¹˜ë§ˆí¬
        neural_hmm = NeuralHMM(
            num_states=num_states,
            observation_dim=num_states,
            context_dim=obs_dim//2,
            hidden_dim=64
        ).to(device)
        
        start_time = time.time()
        with torch.no_grad():
            neural_posteriors, _, _ = neural_hmm(observations, context)
        neural_time = time.time() - start_time
        
        # DTW ë²¤ì¹˜ë§ˆí¬ (ë‹¨ì¼ ì‹œí€€ìŠ¤)
        dtw_aligner = DTWAligner()
        ref_seq = torch.randn(num_states, obs_dim, device=device)
        
        start_time = time.time()
        with torch.no_grad():
            path_i, path_j, _ = dtw_aligner(ref_seq, observations[0])
        dtw_time = time.time() - start_time
        
        # ê²°ê³¼ ì¶œë ¥
        frames_per_sec_basic = (batch_size * seq_len) / basic_time
        frames_per_sec_neural = (batch_size * seq_len) / neural_time
        frames_per_sec_dtw = seq_len / dtw_time
        
        print(f"  Basic HMM:   {basic_time:.4f}s ({frames_per_sec_basic:.0f} frames/sec)")
        print(f"  Neural HMM:  {neural_time:.4f}s ({frames_per_sec_neural:.0f} frames/sec)")
        print(f"  DTW:         {dtw_time:.4f}s ({frames_per_sec_dtw:.0f} frames/sec)")
        
        # ì‹¤ì‹œê°„ ì²˜ë¦¬ ê°€ëŠ¥ ì—¬ë¶€ (80fps ê¸°ì¤€)
        realtime_threshold = 80
        print(f"  Realtime capable (>80fps): Basic={frames_per_sec_basic>realtime_threshold}, "
              f"Neural={frames_per_sec_neural>realtime_threshold}, DTW={frames_per_sec_dtw>realtime_threshold}")
    
    print("\n" + "="*50)


def run_comprehensive_tests():
    """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("Starting Comprehensive PyTorch HMM Tests...")
    print("="*60)
    
    test_suite = TestAdvancedFeatures()
    test_suite.setup_method()
    
    # ëª¨ë“  í…ŒìŠ¤íŠ¸ ë©”ì„œë“œ ì‹¤í–‰
    test_methods = [
        test_suite.test_dtw_alignment,
        test_suite.test_ctc_alignment,
        test_suite.test_neural_hmm,
        test_suite.test_contextual_neural_hmm,
        test_suite.test_semi_markov_hmm,
        test_suite.test_duration_model,
        test_suite.test_metrics,
        test_suite.test_integration_workflow,
        test_suite.test_batch_processing,
        test_suite.test_device_compatibility,
        test_suite.test_error_handling
    ]
    
    passed = 0
    failed = 0
    
    for test_method in test_methods:
        try:
            test_method()
            passed += 1
        except Exception as e:
            print(f"âŒ {test_method.__name__} FAILED: {e}")
            failed += 1
    
    print("\n" + "="*60)
    print(f"TEST SUMMARY: {passed} passed, {failed} failed")
    
    if failed == 0:
        print("ğŸ‰ ALL TESTS PASSED!")
        
        # ì„±ê³µì ìœ¼ë¡œ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ í†µê³¼í•˜ë©´ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        test_performance_benchmark()
    else:
        print("âŒ Some tests failed. Please check the implementation.")
    
    return failed == 0


if __name__ == "__main__":
    # ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    success = run_comprehensive_tests()
    
    # pytest í˜¸í™˜ì„±ì„ ìœ„í•œ ë°˜í™˜ê°’
    if not success:
        sys.exit(1)
